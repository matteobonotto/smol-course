{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# How to Fine-Tune LLMs with LoRA Adapters using Hugging Face TRL\n",
    "\n",
    "This notebook demonstrates how to efficiently fine-tune large language models using LoRA (Low-Rank Adaptation) adapters. LoRA is a parameter-efficient fine-tuning technique that:\n",
    "- Freezes the pre-trained model weights\n",
    "- Adds small trainable rank decomposition matrices to attention layers\n",
    "- Typically reduces trainable parameters by ~90%\n",
    "- Maintains model performance while being memory efficient\n",
    "\n",
    "We'll cover:\n",
    "1. Setup development environment and LoRA configuration\n",
    "2. Create and prepare the dataset for adapter training\n",
    "3. Fine-tune using `trl` and `SFTTrainer` with LoRA adapters\n",
    "4. Test the model and merge adapters (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pytorch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "\n",
    "# login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# device = \"cuda:1\"\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 100, 16425, 38, 3326, 230, 42338, 18024, 12, 975, 25322, 4581, 31, 127, 569, 1400, 142, 9, 70, 5, 6170, 14, 7501, 24, 77, 24, 21, 78, 703, 11, 13025, 4, 38, 67, 1317, 14, 23, 78, 24, 21, 5942, 30, 121, 4, 104, 4, 10102, 114, 24, 655, 1381, 7, 2914, 42, 247, 6, 3891, 145, 10, 2378, 9, 3541, 1687, 22, 10800, 34689, 113, 38, 269, 56, 7, 192, 42, 13, 2185, 49069, 3809, 1589, 49007, 3809, 48709, 133, 6197, 16, 14889, 198, 10, 664, 9004, 4149, 1294, 1440, 27450, 54, 1072, 7, 1532, 960, 79, 64, 59, 301, 4, 96, 1989, 79, 1072, 7, 1056, 69, 39879, 2485, 7, 442, 103, 2345, 9, 6717, 15, 99, 5, 674, 25517, 242, 802, 59, 1402, 559, 743, 215, 25, 5, 5490, 1771, 8, 1015, 743, 11, 5, 315, 532, 4, 96, 227, 1996, 3770, 8, 7945, 3069, 38839, 9, 18850, 59, 49, 5086, 15, 2302, 6, 79, 34, 2099, 19, 69, 4149, 3254, 6, 18295, 6, 8, 2997, 604, 49069, 3809, 1589, 49007, 3809, 48709, 2264, 10469, 162, 59, 38, 3326, 230, 42338, 18024, 12, 975, 25322, 4581, 16, 14, 843, 107, 536, 6, 42, 21, 1687, 38739, 4, 16923, 6, 5, 2099, 8, 36067, 5422, 32, 367, 8, 444, 227, 6, 190, 172, 24, 18, 45, 738, 101, 103, 34358, 156, 13971, 139, 4, 616, 127, 247, 2262, 1508, 465, 24, 8777, 6, 11, 2015, 2099, 8, 36067, 32, 10, 538, 17771, 11, 9004, 11605, 4, 1648, 11996, 3916, 15303, 397, 6, 10522, 49, 1948, 7, 205, 793, 2143, 610, 2493, 6, 56, 2099, 5422, 11, 39, 3541, 49069, 3809, 1589, 49007, 3809, 48709, 100, 109, 19781, 5, 17504, 13, 5, 754, 14, 143, 2099, 2343, 11, 5, 822, 16, 2343, 13, 11419, 6216, 1195, 87, 95, 7, 4817, 82, 8, 146, 418, 7, 28, 2343, 11, 38739, 11327, 11, 730, 4, 38, 3326, 230, 42338, 18024, 12, 975, 25322, 4581, 16, 10, 205, 822, 13, 1268, 6923, 7, 892, 5, 4884, 8, 15042, 36, 2362, 7434, 3833, 43, 9, 9004, 11605, 4, 125, 269, 6, 42, 822, 630, 75, 33, 203, 9, 10, 6197, 4], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "</s>I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train[:100]\")\n",
    "from trl import SFTConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-350m\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    args = SFTConfig(\n",
    "      dataset_text_field=\"text\",\n",
    "      output_dir=\"tmp\",\n",
    "      max_seq_length=512,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(trainer.train_dataset[0])\n",
    "\n",
    "print(\n",
    "    tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'messages'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'messages'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "# dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\", split=\"train\")\n",
    "# dataset = load_dataset(path=\"prithivMLmods/Math-Solve\", split=\"train[:25000]\")\n",
    "\n",
    "# def format_dataset(sample):\n",
    "#     content = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are helpful\"},\n",
    "#         {\"role\": \"user\", \"content\": sample[\"problem\"]}, \n",
    "#         {\"role\": \"assistant\", \"content\": sample[\"solution\"]}\n",
    "#     ]\n",
    "#     return {\"messages\":content}\n",
    "\n",
    "# dataset = dataset.map(format_dataset)\n",
    "\n",
    "\n",
    "# dataset = load_dataset(path=\"Qurtana/medical-o1-reasoning-SFT-orpo\",  split=\"train\")\n",
    "# def format(sample):\n",
    "#     content = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": sample[\"prompt\"]}, \n",
    "#         {\"role\": \"assistant\", \"content\": sample[\"accepted\"]}\n",
    "#     ]\n",
    "#     return {\"messages\":content}\n",
    "\n",
    "# dataset=dataset.map(format)\n",
    "\n",
    "# dataset_train = dataset.select(range(20000))\n",
    "# dataset_test = dataset.select(range(dataset.shape[0]-2000, dataset.shape[0]))\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "dataset = load_dataset(path=\"openai/gsm8k\", name=\"main\")\n",
    "\n",
    "def format(sample):\n",
    "    pass\n",
    "    content = [\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": sample[\"question\"]}, \n",
    "        {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "    return {\"messages\":content}\n",
    "\n",
    "dataset=dataset.map(format)\n",
    "\n",
    "dataset_train = dataset[\"train\"]\n",
    "dataset_test = dataset[\"test\"]\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. Fine-tune LLM using `trl` and the `SFTTrainer` with LoRA\n",
    "\n",
    "The [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` provides integration with LoRA adapters through the [PEFT](https://huggingface.co/docs/peft/en/index) library. Key advantages of this setup include:\n",
    "\n",
    "1. **Memory Efficiency**: \n",
    "   - Only adapter parameters are stored in GPU memory\n",
    "   - Base model weights remain frozen and can be loaded in lower precision\n",
    "   - Enables fine-tuning of large models on consumer GPUs\n",
    "\n",
    "2. **Training Features**:\n",
    "   - Native PEFT/LoRA integration with minimal setup\n",
    "   - Support for QLoRA (Quantized LoRA) for even better memory efficiency\n",
    "\n",
    "3. **Adapter Management**:\n",
    "   - Adapter weight saving during checkpoints\n",
    "   - Features to merge adapters back into base model\n",
    "\n",
    "We'll use LoRA in our example, which combines LoRA with 4-bit quantization to further reduce memory usage without sacrificing performance. The setup requires just a few configuration steps:\n",
    "1. Define the LoRA configuration (rank, alpha, dropout)\n",
    "2. Create the SFTTrainer with PEFT config\n",
    "3. Train and save the adapter weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map = device,\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-gsm8k-sft-peft\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]\n",
    "\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "The¬†`SFTTrainer`¬† supports a native integration with¬†`peft`, which makes it super easy to efficiently tune LLMs using, e.g. LoRA. We only need to create our¬†`LoraConfig`¬†and provide it to the trainer.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Define LoRA parameters for finetuning</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the general parameters for an abitrary finetune</p>\n",
    "    <p>üêï Adjust the parameters and review in weights & biases.</p>\n",
    "    <p>ü¶Å Adjust the parameters and show change in inference results.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 48\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 64\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.01\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "Before we can start our training we need to define the hyperparameters (`TrainingArguments`) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matte/smol-course/.venv/lib/python3.10/site-packages/transformers/training_args.py:1576: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "max_seq_length = 1512  # max sequence length for model and packing of the dataset\n",
    "\n",
    "args = SFTConfig(\n",
    "    # Small learning rate to prevent catastrophic forgetting\n",
    "    learning_rate=1e-5,\n",
    "    # Linear learning rate decay over training\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # Maximum combined length of prompt + completion\n",
    "    max_seq_length=1512,\n",
    "    # # Maximum length for input prompts\n",
    "    # max_prompt_length=512,\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    # Helps with training stability by accumulating gradients before updating\n",
    "    gradient_accumulation_steps=4,\n",
    "    # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
    "    optim=\"paged_adamw_8bit\" if device == \"cuda\" else \"adamw_torch\",\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=4,\n",
    "    # When to run evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # Evaluate every 20% of training\n",
    "    eval_steps=0.2,\n",
    "    # Log metrics every step\n",
    "    logging_steps=1,\n",
    "    # Gradual learning rate warmup\n",
    "    warmup_steps=100,\n",
    "    # Disable external logging\n",
    "    report_to=\"wandb\",\n",
    "    # Where to save model/checkpoints\n",
    "    output_dir=finetune_name,\n",
    "    # Enable MPS (Metal Performance Shaders) if available\n",
    "    use_mps_device=device == \"mps\",\n",
    "    hub_model_id=finetune_name,\n",
    "    # Use bfloat16 precision for faster training\n",
    "    bf16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "We now have every building block we need to create our¬†`SFTTrainer`¬†to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [],
   "source": [
    "\n",
    "max_seq_length = 1512  # max sequence length for model and packing of the dataset\n",
    "\n",
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 9690, 198, 2683, 359, 5356, 11173, 2, 198, 1, 4093, 198, 71, 1059, 38668, 1885, 33, 34, 354, 5353, 327, 3383, 672, 9584, 30, 718, 15955, 28, 1041, 915, 1250, 216, 37, 32, 3487, 282, 3383, 672, 9584, 30, 1073, 1083, 1250, 1041, 5301, 47, 2, 198, 1, 520, 9531, 198, 71, 1059, 38668, 216, 33, 34, 31, 38, 32, 446, 1885, 33691, 33, 34, 31, 38, 32, 45, 32, 30, 34, 7791, 32, 30, 34, 567, 8427, 30, 198, 23830, 216, 37, 32, 3487, 28, 1041, 11420, 216, 32, 30, 34, 1792, 216, 37, 32, 446, 1885, 33691, 32, 30, 34, 26, 37, 32, 45, 33, 32, 7791, 33, 32, 30, 198, 1229, 216, 33, 32, 2, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "<|im_start|>system\n",
      "You are helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "#### 10<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trainer.train_dataset[1])\n",
    "\n",
    "print(\n",
    "    tokenizer.decode(trainer.train_dataset[1][\"input_ids\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainer.train_dataset[0])\n",
    "# print(\"\\n\\n\")\n",
    "# print(tokenizer.decode(trainer.train_dataset[0][\"input_ids\"]))\n",
    "# print(\"\\n\\n\")\n",
    "# print(tokenizer.decode(trainer.train_dataset[0][\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteob-90-hotmail-it\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matte/smol-course/3_parameter_efficient_finetuning/notebooks/wandb/run-20250123_204328-qxiy6boa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteob-90-hotmail-it/huggingface/runs/qxiy6boa' target=\"_blank\">SmolLM2-FT-gsm8k-sft-peft</a></strong> to <a href='https://wandb.ai/matteob-90-hotmail-it/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteob-90-hotmail-it/huggingface' target=\"_blank\">https://wandb.ai/matteob-90-hotmail-it/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteob-90-hotmail-it/huggingface/runs/qxiy6boa' target=\"_blank\">https://wandb.ai/matteob-90-hotmail-it/huggingface/runs/qxiy6boa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3736' max='3736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3736/3736 1:20:45, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>1.191400</td>\n",
       "      <td>1.209106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>1.043900</td>\n",
       "      <td>1.159239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2244</td>\n",
       "      <td>1.095800</td>\n",
       "      <td>1.138477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2992</td>\n",
       "      <td>1.089400</td>\n",
       "      <td>1.131238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "The training with Flash Attention for 3 epochs with a dataset of 15k samples took 4:14:36 on a `g5.2xlarge`. The instance costs `1.21$/h` which brings us to a total cost of only ~`5.3$`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### Merge LoRA Adapter into the Original Model\n",
    "\n",
    "When using LoRA, we only train adapter weights while keeping the base model frozen. During training, we save only these lightweight adapter weights (~2-10MB) rather than a full model copy. However, for deployment, you might want to merge the adapters back into the base model for:\n",
    "\n",
    "1. **Simplified Deployment**: Single model file instead of base model + adapters\n",
    "2. **Inference Speed**: No adapter computation overhead\n",
    "3. **Framework Compatibility**: Better compatibility with serving frameworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Load LoRA Adapter</h2>\n",
    "    <p>Use what you learnt from the ecample note book to load your trained LoRA adapter for inference.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=merged_model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=device, \n",
    "    max_new_tokens=100,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "Lets test some prompt samples and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prompt:\n",
      "What is the capital of Germany? Explain why thats the case and if it was different in the past?\n",
      "    response:\n",
      "In 1945, there were more than a million refugees from Nazi-occupied Europe. How many people did they take with them on their way to America after being liberated by Allied forces during World War II (WWII)? In what year do you think this number would have been higher or lower for those who had already taken up permanent residence at Ellis Island before WW2 began taking place as an occupation force took over New York City's East River which then became its own separate city called Manhattan\n",
      "--------------------------------------------------\n",
      "    prompt:\n",
      "Write a Python function to calculate the factorial of a number.\n",
      "    response:\n",
      "#### 1234567890 is not an integer because it has more than one digit in its first two digits, so we can‚Äôt add them all up and find out how many numbers there are that have this as their base-ten numeral representation (because they would be too large for our calculator). We will instead use another method: multiplying by itself multiple times until you get something with no decimal places on top or bottom; then adding those values together gives us\n",
      "--------------------------------------------------\n",
      "    prompt:\n",
      "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "    response:\n",
      "There are twice as much space on each side than there is in one square foot so we can calculate that by multiplying both numbers together: two times five equals <<40+8=76>>seven hundred sixty-five spaces per meter squared (m/s¬≤). We then divide this number into three equal parts which gives us seven millionths because it's been divided evenly between all four sides; thus making our final answer be seventy six thousand fifty nine meters or about half an inch long!\n",
      "--------------------------------------------------\n",
      "    prompt:\n",
      "What is the difference between a fruit and a vegetable? Give examples of each.\n",
      "    response:\n",
      "A 20-year old man has been diagnosed with cancer, but his doctor tells him that he will be able to live for at least two more years without any side effects from chemo because it works so well on people who have already had their tumors removed by surgery or radiation therapy (or both). How much longer can this patient wait before being offered another treatment option such as chemotherapy if they are still in remission after all other treatments stop working?). Explain how you would go about answering these questions\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prompt:\n",
      "Write a haiku\n",
      "    response:\n",
      "ASSISTANT SUPERSTITIONARY GUARDIAN 2018: I am an assistant super-supporter who helps people with their daily tasks. My job is to make sure that everyone has enough food and water, so they don‚Äôt get sick or die because of the cold weather in New York City (NYC). If you have any questions about my work please email me at [email¬†protected] You can also find out more by reading this blog post\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Write a haiku\"]\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt:\n",
    "What is the capital of Germany? Explain why thats the case and if it was different in the past?\n",
    "    response:\n",
    "To answer this question, we need to look at some historical facts. In 1870 there were two German states: Baden-Wuerttemberg (German for \"Bavaria with Wurttenberg\" or Bavarian Palatinate), which had a long history as an independent principality before being annexed by Prussia during World War II; Saxony/Saxonschweiz (\"Saxon Schleswig\"), whose territory came under Prussian control after gaining its independence from Denmark through negotiations between Otto von Bismarck's Chancellor Wilhelmina zu Hohenzollern Wilhelm I & King Frederick William IV on August 26th - September  3rd. The former state has since been reincorporated into S√ºddeutsche Bundesrepubliken / Deutsches Volkskreuzgefahrzeug [DWDG], also called Deutschland √ºber all diese Zeit [\"Germany Under All Time\n",
    "--------------------------------------------------\n",
    "    prompt:\n",
    "Write a Python function to calculate the factorial of a number.\n",
    "    response:\n",
    "Let $n$ be an integer greater than 1, and let $\\sum_{k=0}^{\\infty} n = \\frac{3}{2!}$ for some non-zero prime power $(p_i)$. We can rewrite this expression as follows: (Note that we are working with positive integers here.) The sum is computed by adding up each term in its own row or column until it reaches zero; if there were no terms between two consecutive numbers then they would have been added together at least once before being subtracted from their respective rows/colonelstheoreticalexpansionofthefactorialfunctioniscomputablebymultiplying themtogetherandaddingone moreterm). This gives us$\\left(\\dfrac{\\pi^4}{\\sqrt[5]{9}}+\\cdots+7^{6}\\right)$for all primes not exceeding one hundred thousand but excluding those whose only common divisor divides evenly into foursquare root(which means\n",
    "--------------------------------------------------\n",
    "    prompt:\n",
    "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
    "    response:\n",
    "Let's start by thinking about what we are looking for in this problem: How much space is there between two consecutive points on an equilateral triangle (a rectangle)? We know that it can be written as $\\triangle ABC$, where $AB = \\frac{4}{3} A + B$. So if I add up all three sides along with their corresponding angles ($\\overrightarrow {ABC}$$=60^\\circ) then my equation becomes $(AD+BC)\\cos(\\theta)+(AC+\\Delta)=897.\\]I have already seen some similar problems involving right triangles but they don't work out so well when using area or perimeter formulas! Let me try one more example from real-life situations before moving onto our main task - finding total cost per square foot without considering any extra costs like utility bills etc., which might come into play at later stages depending upon various factors mentioned earlier during your math practice session...[SOLVED](https://www\n",
    "--------------------------------------------------\n",
    "    prompt:\n",
    "What is the difference between a fruit and a vegetable? Give examples of each.\n",
    "    response:\n",
    "The word \"fruit\" refers to something that grows on trees, such as an apple or banana; it can also be used for any edible plant with seeds inside (like tomatoes). Vegetables are different from fruits because they don't produce their own food through photosynthesis but instead grow by eating other living things called plants! Some common vegetables include broccoli florets (*), carrots*, bell peppers**, zucchinis**. You might have seen them growing in your garden when you were younger - just think about all those delicious little green ones popping up every time someone goes outside during hot summer days... That's what we call'vegetables'. Now let me explain how these two categories differ:\n",
    "\n",
    "1Ô∏è‚É£ **Fruits**: Fruits contain natural sugars like watermelons *and* grapes which give us energy while satisfying our sweet tooth without adding extra calories.* They're usually eaten fresh rather than cooked down into ice cream sundaes at birthday parties.**[Example:* An orange contains 90\n",
    "--------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}