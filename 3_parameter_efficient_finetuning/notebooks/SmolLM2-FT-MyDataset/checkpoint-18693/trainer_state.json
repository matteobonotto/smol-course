{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9995988124849555,
  "eval_steps": 500,
  "global_step": 18693,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016047500601781273,
      "grad_norm": 0.3481806814670563,
      "learning_rate": 3.5650623885918005e-07,
      "loss": 2.0275,
      "step": 10
    },
    {
      "epoch": 0.0032095001203562546,
      "grad_norm": 0.27435800433158875,
      "learning_rate": 7.130124777183601e-07,
      "loss": 1.7784,
      "step": 20
    },
    {
      "epoch": 0.004814250180534382,
      "grad_norm": 0.2910098433494568,
      "learning_rate": 1.0695187165775401e-06,
      "loss": 1.7868,
      "step": 30
    },
    {
      "epoch": 0.006419000240712509,
      "grad_norm": 0.3374997675418854,
      "learning_rate": 1.4260249554367202e-06,
      "loss": 1.7637,
      "step": 40
    },
    {
      "epoch": 0.008023750300890637,
      "grad_norm": 0.5533764362335205,
      "learning_rate": 1.7825311942959003e-06,
      "loss": 1.7994,
      "step": 50
    },
    {
      "epoch": 0.009628500361068763,
      "grad_norm": 0.3769804835319519,
      "learning_rate": 2.1390374331550802e-06,
      "loss": 1.8867,
      "step": 60
    },
    {
      "epoch": 0.01123325042124689,
      "grad_norm": 0.3294832408428192,
      "learning_rate": 2.4955436720142603e-06,
      "loss": 1.8388,
      "step": 70
    },
    {
      "epoch": 0.012838000481425018,
      "grad_norm": 0.28214165568351746,
      "learning_rate": 2.8520499108734404e-06,
      "loss": 1.6355,
      "step": 80
    },
    {
      "epoch": 0.014442750541603145,
      "grad_norm": 0.28274163603782654,
      "learning_rate": 3.2085561497326205e-06,
      "loss": 1.7754,
      "step": 90
    },
    {
      "epoch": 0.016047500601781273,
      "grad_norm": 0.3549898862838745,
      "learning_rate": 3.5650623885918006e-06,
      "loss": 1.6496,
      "step": 100
    },
    {
      "epoch": 0.0176522506619594,
      "grad_norm": 0.2647390365600586,
      "learning_rate": 3.92156862745098e-06,
      "loss": 1.5751,
      "step": 110
    },
    {
      "epoch": 0.019257000722137527,
      "grad_norm": 0.3015369772911072,
      "learning_rate": 4.2780748663101604e-06,
      "loss": 1.8334,
      "step": 120
    },
    {
      "epoch": 0.020861750782315655,
      "grad_norm": 0.2643004357814789,
      "learning_rate": 4.6345811051693405e-06,
      "loss": 1.8201,
      "step": 130
    },
    {
      "epoch": 0.02246650084249378,
      "grad_norm": 0.4140813946723938,
      "learning_rate": 4.991087344028521e-06,
      "loss": 1.7839,
      "step": 140
    },
    {
      "epoch": 0.02407125090267191,
      "grad_norm": 0.3582390248775482,
      "learning_rate": 5.347593582887702e-06,
      "loss": 1.668,
      "step": 150
    },
    {
      "epoch": 0.025676000962850037,
      "grad_norm": 0.5730404853820801,
      "learning_rate": 5.704099821746881e-06,
      "loss": 1.9012,
      "step": 160
    },
    {
      "epoch": 0.027280751023028165,
      "grad_norm": 0.46824926137924194,
      "learning_rate": 6.060606060606061e-06,
      "loss": 1.8621,
      "step": 170
    },
    {
      "epoch": 0.02888550108320629,
      "grad_norm": 0.46423736214637756,
      "learning_rate": 6.417112299465241e-06,
      "loss": 1.8651,
      "step": 180
    },
    {
      "epoch": 0.03049025114338442,
      "grad_norm": 0.34394699335098267,
      "learning_rate": 6.773618538324421e-06,
      "loss": 1.9437,
      "step": 190
    },
    {
      "epoch": 0.03209500120356255,
      "grad_norm": 0.5196558833122253,
      "learning_rate": 7.130124777183601e-06,
      "loss": 1.9508,
      "step": 200
    },
    {
      "epoch": 0.033699751263740675,
      "grad_norm": 0.8910748958587646,
      "learning_rate": 7.486631016042781e-06,
      "loss": 1.9862,
      "step": 210
    },
    {
      "epoch": 0.0353045013239188,
      "grad_norm": 0.6707438826560974,
      "learning_rate": 7.84313725490196e-06,
      "loss": 1.6518,
      "step": 220
    },
    {
      "epoch": 0.036909251384096925,
      "grad_norm": 0.428229957818985,
      "learning_rate": 8.19964349376114e-06,
      "loss": 1.5906,
      "step": 230
    },
    {
      "epoch": 0.03851400144427505,
      "grad_norm": 0.39312443137168884,
      "learning_rate": 8.556149732620321e-06,
      "loss": 1.7584,
      "step": 240
    },
    {
      "epoch": 0.04011875150445318,
      "grad_norm": 0.42399024963378906,
      "learning_rate": 8.912655971479501e-06,
      "loss": 1.6653,
      "step": 250
    },
    {
      "epoch": 0.04172350156463131,
      "grad_norm": 0.44612210988998413,
      "learning_rate": 9.269162210338681e-06,
      "loss": 1.8681,
      "step": 260
    },
    {
      "epoch": 0.04332825162480944,
      "grad_norm": 0.7041329741477966,
      "learning_rate": 9.625668449197861e-06,
      "loss": 1.6439,
      "step": 270
    },
    {
      "epoch": 0.04493300168498756,
      "grad_norm": 0.7503124475479126,
      "learning_rate": 9.982174688057041e-06,
      "loss": 1.6185,
      "step": 280
    },
    {
      "epoch": 0.04653775174516569,
      "grad_norm": 0.5278136134147644,
      "learning_rate": 1.0338680926916223e-05,
      "loss": 1.8324,
      "step": 290
    },
    {
      "epoch": 0.04814250180534382,
      "grad_norm": 0.42845892906188965,
      "learning_rate": 1.0695187165775403e-05,
      "loss": 1.7182,
      "step": 300
    },
    {
      "epoch": 0.049747251865521945,
      "grad_norm": 0.32218316197395325,
      "learning_rate": 1.1051693404634583e-05,
      "loss": 1.9951,
      "step": 310
    },
    {
      "epoch": 0.05135200192570007,
      "grad_norm": 0.21394136548042297,
      "learning_rate": 1.1408199643493762e-05,
      "loss": 1.648,
      "step": 320
    },
    {
      "epoch": 0.0529567519858782,
      "grad_norm": 0.2552504241466522,
      "learning_rate": 1.1764705882352942e-05,
      "loss": 1.6926,
      "step": 330
    },
    {
      "epoch": 0.05456150204605633,
      "grad_norm": 0.43248167634010315,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 1.863,
      "step": 340
    },
    {
      "epoch": 0.05616625210623445,
      "grad_norm": 0.28484341502189636,
      "learning_rate": 1.2477718360071302e-05,
      "loss": 1.7311,
      "step": 350
    },
    {
      "epoch": 0.05777100216641258,
      "grad_norm": 0.18368053436279297,
      "learning_rate": 1.2834224598930482e-05,
      "loss": 1.6479,
      "step": 360
    },
    {
      "epoch": 0.05937575222659071,
      "grad_norm": 0.17929795384407043,
      "learning_rate": 1.3190730837789662e-05,
      "loss": 1.6374,
      "step": 370
    },
    {
      "epoch": 0.06098050228676884,
      "grad_norm": 0.14017082750797272,
      "learning_rate": 1.3547237076648842e-05,
      "loss": 1.7078,
      "step": 380
    },
    {
      "epoch": 0.06258525234694697,
      "grad_norm": 0.22329741716384888,
      "learning_rate": 1.3903743315508022e-05,
      "loss": 1.6731,
      "step": 390
    },
    {
      "epoch": 0.0641900024071251,
      "grad_norm": 0.14382880926132202,
      "learning_rate": 1.4260249554367203e-05,
      "loss": 1.5604,
      "step": 400
    },
    {
      "epoch": 0.06579475246730322,
      "grad_norm": 0.3925539553165436,
      "learning_rate": 1.4616755793226383e-05,
      "loss": 1.53,
      "step": 410
    },
    {
      "epoch": 0.06739950252748135,
      "grad_norm": 0.27906155586242676,
      "learning_rate": 1.4973262032085563e-05,
      "loss": 1.5849,
      "step": 420
    },
    {
      "epoch": 0.06900425258765948,
      "grad_norm": 0.18232017755508423,
      "learning_rate": 1.532976827094474e-05,
      "loss": 1.3326,
      "step": 430
    },
    {
      "epoch": 0.0706090026478376,
      "grad_norm": 0.23918840289115906,
      "learning_rate": 1.568627450980392e-05,
      "loss": 1.5074,
      "step": 440
    },
    {
      "epoch": 0.07221375270801572,
      "grad_norm": 0.2625289261341095,
      "learning_rate": 1.60427807486631e-05,
      "loss": 1.6655,
      "step": 450
    },
    {
      "epoch": 0.07381850276819385,
      "grad_norm": 0.3620847165584564,
      "learning_rate": 1.639928698752228e-05,
      "loss": 1.66,
      "step": 460
    },
    {
      "epoch": 0.07542325282837198,
      "grad_norm": 0.23106391727924347,
      "learning_rate": 1.675579322638146e-05,
      "loss": 1.4607,
      "step": 470
    },
    {
      "epoch": 0.0770280028885501,
      "grad_norm": 0.3901979327201843,
      "learning_rate": 1.7112299465240642e-05,
      "loss": 1.4881,
      "step": 480
    },
    {
      "epoch": 0.07863275294872824,
      "grad_norm": 0.23092664778232574,
      "learning_rate": 1.7468805704099822e-05,
      "loss": 1.6274,
      "step": 490
    },
    {
      "epoch": 0.08023750300890636,
      "grad_norm": 0.23557554185390472,
      "learning_rate": 1.7825311942959002e-05,
      "loss": 1.4452,
      "step": 500
    },
    {
      "epoch": 0.08184225306908449,
      "grad_norm": 0.3031589090824127,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.5062,
      "step": 510
    },
    {
      "epoch": 0.08344700312926262,
      "grad_norm": 0.3062126338481903,
      "learning_rate": 1.8538324420677362e-05,
      "loss": 1.4598,
      "step": 520
    },
    {
      "epoch": 0.08505175318944075,
      "grad_norm": 0.34174180030822754,
      "learning_rate": 1.8894830659536542e-05,
      "loss": 1.4588,
      "step": 530
    },
    {
      "epoch": 0.08665650324961888,
      "grad_norm": 0.24502912163734436,
      "learning_rate": 1.9251336898395722e-05,
      "loss": 1.5626,
      "step": 540
    },
    {
      "epoch": 0.088261253309797,
      "grad_norm": 0.2255953699350357,
      "learning_rate": 1.9607843137254903e-05,
      "loss": 1.4311,
      "step": 550
    },
    {
      "epoch": 0.08986600336997512,
      "grad_norm": 0.31927159428596497,
      "learning_rate": 1.9964349376114083e-05,
      "loss": 1.558,
      "step": 560
    },
    {
      "epoch": 0.09147075343015325,
      "grad_norm": 0.2991087734699249,
      "learning_rate": 1.9999987841968625e-05,
      "loss": 1.4276,
      "step": 570
    },
    {
      "epoch": 0.09307550349033138,
      "grad_norm": 0.21444474160671234,
      "learning_rate": 1.9999945814243798e-05,
      "loss": 1.4296,
      "step": 580
    },
    {
      "epoch": 0.0946802535505095,
      "grad_norm": 0.27173665165901184,
      "learning_rate": 1.9999873766852508e-05,
      "loss": 1.5291,
      "step": 590
    },
    {
      "epoch": 0.09628500361068763,
      "grad_norm": 0.2554818093776703,
      "learning_rate": 1.999977170001103e-05,
      "loss": 1.4119,
      "step": 600
    },
    {
      "epoch": 0.09788975367086576,
      "grad_norm": 0.5347756743431091,
      "learning_rate": 1.999963961402578e-05,
      "loss": 1.5833,
      "step": 610
    },
    {
      "epoch": 0.09949450373104389,
      "grad_norm": 0.2503623962402344,
      "learning_rate": 1.999947750929327e-05,
      "loss": 1.5966,
      "step": 620
    },
    {
      "epoch": 0.10109925379122202,
      "grad_norm": 0.24172484874725342,
      "learning_rate": 1.9999285386300132e-05,
      "loss": 1.5476,
      "step": 630
    },
    {
      "epoch": 0.10270400385140015,
      "grad_norm": 0.44592422246932983,
      "learning_rate": 1.9999063245623124e-05,
      "loss": 1.5053,
      "step": 640
    },
    {
      "epoch": 0.10430875391157828,
      "grad_norm": 0.29498621821403503,
      "learning_rate": 1.9998811087929107e-05,
      "loss": 1.5095,
      "step": 650
    },
    {
      "epoch": 0.1059135039717564,
      "grad_norm": 0.22031773626804352,
      "learning_rate": 1.999852891397505e-05,
      "loss": 1.5511,
      "step": 660
    },
    {
      "epoch": 0.10751825403193453,
      "grad_norm": 0.2750484347343445,
      "learning_rate": 1.9998216724608038e-05,
      "loss": 1.4374,
      "step": 670
    },
    {
      "epoch": 0.10912300409211266,
      "grad_norm": 0.16369891166687012,
      "learning_rate": 1.9997874520765257e-05,
      "loss": 1.4311,
      "step": 680
    },
    {
      "epoch": 0.11072775415229077,
      "grad_norm": 0.30632859468460083,
      "learning_rate": 1.9997502303473997e-05,
      "loss": 1.3998,
      "step": 690
    },
    {
      "epoch": 0.1123325042124689,
      "grad_norm": 0.2544844448566437,
      "learning_rate": 1.999710007385165e-05,
      "loss": 1.4906,
      "step": 700
    },
    {
      "epoch": 0.11393725427264703,
      "grad_norm": 0.27073752880096436,
      "learning_rate": 1.9996667833105697e-05,
      "loss": 1.5101,
      "step": 710
    },
    {
      "epoch": 0.11554200433282516,
      "grad_norm": 0.2245398908853531,
      "learning_rate": 1.9996205582533723e-05,
      "loss": 1.3774,
      "step": 720
    },
    {
      "epoch": 0.11714675439300329,
      "grad_norm": 0.19440914690494537,
      "learning_rate": 1.9995713323523395e-05,
      "loss": 1.3311,
      "step": 730
    },
    {
      "epoch": 0.11875150445318142,
      "grad_norm": 0.3126305639743805,
      "learning_rate": 1.9995191057552463e-05,
      "loss": 1.3695,
      "step": 740
    },
    {
      "epoch": 0.12035625451335955,
      "grad_norm": 0.23222993314266205,
      "learning_rate": 1.9994638786188765e-05,
      "loss": 1.2686,
      "step": 750
    },
    {
      "epoch": 0.12196100457353767,
      "grad_norm": 0.21145564317703247,
      "learning_rate": 1.9994056511090206e-05,
      "loss": 1.4826,
      "step": 760
    },
    {
      "epoch": 0.1235657546337158,
      "grad_norm": 0.3231908679008484,
      "learning_rate": 1.999344423400477e-05,
      "loss": 1.4589,
      "step": 770
    },
    {
      "epoch": 0.12517050469389393,
      "grad_norm": 0.3717208802700043,
      "learning_rate": 1.99928019567705e-05,
      "loss": 1.3377,
      "step": 780
    },
    {
      "epoch": 0.12677525475407206,
      "grad_norm": 0.43220949172973633,
      "learning_rate": 1.99921296813155e-05,
      "loss": 1.5013,
      "step": 790
    },
    {
      "epoch": 0.1283800048142502,
      "grad_norm": 0.3270035982131958,
      "learning_rate": 1.9991427409657927e-05,
      "loss": 1.3934,
      "step": 800
    },
    {
      "epoch": 0.12998475487442832,
      "grad_norm": 0.30780377984046936,
      "learning_rate": 1.9990695143906e-05,
      "loss": 1.2953,
      "step": 810
    },
    {
      "epoch": 0.13158950493460644,
      "grad_norm": 0.33227553963661194,
      "learning_rate": 1.9989932886257956e-05,
      "loss": 1.439,
      "step": 820
    },
    {
      "epoch": 0.13319425499478457,
      "grad_norm": 0.28366976976394653,
      "learning_rate": 1.9989140639002087e-05,
      "loss": 1.3491,
      "step": 830
    },
    {
      "epoch": 0.1347990050549627,
      "grad_norm": 0.26644396781921387,
      "learning_rate": 1.9988318404516704e-05,
      "loss": 1.511,
      "step": 840
    },
    {
      "epoch": 0.13640375511514083,
      "grad_norm": 0.4942858815193176,
      "learning_rate": 1.9987466185270136e-05,
      "loss": 1.357,
      "step": 850
    },
    {
      "epoch": 0.13800850517531896,
      "grad_norm": 0.3311348557472229,
      "learning_rate": 1.9986583983820735e-05,
      "loss": 1.3618,
      "step": 860
    },
    {
      "epoch": 0.13961325523549706,
      "grad_norm": 0.31968429684638977,
      "learning_rate": 1.9985671802816856e-05,
      "loss": 1.4922,
      "step": 870
    },
    {
      "epoch": 0.1412180052956752,
      "grad_norm": 0.3222774267196655,
      "learning_rate": 1.9984729644996847e-05,
      "loss": 1.4004,
      "step": 880
    },
    {
      "epoch": 0.14282275535585331,
      "grad_norm": 0.18249015510082245,
      "learning_rate": 1.9983757513189052e-05,
      "loss": 1.5577,
      "step": 890
    },
    {
      "epoch": 0.14442750541603144,
      "grad_norm": 0.42832791805267334,
      "learning_rate": 1.9982755410311797e-05,
      "loss": 1.5892,
      "step": 900
    },
    {
      "epoch": 0.14603225547620957,
      "grad_norm": 0.18397381901741028,
      "learning_rate": 1.9981723339373368e-05,
      "loss": 1.3315,
      "step": 910
    },
    {
      "epoch": 0.1476370055363877,
      "grad_norm": 0.3339751064777374,
      "learning_rate": 1.9980661303472037e-05,
      "loss": 1.5259,
      "step": 920
    },
    {
      "epoch": 0.14924175559656583,
      "grad_norm": 0.3031497001647949,
      "learning_rate": 1.9979569305796007e-05,
      "loss": 1.4001,
      "step": 930
    },
    {
      "epoch": 0.15084650565674396,
      "grad_norm": 0.405518114566803,
      "learning_rate": 1.997844734962344e-05,
      "loss": 1.3445,
      "step": 940
    },
    {
      "epoch": 0.15245125571692208,
      "grad_norm": 0.47668471932411194,
      "learning_rate": 1.9977295438322436e-05,
      "loss": 1.5786,
      "step": 950
    },
    {
      "epoch": 0.1540560057771002,
      "grad_norm": 0.3258664011955261,
      "learning_rate": 1.9976113575351e-05,
      "loss": 1.4733,
      "step": 960
    },
    {
      "epoch": 0.15566075583727834,
      "grad_norm": 0.3526577651500702,
      "learning_rate": 1.9974901764257076e-05,
      "loss": 1.4762,
      "step": 970
    },
    {
      "epoch": 0.15726550589745647,
      "grad_norm": 0.46706467866897583,
      "learning_rate": 1.9973660008678495e-05,
      "loss": 1.3709,
      "step": 980
    },
    {
      "epoch": 0.1588702559576346,
      "grad_norm": 0.21115006506443024,
      "learning_rate": 1.9972388312342985e-05,
      "loss": 1.2211,
      "step": 990
    },
    {
      "epoch": 0.16047500601781273,
      "grad_norm": 0.3447129428386688,
      "learning_rate": 1.9971086679068158e-05,
      "loss": 1.4605,
      "step": 1000
    },
    {
      "epoch": 0.16207975607799086,
      "grad_norm": 0.4207737445831299,
      "learning_rate": 1.9969755112761498e-05,
      "loss": 1.4411,
      "step": 1010
    },
    {
      "epoch": 0.16368450613816898,
      "grad_norm": 0.2579144835472107,
      "learning_rate": 1.9968393617420338e-05,
      "loss": 1.64,
      "step": 1020
    },
    {
      "epoch": 0.1652892561983471,
      "grad_norm": 0.344101220369339,
      "learning_rate": 1.996700219713187e-05,
      "loss": 1.4166,
      "step": 1030
    },
    {
      "epoch": 0.16689400625852524,
      "grad_norm": 0.23667895793914795,
      "learning_rate": 1.996558085607311e-05,
      "loss": 1.3532,
      "step": 1040
    },
    {
      "epoch": 0.16849875631870337,
      "grad_norm": 0.21985171735286713,
      "learning_rate": 1.99641295985109e-05,
      "loss": 1.243,
      "step": 1050
    },
    {
      "epoch": 0.1701035063788815,
      "grad_norm": 0.27326464653015137,
      "learning_rate": 1.996264842880189e-05,
      "loss": 1.3649,
      "step": 1060
    },
    {
      "epoch": 0.17170825643905963,
      "grad_norm": 0.4023990035057068,
      "learning_rate": 1.996113735139253e-05,
      "loss": 1.3713,
      "step": 1070
    },
    {
      "epoch": 0.17331300649923775,
      "grad_norm": 0.3184084892272949,
      "learning_rate": 1.9959596370819044e-05,
      "loss": 1.6325,
      "step": 1080
    },
    {
      "epoch": 0.17491775655941588,
      "grad_norm": 0.26201358437538147,
      "learning_rate": 1.9958025491707433e-05,
      "loss": 1.3561,
      "step": 1090
    },
    {
      "epoch": 0.176522506619594,
      "grad_norm": 0.26962459087371826,
      "learning_rate": 1.9956424718773446e-05,
      "loss": 1.3779,
      "step": 1100
    },
    {
      "epoch": 0.17812725667977214,
      "grad_norm": 0.4136592149734497,
      "learning_rate": 1.995479405682258e-05,
      "loss": 1.3318,
      "step": 1110
    },
    {
      "epoch": 0.17973200673995024,
      "grad_norm": 0.270411878824234,
      "learning_rate": 1.995313351075005e-05,
      "loss": 1.4766,
      "step": 1120
    },
    {
      "epoch": 0.18133675680012837,
      "grad_norm": 0.29305052757263184,
      "learning_rate": 1.9951443085540788e-05,
      "loss": 1.2422,
      "step": 1130
    },
    {
      "epoch": 0.1829415068603065,
      "grad_norm": 0.19487319886684418,
      "learning_rate": 1.9949722786269423e-05,
      "loss": 1.1858,
      "step": 1140
    },
    {
      "epoch": 0.18454625692048462,
      "grad_norm": 0.3168809413909912,
      "learning_rate": 1.9947972618100263e-05,
      "loss": 1.3227,
      "step": 1150
    },
    {
      "epoch": 0.18615100698066275,
      "grad_norm": 0.2984825074672699,
      "learning_rate": 1.9946192586287282e-05,
      "loss": 1.3995,
      "step": 1160
    },
    {
      "epoch": 0.18775575704084088,
      "grad_norm": 0.3263907730579376,
      "learning_rate": 1.994438269617411e-05,
      "loss": 1.3973,
      "step": 1170
    },
    {
      "epoch": 0.189360507101019,
      "grad_norm": 0.44990864396095276,
      "learning_rate": 1.9942542953193998e-05,
      "loss": 1.5862,
      "step": 1180
    },
    {
      "epoch": 0.19096525716119714,
      "grad_norm": 0.2634500563144684,
      "learning_rate": 1.994067336286983e-05,
      "loss": 1.3844,
      "step": 1190
    },
    {
      "epoch": 0.19257000722137527,
      "grad_norm": 0.24371641874313354,
      "learning_rate": 1.993877393081408e-05,
      "loss": 1.4164,
      "step": 1200
    },
    {
      "epoch": 0.1941747572815534,
      "grad_norm": 0.3149561882019043,
      "learning_rate": 1.9936844662728812e-05,
      "loss": 1.4246,
      "step": 1210
    },
    {
      "epoch": 0.19577950734173152,
      "grad_norm": 0.4185996353626251,
      "learning_rate": 1.993488556440566e-05,
      "loss": 1.4373,
      "step": 1220
    },
    {
      "epoch": 0.19738425740190965,
      "grad_norm": 0.22970063984394073,
      "learning_rate": 1.9932896641725797e-05,
      "loss": 1.3437,
      "step": 1230
    },
    {
      "epoch": 0.19898900746208778,
      "grad_norm": 0.2679067850112915,
      "learning_rate": 1.9930877900659938e-05,
      "loss": 1.3521,
      "step": 1240
    },
    {
      "epoch": 0.2005937575222659,
      "grad_norm": 0.2405499666929245,
      "learning_rate": 1.992882934726831e-05,
      "loss": 1.5418,
      "step": 1250
    },
    {
      "epoch": 0.20219850758244404,
      "grad_norm": 0.28226134181022644,
      "learning_rate": 1.992675098770063e-05,
      "loss": 1.3583,
      "step": 1260
    },
    {
      "epoch": 0.20380325764262217,
      "grad_norm": 0.2068435102701187,
      "learning_rate": 1.9924642828196107e-05,
      "loss": 1.3112,
      "step": 1270
    },
    {
      "epoch": 0.2054080077028003,
      "grad_norm": 0.40549182891845703,
      "learning_rate": 1.9922504875083394e-05,
      "loss": 1.4267,
      "step": 1280
    },
    {
      "epoch": 0.20701275776297842,
      "grad_norm": 0.38925135135650635,
      "learning_rate": 1.9920337134780588e-05,
      "loss": 1.3663,
      "step": 1290
    },
    {
      "epoch": 0.20861750782315655,
      "grad_norm": 0.3278457224369049,
      "learning_rate": 1.9918139613795217e-05,
      "loss": 1.4885,
      "step": 1300
    },
    {
      "epoch": 0.21022225788333468,
      "grad_norm": 0.25548163056373596,
      "learning_rate": 1.991591231872419e-05,
      "loss": 1.3177,
      "step": 1310
    },
    {
      "epoch": 0.2118270079435128,
      "grad_norm": 0.2191590666770935,
      "learning_rate": 1.9913655256253815e-05,
      "loss": 1.3191,
      "step": 1320
    },
    {
      "epoch": 0.21343175800369094,
      "grad_norm": 0.3952718675136566,
      "learning_rate": 1.9911368433159754e-05,
      "loss": 1.4215,
      "step": 1330
    },
    {
      "epoch": 0.21503650806386906,
      "grad_norm": 0.40167179703712463,
      "learning_rate": 1.990905185630701e-05,
      "loss": 1.5291,
      "step": 1340
    },
    {
      "epoch": 0.2166412581240472,
      "grad_norm": 0.44336745142936707,
      "learning_rate": 1.990670553264991e-05,
      "loss": 1.37,
      "step": 1350
    },
    {
      "epoch": 0.21824600818422532,
      "grad_norm": 0.4026157557964325,
      "learning_rate": 1.9904329469232076e-05,
      "loss": 1.4131,
      "step": 1360
    },
    {
      "epoch": 0.21985075824440342,
      "grad_norm": 0.33255305886268616,
      "learning_rate": 1.990192367318641e-05,
      "loss": 1.4972,
      "step": 1370
    },
    {
      "epoch": 0.22145550830458155,
      "grad_norm": 0.28011998534202576,
      "learning_rate": 1.989948815173507e-05,
      "loss": 1.3647,
      "step": 1380
    },
    {
      "epoch": 0.22306025836475968,
      "grad_norm": 0.23420965671539307,
      "learning_rate": 1.9897022912189445e-05,
      "loss": 1.3953,
      "step": 1390
    },
    {
      "epoch": 0.2246650084249378,
      "grad_norm": 0.386967271566391,
      "learning_rate": 1.989452796195015e-05,
      "loss": 1.3783,
      "step": 1400
    },
    {
      "epoch": 0.22626975848511593,
      "grad_norm": 0.26505544781684875,
      "learning_rate": 1.989200330850698e-05,
      "loss": 1.4478,
      "step": 1410
    },
    {
      "epoch": 0.22787450854529406,
      "grad_norm": 0.22895698249340057,
      "learning_rate": 1.9889448959438903e-05,
      "loss": 1.4864,
      "step": 1420
    },
    {
      "epoch": 0.2294792586054722,
      "grad_norm": 0.27550992369651794,
      "learning_rate": 1.988686492241403e-05,
      "loss": 1.4158,
      "step": 1430
    },
    {
      "epoch": 0.23108400866565032,
      "grad_norm": 0.4306904375553131,
      "learning_rate": 1.9884251205189593e-05,
      "loss": 1.3682,
      "step": 1440
    },
    {
      "epoch": 0.23268875872582845,
      "grad_norm": 0.21296507120132446,
      "learning_rate": 1.9881607815611927e-05,
      "loss": 1.3884,
      "step": 1450
    },
    {
      "epoch": 0.23429350878600658,
      "grad_norm": 0.24282608926296234,
      "learning_rate": 1.9878934761616447e-05,
      "loss": 1.2864,
      "step": 1460
    },
    {
      "epoch": 0.2358982588461847,
      "grad_norm": 0.36954039335250854,
      "learning_rate": 1.9876232051227613e-05,
      "loss": 1.4983,
      "step": 1470
    },
    {
      "epoch": 0.23750300890636283,
      "grad_norm": 0.34447595477104187,
      "learning_rate": 1.9873499692558914e-05,
      "loss": 1.4084,
      "step": 1480
    },
    {
      "epoch": 0.23910775896654096,
      "grad_norm": 0.423593133687973,
      "learning_rate": 1.9870737693812846e-05,
      "loss": 1.35,
      "step": 1490
    },
    {
      "epoch": 0.2407125090267191,
      "grad_norm": 0.38388440012931824,
      "learning_rate": 1.9867946063280882e-05,
      "loss": 1.389,
      "step": 1500
    },
    {
      "epoch": 0.24231725908689722,
      "grad_norm": 0.3835993707180023,
      "learning_rate": 1.9865124809343447e-05,
      "loss": 1.2998,
      "step": 1510
    },
    {
      "epoch": 0.24392200914707535,
      "grad_norm": 0.6537964344024658,
      "learning_rate": 1.9862273940469898e-05,
      "loss": 1.3418,
      "step": 1520
    },
    {
      "epoch": 0.24552675920725348,
      "grad_norm": 0.29857540130615234,
      "learning_rate": 1.9859393465218497e-05,
      "loss": 1.4264,
      "step": 1530
    },
    {
      "epoch": 0.2471315092674316,
      "grad_norm": 0.36309733986854553,
      "learning_rate": 1.985648339223638e-05,
      "loss": 1.4351,
      "step": 1540
    },
    {
      "epoch": 0.24873625932760973,
      "grad_norm": 0.30575835704803467,
      "learning_rate": 1.9853543730259535e-05,
      "loss": 1.4307,
      "step": 1550
    },
    {
      "epoch": 0.25034100938778786,
      "grad_norm": 0.286000519990921,
      "learning_rate": 1.9850574488112776e-05,
      "loss": 1.3838,
      "step": 1560
    },
    {
      "epoch": 0.251945759447966,
      "grad_norm": 0.2919764220714569,
      "learning_rate": 1.9847575674709723e-05,
      "loss": 1.3995,
      "step": 1570
    },
    {
      "epoch": 0.2535505095081441,
      "grad_norm": 0.30116719007492065,
      "learning_rate": 1.9844547299052756e-05,
      "loss": 1.3908,
      "step": 1580
    },
    {
      "epoch": 0.25515525956832225,
      "grad_norm": 0.3683610260486603,
      "learning_rate": 1.9841489370233012e-05,
      "loss": 1.3468,
      "step": 1590
    },
    {
      "epoch": 0.2567600096285004,
      "grad_norm": 0.32669150829315186,
      "learning_rate": 1.9838401897430336e-05,
      "loss": 1.2323,
      "step": 1600
    },
    {
      "epoch": 0.2583647596886785,
      "grad_norm": 0.3483133316040039,
      "learning_rate": 1.9835284889913275e-05,
      "loss": 1.3231,
      "step": 1610
    },
    {
      "epoch": 0.25996950974885663,
      "grad_norm": 0.3334028422832489,
      "learning_rate": 1.9832138357039024e-05,
      "loss": 1.3346,
      "step": 1620
    },
    {
      "epoch": 0.26157425980903476,
      "grad_norm": 0.2740734815597534,
      "learning_rate": 1.982896230825343e-05,
      "loss": 1.275,
      "step": 1630
    },
    {
      "epoch": 0.2631790098692129,
      "grad_norm": 0.21739724278450012,
      "learning_rate": 1.982575675309093e-05,
      "loss": 1.3771,
      "step": 1640
    },
    {
      "epoch": 0.264783759929391,
      "grad_norm": 0.3791095018386841,
      "learning_rate": 1.982252170117455e-05,
      "loss": 1.3938,
      "step": 1650
    },
    {
      "epoch": 0.26638850998956914,
      "grad_norm": 0.4676331579685211,
      "learning_rate": 1.981925716221586e-05,
      "loss": 1.4098,
      "step": 1660
    },
    {
      "epoch": 0.2679932600497473,
      "grad_norm": 0.42060697078704834,
      "learning_rate": 1.9815963146014948e-05,
      "loss": 1.489,
      "step": 1670
    },
    {
      "epoch": 0.2695980101099254,
      "grad_norm": 0.3207416534423828,
      "learning_rate": 1.9812639662460396e-05,
      "loss": 1.5135,
      "step": 1680
    },
    {
      "epoch": 0.27120276017010353,
      "grad_norm": 0.19730353355407715,
      "learning_rate": 1.9809286721529246e-05,
      "loss": 1.5137,
      "step": 1690
    },
    {
      "epoch": 0.27280751023028166,
      "grad_norm": 0.22617162764072418,
      "learning_rate": 1.980590433328697e-05,
      "loss": 1.4917,
      "step": 1700
    },
    {
      "epoch": 0.2744122602904598,
      "grad_norm": 0.3663794994354248,
      "learning_rate": 1.9802492507887434e-05,
      "loss": 1.3597,
      "step": 1710
    },
    {
      "epoch": 0.2760170103506379,
      "grad_norm": 0.3326999843120575,
      "learning_rate": 1.9799051255572884e-05,
      "loss": 1.2681,
      "step": 1720
    },
    {
      "epoch": 0.27762176041081604,
      "grad_norm": 0.3930642008781433,
      "learning_rate": 1.97955805866739e-05,
      "loss": 1.501,
      "step": 1730
    },
    {
      "epoch": 0.2792265104709941,
      "grad_norm": 0.34715524315834045,
      "learning_rate": 1.9792080511609372e-05,
      "loss": 1.3983,
      "step": 1740
    },
    {
      "epoch": 0.28083126053117224,
      "grad_norm": 0.36794325709342957,
      "learning_rate": 1.9788551040886465e-05,
      "loss": 1.378,
      "step": 1750
    },
    {
      "epoch": 0.2824360105913504,
      "grad_norm": 0.2995903193950653,
      "learning_rate": 1.9784992185100585e-05,
      "loss": 1.4939,
      "step": 1760
    },
    {
      "epoch": 0.2840407606515285,
      "grad_norm": 0.2723531723022461,
      "learning_rate": 1.9781403954935365e-05,
      "loss": 1.4785,
      "step": 1770
    },
    {
      "epoch": 0.28564551071170663,
      "grad_norm": 0.3530333638191223,
      "learning_rate": 1.97777863611626e-05,
      "loss": 1.4255,
      "step": 1780
    },
    {
      "epoch": 0.28725026077188476,
      "grad_norm": 0.3557887077331543,
      "learning_rate": 1.9774139414642256e-05,
      "loss": 1.381,
      "step": 1790
    },
    {
      "epoch": 0.2888550108320629,
      "grad_norm": 0.3029628098011017,
      "learning_rate": 1.9770463126322396e-05,
      "loss": 1.3856,
      "step": 1800
    },
    {
      "epoch": 0.290459760892241,
      "grad_norm": 0.30022427439689636,
      "learning_rate": 1.976675750723918e-05,
      "loss": 1.2598,
      "step": 1810
    },
    {
      "epoch": 0.29206451095241914,
      "grad_norm": 0.25942111015319824,
      "learning_rate": 1.976302256851681e-05,
      "loss": 1.369,
      "step": 1820
    },
    {
      "epoch": 0.29366926101259727,
      "grad_norm": 0.39943525195121765,
      "learning_rate": 1.9759258321367506e-05,
      "loss": 1.2573,
      "step": 1830
    },
    {
      "epoch": 0.2952740110727754,
      "grad_norm": 0.28279179334640503,
      "learning_rate": 1.9755464777091477e-05,
      "loss": 1.4102,
      "step": 1840
    },
    {
      "epoch": 0.29687876113295353,
      "grad_norm": 0.31728804111480713,
      "learning_rate": 1.9751641947076877e-05,
      "loss": 1.3843,
      "step": 1850
    },
    {
      "epoch": 0.29848351119313166,
      "grad_norm": 0.39161691069602966,
      "learning_rate": 1.974778984279978e-05,
      "loss": 1.3455,
      "step": 1860
    },
    {
      "epoch": 0.3000882612533098,
      "grad_norm": 0.40414389967918396,
      "learning_rate": 1.9743908475824134e-05,
      "loss": 1.3299,
      "step": 1870
    },
    {
      "epoch": 0.3016930113134879,
      "grad_norm": 0.3219362497329712,
      "learning_rate": 1.9739997857801737e-05,
      "loss": 1.3351,
      "step": 1880
    },
    {
      "epoch": 0.30329776137366604,
      "grad_norm": 0.4149847626686096,
      "learning_rate": 1.9736058000472195e-05,
      "loss": 1.2271,
      "step": 1890
    },
    {
      "epoch": 0.30490251143384417,
      "grad_norm": 0.2929013967514038,
      "learning_rate": 1.9732088915662895e-05,
      "loss": 1.2845,
      "step": 1900
    },
    {
      "epoch": 0.3065072614940223,
      "grad_norm": 0.23451751470565796,
      "learning_rate": 1.972809061528896e-05,
      "loss": 1.5366,
      "step": 1910
    },
    {
      "epoch": 0.3081120115542004,
      "grad_norm": 0.46130135655403137,
      "learning_rate": 1.972406311135322e-05,
      "loss": 1.338,
      "step": 1920
    },
    {
      "epoch": 0.30971676161437856,
      "grad_norm": 0.2804974615573883,
      "learning_rate": 1.9720006415946175e-05,
      "loss": 1.4128,
      "step": 1930
    },
    {
      "epoch": 0.3113215116745567,
      "grad_norm": 0.36125385761260986,
      "learning_rate": 1.9715920541245956e-05,
      "loss": 1.3309,
      "step": 1940
    },
    {
      "epoch": 0.3129262617347348,
      "grad_norm": 0.40560418367385864,
      "learning_rate": 1.9711805499518287e-05,
      "loss": 1.5499,
      "step": 1950
    },
    {
      "epoch": 0.31453101179491294,
      "grad_norm": 0.17646387219429016,
      "learning_rate": 1.970766130311645e-05,
      "loss": 1.5559,
      "step": 1960
    },
    {
      "epoch": 0.31613576185509107,
      "grad_norm": 0.3691346049308777,
      "learning_rate": 1.9703487964481255e-05,
      "loss": 1.3178,
      "step": 1970
    },
    {
      "epoch": 0.3177405119152692,
      "grad_norm": 0.2070036679506302,
      "learning_rate": 1.9699285496140992e-05,
      "loss": 1.3373,
      "step": 1980
    },
    {
      "epoch": 0.3193452619754473,
      "grad_norm": 0.5338124632835388,
      "learning_rate": 1.9695053910711402e-05,
      "loss": 1.4328,
      "step": 1990
    },
    {
      "epoch": 0.32095001203562545,
      "grad_norm": 0.2903488874435425,
      "learning_rate": 1.969079322089563e-05,
      "loss": 1.4666,
      "step": 2000
    },
    {
      "epoch": 0.3225547620958036,
      "grad_norm": 0.3406471312046051,
      "learning_rate": 1.9686503439484193e-05,
      "loss": 1.522,
      "step": 2010
    },
    {
      "epoch": 0.3241595121559817,
      "grad_norm": 0.3346569836139679,
      "learning_rate": 1.9682184579354943e-05,
      "loss": 1.2533,
      "step": 2020
    },
    {
      "epoch": 0.32576426221615984,
      "grad_norm": 0.467851847410202,
      "learning_rate": 1.9677836653473025e-05,
      "loss": 1.2884,
      "step": 2030
    },
    {
      "epoch": 0.32736901227633797,
      "grad_norm": 0.25201156735420227,
      "learning_rate": 1.967345967489084e-05,
      "loss": 1.2345,
      "step": 2040
    },
    {
      "epoch": 0.3289737623365161,
      "grad_norm": 0.292625367641449,
      "learning_rate": 1.9669053656747998e-05,
      "loss": 1.2452,
      "step": 2050
    },
    {
      "epoch": 0.3305785123966942,
      "grad_norm": 0.3019048571586609,
      "learning_rate": 1.9664618612271292e-05,
      "loss": 1.4723,
      "step": 2060
    },
    {
      "epoch": 0.33218326245687235,
      "grad_norm": 0.28457310795783997,
      "learning_rate": 1.966015455477465e-05,
      "loss": 1.5676,
      "step": 2070
    },
    {
      "epoch": 0.3337880125170505,
      "grad_norm": 0.2934150695800781,
      "learning_rate": 1.9655661497659096e-05,
      "loss": 1.5,
      "step": 2080
    },
    {
      "epoch": 0.3353927625772286,
      "grad_norm": 0.2713722884654999,
      "learning_rate": 1.9651139454412707e-05,
      "loss": 1.3849,
      "step": 2090
    },
    {
      "epoch": 0.33699751263740674,
      "grad_norm": 0.35259222984313965,
      "learning_rate": 1.964658843861059e-05,
      "loss": 1.3197,
      "step": 2100
    },
    {
      "epoch": 0.33860226269758487,
      "grad_norm": 0.23005594313144684,
      "learning_rate": 1.9642008463914807e-05,
      "loss": 1.3221,
      "step": 2110
    },
    {
      "epoch": 0.340207012757763,
      "grad_norm": 0.30540376901626587,
      "learning_rate": 1.9637399544074368e-05,
      "loss": 1.4508,
      "step": 2120
    },
    {
      "epoch": 0.3418117628179411,
      "grad_norm": 0.40802201628685,
      "learning_rate": 1.963276169292517e-05,
      "loss": 1.3976,
      "step": 2130
    },
    {
      "epoch": 0.34341651287811925,
      "grad_norm": 0.42711758613586426,
      "learning_rate": 1.962809492438997e-05,
      "loss": 1.3909,
      "step": 2140
    },
    {
      "epoch": 0.3450212629382974,
      "grad_norm": 0.26663872599601746,
      "learning_rate": 1.9623399252478316e-05,
      "loss": 1.3637,
      "step": 2150
    },
    {
      "epoch": 0.3466260129984755,
      "grad_norm": 0.49782994389533997,
      "learning_rate": 1.9618674691286537e-05,
      "loss": 1.3991,
      "step": 2160
    },
    {
      "epoch": 0.34823076305865364,
      "grad_norm": 0.34774404764175415,
      "learning_rate": 1.961392125499769e-05,
      "loss": 1.4745,
      "step": 2170
    },
    {
      "epoch": 0.34983551311883176,
      "grad_norm": 0.4350658059120178,
      "learning_rate": 1.960913895788151e-05,
      "loss": 1.3528,
      "step": 2180
    },
    {
      "epoch": 0.3514402631790099,
      "grad_norm": 0.30385681986808777,
      "learning_rate": 1.9604327814294363e-05,
      "loss": 1.2378,
      "step": 2190
    },
    {
      "epoch": 0.353045013239188,
      "grad_norm": 0.2511212229728699,
      "learning_rate": 1.959948783867923e-05,
      "loss": 1.2355,
      "step": 2200
    },
    {
      "epoch": 0.35464976329936615,
      "grad_norm": 0.30395057797431946,
      "learning_rate": 1.959461904556563e-05,
      "loss": 1.2358,
      "step": 2210
    },
    {
      "epoch": 0.3562545133595443,
      "grad_norm": 0.457220196723938,
      "learning_rate": 1.9589721449569595e-05,
      "loss": 1.2964,
      "step": 2220
    },
    {
      "epoch": 0.35785926341972235,
      "grad_norm": 0.272195965051651,
      "learning_rate": 1.9584795065393633e-05,
      "loss": 1.2718,
      "step": 2230
    },
    {
      "epoch": 0.3594640134799005,
      "grad_norm": 0.3350317180156708,
      "learning_rate": 1.9579839907826655e-05,
      "loss": 1.2571,
      "step": 2240
    },
    {
      "epoch": 0.3610687635400786,
      "grad_norm": 0.351938933134079,
      "learning_rate": 1.957485599174396e-05,
      "loss": 1.3015,
      "step": 2250
    },
    {
      "epoch": 0.36267351360025674,
      "grad_norm": 0.37047067284584045,
      "learning_rate": 1.9569843332107186e-05,
      "loss": 1.2551,
      "step": 2260
    },
    {
      "epoch": 0.36427826366043486,
      "grad_norm": 0.2948170304298401,
      "learning_rate": 1.9564801943964246e-05,
      "loss": 1.3673,
      "step": 2270
    },
    {
      "epoch": 0.365883013720613,
      "grad_norm": 0.4895157516002655,
      "learning_rate": 1.9559731842449303e-05,
      "loss": 1.4032,
      "step": 2280
    },
    {
      "epoch": 0.3674877637807911,
      "grad_norm": 0.29684463143348694,
      "learning_rate": 1.9554633042782717e-05,
      "loss": 1.356,
      "step": 2290
    },
    {
      "epoch": 0.36909251384096925,
      "grad_norm": 0.3872864246368408,
      "learning_rate": 1.9549505560270993e-05,
      "loss": 1.3124,
      "step": 2300
    },
    {
      "epoch": 0.3706972639011474,
      "grad_norm": 0.39776426553726196,
      "learning_rate": 1.954434941030675e-05,
      "loss": 1.3806,
      "step": 2310
    },
    {
      "epoch": 0.3723020139613255,
      "grad_norm": 0.2617858648300171,
      "learning_rate": 1.9539164608368657e-05,
      "loss": 1.3603,
      "step": 2320
    },
    {
      "epoch": 0.37390676402150363,
      "grad_norm": 0.38514503836631775,
      "learning_rate": 1.953395117002141e-05,
      "loss": 1.4355,
      "step": 2330
    },
    {
      "epoch": 0.37551151408168176,
      "grad_norm": 0.30052679777145386,
      "learning_rate": 1.952870911091565e-05,
      "loss": 1.3452,
      "step": 2340
    },
    {
      "epoch": 0.3771162641418599,
      "grad_norm": 0.3276200592517853,
      "learning_rate": 1.952343844678796e-05,
      "loss": 1.3827,
      "step": 2350
    },
    {
      "epoch": 0.378721014202038,
      "grad_norm": 0.2065112441778183,
      "learning_rate": 1.9518139193460775e-05,
      "loss": 1.218,
      "step": 2360
    },
    {
      "epoch": 0.38032576426221615,
      "grad_norm": 0.3966623544692993,
      "learning_rate": 1.9512811366842367e-05,
      "loss": 1.4419,
      "step": 2370
    },
    {
      "epoch": 0.3819305143223943,
      "grad_norm": 0.3066747784614563,
      "learning_rate": 1.950745498292678e-05,
      "loss": 1.6141,
      "step": 2380
    },
    {
      "epoch": 0.3835352643825724,
      "grad_norm": 0.4463857412338257,
      "learning_rate": 1.950207005779379e-05,
      "loss": 1.4721,
      "step": 2390
    },
    {
      "epoch": 0.38514001444275053,
      "grad_norm": 0.41368722915649414,
      "learning_rate": 1.9496656607608845e-05,
      "loss": 1.3954,
      "step": 2400
    },
    {
      "epoch": 0.38674476450292866,
      "grad_norm": 0.3056279718875885,
      "learning_rate": 1.949121464862303e-05,
      "loss": 1.3686,
      "step": 2410
    },
    {
      "epoch": 0.3883495145631068,
      "grad_norm": 0.5210683941841125,
      "learning_rate": 1.9485744197173016e-05,
      "loss": 1.5396,
      "step": 2420
    },
    {
      "epoch": 0.3899542646232849,
      "grad_norm": 0.35678398609161377,
      "learning_rate": 1.9480245269681006e-05,
      "loss": 1.2616,
      "step": 2430
    },
    {
      "epoch": 0.39155901468346305,
      "grad_norm": 0.3954680263996124,
      "learning_rate": 1.9474717882654682e-05,
      "loss": 1.2884,
      "step": 2440
    },
    {
      "epoch": 0.3931637647436412,
      "grad_norm": 0.412992924451828,
      "learning_rate": 1.9469162052687166e-05,
      "loss": 1.5022,
      "step": 2450
    },
    {
      "epoch": 0.3947685148038193,
      "grad_norm": 0.43660473823547363,
      "learning_rate": 1.946357779645697e-05,
      "loss": 1.1959,
      "step": 2460
    },
    {
      "epoch": 0.39637326486399743,
      "grad_norm": 0.35781991481781006,
      "learning_rate": 1.945796513072793e-05,
      "loss": 1.2859,
      "step": 2470
    },
    {
      "epoch": 0.39797801492417556,
      "grad_norm": 0.3486938774585724,
      "learning_rate": 1.945232407234918e-05,
      "loss": 1.3424,
      "step": 2480
    },
    {
      "epoch": 0.3995827649843537,
      "grad_norm": 0.41157066822052,
      "learning_rate": 1.9446654638255066e-05,
      "loss": 1.3515,
      "step": 2490
    },
    {
      "epoch": 0.4011875150445318,
      "grad_norm": 0.2521686851978302,
      "learning_rate": 1.944095684546515e-05,
      "loss": 1.2965,
      "step": 2500
    },
    {
      "epoch": 0.40279226510470995,
      "grad_norm": 0.3465510308742523,
      "learning_rate": 1.9435230711084093e-05,
      "loss": 1.3103,
      "step": 2510
    },
    {
      "epoch": 0.4043970151648881,
      "grad_norm": 0.4512653350830078,
      "learning_rate": 1.9429476252301667e-05,
      "loss": 1.3535,
      "step": 2520
    },
    {
      "epoch": 0.4060017652250662,
      "grad_norm": 0.22823373973369598,
      "learning_rate": 1.942369348639265e-05,
      "loss": 1.2815,
      "step": 2530
    },
    {
      "epoch": 0.40760651528524433,
      "grad_norm": 0.2904421389102936,
      "learning_rate": 1.9417882430716806e-05,
      "loss": 1.274,
      "step": 2540
    },
    {
      "epoch": 0.40921126534542246,
      "grad_norm": 0.22718197107315063,
      "learning_rate": 1.9412043102718826e-05,
      "loss": 1.3225,
      "step": 2550
    },
    {
      "epoch": 0.4108160154056006,
      "grad_norm": 0.36589524149894714,
      "learning_rate": 1.9406175519928277e-05,
      "loss": 1.3205,
      "step": 2560
    },
    {
      "epoch": 0.4124207654657787,
      "grad_norm": 0.2226024568080902,
      "learning_rate": 1.940027969995954e-05,
      "loss": 1.3326,
      "step": 2570
    },
    {
      "epoch": 0.41402551552595684,
      "grad_norm": 0.32647380232810974,
      "learning_rate": 1.9394355660511764e-05,
      "loss": 1.3379,
      "step": 2580
    },
    {
      "epoch": 0.415630265586135,
      "grad_norm": 0.4311791658401489,
      "learning_rate": 1.9388403419368815e-05,
      "loss": 1.3571,
      "step": 2590
    },
    {
      "epoch": 0.4172350156463131,
      "grad_norm": 0.3705790638923645,
      "learning_rate": 1.938242299439922e-05,
      "loss": 1.3533,
      "step": 2600
    },
    {
      "epoch": 0.41883976570649123,
      "grad_norm": 0.243331179022789,
      "learning_rate": 1.937641440355611e-05,
      "loss": 1.4036,
      "step": 2610
    },
    {
      "epoch": 0.42044451576666936,
      "grad_norm": 0.4500161111354828,
      "learning_rate": 1.9370377664877174e-05,
      "loss": 1.4308,
      "step": 2620
    },
    {
      "epoch": 0.4220492658268475,
      "grad_norm": 0.3864496052265167,
      "learning_rate": 1.9364312796484602e-05,
      "loss": 1.3605,
      "step": 2630
    },
    {
      "epoch": 0.4236540158870256,
      "grad_norm": 0.33515220880508423,
      "learning_rate": 1.9358219816585022e-05,
      "loss": 1.3077,
      "step": 2640
    },
    {
      "epoch": 0.42525876594720374,
      "grad_norm": 0.5244264006614685,
      "learning_rate": 1.9352098743469453e-05,
      "loss": 1.2898,
      "step": 2650
    },
    {
      "epoch": 0.42686351600738187,
      "grad_norm": 0.31695449352264404,
      "learning_rate": 1.9345949595513257e-05,
      "loss": 1.4166,
      "step": 2660
    },
    {
      "epoch": 0.42846826606756,
      "grad_norm": 0.30906620621681213,
      "learning_rate": 1.9339772391176065e-05,
      "loss": 1.5144,
      "step": 2670
    },
    {
      "epoch": 0.43007301612773813,
      "grad_norm": 0.39018145203590393,
      "learning_rate": 1.9333567149001742e-05,
      "loss": 1.4156,
      "step": 2680
    },
    {
      "epoch": 0.43167776618791626,
      "grad_norm": 0.5267777442932129,
      "learning_rate": 1.932733388761832e-05,
      "loss": 1.4698,
      "step": 2690
    },
    {
      "epoch": 0.4332825162480944,
      "grad_norm": 0.4009230434894562,
      "learning_rate": 1.9321072625737943e-05,
      "loss": 1.3226,
      "step": 2700
    },
    {
      "epoch": 0.4348872663082725,
      "grad_norm": 0.3111720383167267,
      "learning_rate": 1.931478338215681e-05,
      "loss": 1.4184,
      "step": 2710
    },
    {
      "epoch": 0.43649201636845064,
      "grad_norm": 0.31122878193855286,
      "learning_rate": 1.930846617575513e-05,
      "loss": 1.2398,
      "step": 2720
    },
    {
      "epoch": 0.4380967664286287,
      "grad_norm": 0.40437036752700806,
      "learning_rate": 1.9302121025497037e-05,
      "loss": 1.3129,
      "step": 2730
    },
    {
      "epoch": 0.43970151648880684,
      "grad_norm": 0.3509606420993805,
      "learning_rate": 1.9295747950430575e-05,
      "loss": 1.3335,
      "step": 2740
    },
    {
      "epoch": 0.44130626654898497,
      "grad_norm": 0.3115660548210144,
      "learning_rate": 1.9289346969687598e-05,
      "loss": 1.3304,
      "step": 2750
    },
    {
      "epoch": 0.4429110166091631,
      "grad_norm": 0.30947375297546387,
      "learning_rate": 1.928291810248374e-05,
      "loss": 1.2904,
      "step": 2760
    },
    {
      "epoch": 0.44451576666934123,
      "grad_norm": 0.3383250832557678,
      "learning_rate": 1.927646136811836e-05,
      "loss": 1.3329,
      "step": 2770
    },
    {
      "epoch": 0.44612051672951936,
      "grad_norm": 0.4008086323738098,
      "learning_rate": 1.9269976785974456e-05,
      "loss": 1.364,
      "step": 2780
    },
    {
      "epoch": 0.4477252667896975,
      "grad_norm": 0.28436827659606934,
      "learning_rate": 1.9263464375518634e-05,
      "loss": 1.3141,
      "step": 2790
    },
    {
      "epoch": 0.4493300168498756,
      "grad_norm": 0.40010377764701843,
      "learning_rate": 1.9256924156301046e-05,
      "loss": 1.332,
      "step": 2800
    },
    {
      "epoch": 0.45093476691005374,
      "grad_norm": 0.37946265935897827,
      "learning_rate": 1.9250356147955308e-05,
      "loss": 1.2775,
      "step": 2810
    },
    {
      "epoch": 0.45253951697023187,
      "grad_norm": 0.3174775540828705,
      "learning_rate": 1.9243760370198475e-05,
      "loss": 1.4392,
      "step": 2820
    },
    {
      "epoch": 0.45414426703041,
      "grad_norm": 0.3594072461128235,
      "learning_rate": 1.9237136842830953e-05,
      "loss": 1.4264,
      "step": 2830
    },
    {
      "epoch": 0.4557490170905881,
      "grad_norm": 0.32715851068496704,
      "learning_rate": 1.923048558573647e-05,
      "loss": 1.3317,
      "step": 2840
    },
    {
      "epoch": 0.45735376715076625,
      "grad_norm": 0.30426010489463806,
      "learning_rate": 1.9223806618881974e-05,
      "loss": 1.2423,
      "step": 2850
    },
    {
      "epoch": 0.4589585172109444,
      "grad_norm": 0.31703969836235046,
      "learning_rate": 1.9217099962317614e-05,
      "loss": 1.199,
      "step": 2860
    },
    {
      "epoch": 0.4605632672711225,
      "grad_norm": 0.2574061155319214,
      "learning_rate": 1.9210365636176664e-05,
      "loss": 1.458,
      "step": 2870
    },
    {
      "epoch": 0.46216801733130064,
      "grad_norm": 0.3669300079345703,
      "learning_rate": 1.9203603660675445e-05,
      "loss": 1.1514,
      "step": 2880
    },
    {
      "epoch": 0.46377276739147877,
      "grad_norm": 0.5132482647895813,
      "learning_rate": 1.91968140561133e-05,
      "loss": 1.3781,
      "step": 2890
    },
    {
      "epoch": 0.4653775174516569,
      "grad_norm": 0.4987383186817169,
      "learning_rate": 1.9189996842872504e-05,
      "loss": 1.2753,
      "step": 2900
    },
    {
      "epoch": 0.466982267511835,
      "grad_norm": 0.6567516922950745,
      "learning_rate": 1.9183152041418212e-05,
      "loss": 1.331,
      "step": 2910
    },
    {
      "epoch": 0.46858701757201315,
      "grad_norm": 0.457297146320343,
      "learning_rate": 1.9176279672298403e-05,
      "loss": 1.2382,
      "step": 2920
    },
    {
      "epoch": 0.4701917676321913,
      "grad_norm": 0.3900405764579773,
      "learning_rate": 1.9169379756143814e-05,
      "loss": 1.3317,
      "step": 2930
    },
    {
      "epoch": 0.4717965176923694,
      "grad_norm": 0.46646952629089355,
      "learning_rate": 1.916245231366787e-05,
      "loss": 1.1852,
      "step": 2940
    },
    {
      "epoch": 0.47340126775254754,
      "grad_norm": 0.2655135691165924,
      "learning_rate": 1.9155497365666642e-05,
      "loss": 1.2231,
      "step": 2950
    },
    {
      "epoch": 0.47500601781272567,
      "grad_norm": 0.4266279637813568,
      "learning_rate": 1.9148514933018758e-05,
      "loss": 1.4414,
      "step": 2960
    },
    {
      "epoch": 0.4766107678729038,
      "grad_norm": 0.4702107906341553,
      "learning_rate": 1.9141505036685366e-05,
      "loss": 1.3614,
      "step": 2970
    },
    {
      "epoch": 0.4782155179330819,
      "grad_norm": 0.3613738715648651,
      "learning_rate": 1.9134467697710052e-05,
      "loss": 1.3396,
      "step": 2980
    },
    {
      "epoch": 0.47982026799326005,
      "grad_norm": 0.3401474952697754,
      "learning_rate": 1.912740293721879e-05,
      "loss": 1.5286,
      "step": 2990
    },
    {
      "epoch": 0.4814250180534382,
      "grad_norm": 0.3083174526691437,
      "learning_rate": 1.912031077641987e-05,
      "loss": 1.2264,
      "step": 3000
    },
    {
      "epoch": 0.4830297681136163,
      "grad_norm": 0.5213725566864014,
      "learning_rate": 1.9113191236603835e-05,
      "loss": 1.36,
      "step": 3010
    },
    {
      "epoch": 0.48463451817379444,
      "grad_norm": 0.3317253589630127,
      "learning_rate": 1.9106044339143427e-05,
      "loss": 1.4199,
      "step": 3020
    },
    {
      "epoch": 0.48623926823397257,
      "grad_norm": 0.548594057559967,
      "learning_rate": 1.9098870105493504e-05,
      "loss": 1.2182,
      "step": 3030
    },
    {
      "epoch": 0.4878440182941507,
      "grad_norm": 0.40180525183677673,
      "learning_rate": 1.9091668557190996e-05,
      "loss": 1.2334,
      "step": 3040
    },
    {
      "epoch": 0.4894487683543288,
      "grad_norm": 0.3728240430355072,
      "learning_rate": 1.9084439715854828e-05,
      "loss": 1.483,
      "step": 3050
    },
    {
      "epoch": 0.49105351841450695,
      "grad_norm": 0.30843567848205566,
      "learning_rate": 1.9077183603185858e-05,
      "loss": 1.5069,
      "step": 3060
    },
    {
      "epoch": 0.4926582684746851,
      "grad_norm": 0.3186185657978058,
      "learning_rate": 1.906990024096681e-05,
      "loss": 1.2054,
      "step": 3070
    },
    {
      "epoch": 0.4942630185348632,
      "grad_norm": 0.4824373424053192,
      "learning_rate": 1.9062589651062215e-05,
      "loss": 1.4438,
      "step": 3080
    },
    {
      "epoch": 0.49586776859504134,
      "grad_norm": 0.2819381356239319,
      "learning_rate": 1.9055251855418343e-05,
      "loss": 1.2917,
      "step": 3090
    },
    {
      "epoch": 0.49747251865521946,
      "grad_norm": 0.34851837158203125,
      "learning_rate": 1.9047886876063124e-05,
      "loss": 1.5048,
      "step": 3100
    },
    {
      "epoch": 0.4990772687153976,
      "grad_norm": 0.3761436641216278,
      "learning_rate": 1.90404947351061e-05,
      "loss": 1.2536,
      "step": 3110
    },
    {
      "epoch": 0.5006820187755757,
      "grad_norm": 0.27981412410736084,
      "learning_rate": 1.903307545473836e-05,
      "loss": 1.2497,
      "step": 3120
    },
    {
      "epoch": 0.5022867688357538,
      "grad_norm": 0.36180564761161804,
      "learning_rate": 1.902562905723245e-05,
      "loss": 1.3459,
      "step": 3130
    },
    {
      "epoch": 0.503891518895932,
      "grad_norm": 0.36484453082084656,
      "learning_rate": 1.901815556494233e-05,
      "loss": 1.4141,
      "step": 3140
    },
    {
      "epoch": 0.5054962689561101,
      "grad_norm": 0.23243507742881775,
      "learning_rate": 1.9010655000303296e-05,
      "loss": 1.4432,
      "step": 3150
    },
    {
      "epoch": 0.5071010190162882,
      "grad_norm": 0.35065749287605286,
      "learning_rate": 1.9003127385831915e-05,
      "loss": 1.3832,
      "step": 3160
    },
    {
      "epoch": 0.5087057690764664,
      "grad_norm": 0.4324915409088135,
      "learning_rate": 1.8995572744125957e-05,
      "loss": 1.4079,
      "step": 3170
    },
    {
      "epoch": 0.5103105191366445,
      "grad_norm": 0.4284380078315735,
      "learning_rate": 1.898799109786433e-05,
      "loss": 1.5292,
      "step": 3180
    },
    {
      "epoch": 0.5119152691968226,
      "grad_norm": 0.3151065707206726,
      "learning_rate": 1.8980382469807003e-05,
      "loss": 1.5696,
      "step": 3190
    },
    {
      "epoch": 0.5135200192570007,
      "grad_norm": 0.3540195822715759,
      "learning_rate": 1.8972746882794947e-05,
      "loss": 1.2148,
      "step": 3200
    },
    {
      "epoch": 0.5151247693171789,
      "grad_norm": 0.3250848352909088,
      "learning_rate": 1.8965084359750063e-05,
      "loss": 1.3675,
      "step": 3210
    },
    {
      "epoch": 0.516729519377357,
      "grad_norm": 0.5598359107971191,
      "learning_rate": 1.895739492367512e-05,
      "loss": 1.3954,
      "step": 3220
    },
    {
      "epoch": 0.5183342694375351,
      "grad_norm": 0.48755258321762085,
      "learning_rate": 1.8949678597653667e-05,
      "loss": 1.2994,
      "step": 3230
    },
    {
      "epoch": 0.5199390194977133,
      "grad_norm": 0.4555019438266754,
      "learning_rate": 1.8941935404849987e-05,
      "loss": 1.4163,
      "step": 3240
    },
    {
      "epoch": 0.5215437695578914,
      "grad_norm": 0.423035204410553,
      "learning_rate": 1.8934165368509012e-05,
      "loss": 1.3923,
      "step": 3250
    },
    {
      "epoch": 0.5231485196180695,
      "grad_norm": 0.3289142847061157,
      "learning_rate": 1.8926368511956256e-05,
      "loss": 1.3082,
      "step": 3260
    },
    {
      "epoch": 0.5247532696782476,
      "grad_norm": 0.49323341250419617,
      "learning_rate": 1.8918544858597755e-05,
      "loss": 1.4025,
      "step": 3270
    },
    {
      "epoch": 0.5263580197384258,
      "grad_norm": 0.3933897316455841,
      "learning_rate": 1.8910694431919974e-05,
      "loss": 1.3069,
      "step": 3280
    },
    {
      "epoch": 0.5279627697986039,
      "grad_norm": 0.3914484977722168,
      "learning_rate": 1.8902817255489766e-05,
      "loss": 1.4352,
      "step": 3290
    },
    {
      "epoch": 0.529567519858782,
      "grad_norm": 0.36485975980758667,
      "learning_rate": 1.8894913352954282e-05,
      "loss": 1.2946,
      "step": 3300
    },
    {
      "epoch": 0.5311722699189602,
      "grad_norm": 0.49147748947143555,
      "learning_rate": 1.8886982748040892e-05,
      "loss": 1.3534,
      "step": 3310
    },
    {
      "epoch": 0.5327770199791383,
      "grad_norm": 0.5096155405044556,
      "learning_rate": 1.8879025464557146e-05,
      "loss": 1.309,
      "step": 3320
    },
    {
      "epoch": 0.5343817700393164,
      "grad_norm": 0.3081822395324707,
      "learning_rate": 1.8871041526390673e-05,
      "loss": 1.3639,
      "step": 3330
    },
    {
      "epoch": 0.5359865200994945,
      "grad_norm": 0.3889482617378235,
      "learning_rate": 1.886303095750911e-05,
      "loss": 1.2441,
      "step": 3340
    },
    {
      "epoch": 0.5375912701596727,
      "grad_norm": 0.4356420040130615,
      "learning_rate": 1.885499378196006e-05,
      "loss": 1.3678,
      "step": 3350
    },
    {
      "epoch": 0.5391960202198508,
      "grad_norm": 0.40843820571899414,
      "learning_rate": 1.8846930023870977e-05,
      "loss": 1.309,
      "step": 3360
    },
    {
      "epoch": 0.5408007702800289,
      "grad_norm": 0.39451417326927185,
      "learning_rate": 1.883883970744913e-05,
      "loss": 1.2417,
      "step": 3370
    },
    {
      "epoch": 0.5424055203402071,
      "grad_norm": 0.3055882155895233,
      "learning_rate": 1.8830722856981508e-05,
      "loss": 1.2256,
      "step": 3380
    },
    {
      "epoch": 0.5440102704003852,
      "grad_norm": 0.4477522671222687,
      "learning_rate": 1.8822579496834762e-05,
      "loss": 1.4449,
      "step": 3390
    },
    {
      "epoch": 0.5456150204605633,
      "grad_norm": 0.36812475323677063,
      "learning_rate": 1.8814409651455115e-05,
      "loss": 1.2899,
      "step": 3400
    },
    {
      "epoch": 0.5472197705207414,
      "grad_norm": 0.38545629382133484,
      "learning_rate": 1.8806213345368304e-05,
      "loss": 1.1903,
      "step": 3410
    },
    {
      "epoch": 0.5488245205809196,
      "grad_norm": 0.35380756855010986,
      "learning_rate": 1.8797990603179505e-05,
      "loss": 1.4448,
      "step": 3420
    },
    {
      "epoch": 0.5504292706410977,
      "grad_norm": 0.506960391998291,
      "learning_rate": 1.8789741449573243e-05,
      "loss": 1.2862,
      "step": 3430
    },
    {
      "epoch": 0.5520340207012758,
      "grad_norm": 0.3036964535713196,
      "learning_rate": 1.878146590931334e-05,
      "loss": 1.2406,
      "step": 3440
    },
    {
      "epoch": 0.553638770761454,
      "grad_norm": 0.4081314504146576,
      "learning_rate": 1.877316400724282e-05,
      "loss": 1.4669,
      "step": 3450
    },
    {
      "epoch": 0.5552435208216321,
      "grad_norm": 0.35533151030540466,
      "learning_rate": 1.876483576828386e-05,
      "loss": 1.2666,
      "step": 3460
    },
    {
      "epoch": 0.5568482708818101,
      "grad_norm": 0.4648311138153076,
      "learning_rate": 1.8756481217437685e-05,
      "loss": 1.3472,
      "step": 3470
    },
    {
      "epoch": 0.5584530209419882,
      "grad_norm": 0.3758234679698944,
      "learning_rate": 1.874810037978452e-05,
      "loss": 1.3189,
      "step": 3480
    },
    {
      "epoch": 0.5600577710021664,
      "grad_norm": 0.34578830003738403,
      "learning_rate": 1.873969328048349e-05,
      "loss": 1.5464,
      "step": 3490
    },
    {
      "epoch": 0.5616625210623445,
      "grad_norm": 0.44013527035713196,
      "learning_rate": 1.8731259944772566e-05,
      "loss": 1.3269,
      "step": 3500
    },
    {
      "epoch": 0.5632672711225226,
      "grad_norm": 0.30117514729499817,
      "learning_rate": 1.872280039796848e-05,
      "loss": 1.2317,
      "step": 3510
    },
    {
      "epoch": 0.5648720211827007,
      "grad_norm": 0.33515483140945435,
      "learning_rate": 1.871431466546664e-05,
      "loss": 1.2923,
      "step": 3520
    },
    {
      "epoch": 0.5664767712428789,
      "grad_norm": 0.3113756477832794,
      "learning_rate": 1.870580277274108e-05,
      "loss": 1.286,
      "step": 3530
    },
    {
      "epoch": 0.568081521303057,
      "grad_norm": 0.42696085572242737,
      "learning_rate": 1.869726474534435e-05,
      "loss": 1.3726,
      "step": 3540
    },
    {
      "epoch": 0.5696862713632351,
      "grad_norm": 0.33821311593055725,
      "learning_rate": 1.8688700608907462e-05,
      "loss": 1.3283,
      "step": 3550
    },
    {
      "epoch": 0.5712910214234133,
      "grad_norm": 0.30294883251190186,
      "learning_rate": 1.8680110389139806e-05,
      "loss": 1.3961,
      "step": 3560
    },
    {
      "epoch": 0.5728957714835914,
      "grad_norm": 0.37252262234687805,
      "learning_rate": 1.8671494111829084e-05,
      "loss": 1.3105,
      "step": 3570
    },
    {
      "epoch": 0.5745005215437695,
      "grad_norm": 0.3752776086330414,
      "learning_rate": 1.8662851802841202e-05,
      "loss": 1.3839,
      "step": 3580
    },
    {
      "epoch": 0.5761052716039476,
      "grad_norm": 0.4236966371536255,
      "learning_rate": 1.865418348812023e-05,
      "loss": 1.2352,
      "step": 3590
    },
    {
      "epoch": 0.5777100216641258,
      "grad_norm": 0.5295116901397705,
      "learning_rate": 1.8645489193688303e-05,
      "loss": 1.5045,
      "step": 3600
    },
    {
      "epoch": 0.5793147717243039,
      "grad_norm": 0.32668522000312805,
      "learning_rate": 1.863676894564554e-05,
      "loss": 1.1554,
      "step": 3610
    },
    {
      "epoch": 0.580919521784482,
      "grad_norm": 0.3489832580089569,
      "learning_rate": 1.8628022770169975e-05,
      "loss": 1.3662,
      "step": 3620
    },
    {
      "epoch": 0.5825242718446602,
      "grad_norm": 0.4019912779331207,
      "learning_rate": 1.8619250693517478e-05,
      "loss": 1.3283,
      "step": 3630
    },
    {
      "epoch": 0.5841290219048383,
      "grad_norm": 0.6127420663833618,
      "learning_rate": 1.861045274202168e-05,
      "loss": 1.4119,
      "step": 3640
    },
    {
      "epoch": 0.5857337719650164,
      "grad_norm": 0.506712794303894,
      "learning_rate": 1.8601628942093874e-05,
      "loss": 1.3606,
      "step": 3650
    },
    {
      "epoch": 0.5873385220251945,
      "grad_norm": 0.34272444248199463,
      "learning_rate": 1.859277932022296e-05,
      "loss": 1.3022,
      "step": 3660
    },
    {
      "epoch": 0.5889432720853727,
      "grad_norm": 0.2564506530761719,
      "learning_rate": 1.858390390297535e-05,
      "loss": 1.3398,
      "step": 3670
    },
    {
      "epoch": 0.5905480221455508,
      "grad_norm": 0.3201383352279663,
      "learning_rate": 1.8575002716994894e-05,
      "loss": 1.1833,
      "step": 3680
    },
    {
      "epoch": 0.5921527722057289,
      "grad_norm": 0.26274386048316956,
      "learning_rate": 1.85660757890028e-05,
      "loss": 1.2,
      "step": 3690
    },
    {
      "epoch": 0.5937575222659071,
      "grad_norm": 0.4444594085216522,
      "learning_rate": 1.855712314579756e-05,
      "loss": 1.3535,
      "step": 3700
    },
    {
      "epoch": 0.5953622723260852,
      "grad_norm": 0.3849436640739441,
      "learning_rate": 1.8548144814254846e-05,
      "loss": 1.3507,
      "step": 3710
    },
    {
      "epoch": 0.5969670223862633,
      "grad_norm": 0.7471434473991394,
      "learning_rate": 1.8539140821327465e-05,
      "loss": 1.3687,
      "step": 3720
    },
    {
      "epoch": 0.5985717724464414,
      "grad_norm": 0.37937498092651367,
      "learning_rate": 1.8530111194045244e-05,
      "loss": 1.3297,
      "step": 3730
    },
    {
      "epoch": 0.6001765225066196,
      "grad_norm": 0.5485889315605164,
      "learning_rate": 1.852105595951497e-05,
      "loss": 1.2996,
      "step": 3740
    },
    {
      "epoch": 0.6017812725667977,
      "grad_norm": 0.344595342874527,
      "learning_rate": 1.8511975144920303e-05,
      "loss": 1.4174,
      "step": 3750
    },
    {
      "epoch": 0.6033860226269758,
      "grad_norm": 0.36519238352775574,
      "learning_rate": 1.8502868777521698e-05,
      "loss": 1.3573,
      "step": 3760
    },
    {
      "epoch": 0.604990772687154,
      "grad_norm": 0.33092305064201355,
      "learning_rate": 1.8493736884656308e-05,
      "loss": 1.1399,
      "step": 3770
    },
    {
      "epoch": 0.6065955227473321,
      "grad_norm": 0.6363629698753357,
      "learning_rate": 1.8484579493737922e-05,
      "loss": 1.5172,
      "step": 3780
    },
    {
      "epoch": 0.6082002728075102,
      "grad_norm": 0.5183701515197754,
      "learning_rate": 1.847539663225687e-05,
      "loss": 1.4047,
      "step": 3790
    },
    {
      "epoch": 0.6098050228676883,
      "grad_norm": 0.4549940824508667,
      "learning_rate": 1.8466188327779945e-05,
      "loss": 1.338,
      "step": 3800
    },
    {
      "epoch": 0.6114097729278665,
      "grad_norm": 0.4385157823562622,
      "learning_rate": 1.8456954607950323e-05,
      "loss": 1.4217,
      "step": 3810
    },
    {
      "epoch": 0.6130145229880446,
      "grad_norm": 0.3409535586833954,
      "learning_rate": 1.844769550048747e-05,
      "loss": 1.215,
      "step": 3820
    },
    {
      "epoch": 0.6146192730482227,
      "grad_norm": 0.38314902782440186,
      "learning_rate": 1.8438411033187072e-05,
      "loss": 1.347,
      "step": 3830
    },
    {
      "epoch": 0.6162240231084009,
      "grad_norm": 0.32588592171669006,
      "learning_rate": 1.8429101233920934e-05,
      "loss": 1.2749,
      "step": 3840
    },
    {
      "epoch": 0.617828773168579,
      "grad_norm": 0.3507879674434662,
      "learning_rate": 1.8419766130636922e-05,
      "loss": 1.2889,
      "step": 3850
    },
    {
      "epoch": 0.6194335232287571,
      "grad_norm": 0.4801709055900574,
      "learning_rate": 1.841040575135885e-05,
      "loss": 1.2979,
      "step": 3860
    },
    {
      "epoch": 0.6210382732889352,
      "grad_norm": 0.3517807126045227,
      "learning_rate": 1.840102012418642e-05,
      "loss": 1.3173,
      "step": 3870
    },
    {
      "epoch": 0.6226430233491134,
      "grad_norm": 0.28233009576797485,
      "learning_rate": 1.839160927729513e-05,
      "loss": 1.2639,
      "step": 3880
    },
    {
      "epoch": 0.6242477734092915,
      "grad_norm": 0.3936665654182434,
      "learning_rate": 1.838217323893617e-05,
      "loss": 1.4767,
      "step": 3890
    },
    {
      "epoch": 0.6258525234694696,
      "grad_norm": 0.42626455426216125,
      "learning_rate": 1.8372712037436377e-05,
      "loss": 1.4665,
      "step": 3900
    },
    {
      "epoch": 0.6274572735296478,
      "grad_norm": 0.36147841811180115,
      "learning_rate": 1.8363225701198105e-05,
      "loss": 1.3301,
      "step": 3910
    },
    {
      "epoch": 0.6290620235898259,
      "grad_norm": 0.48877590894699097,
      "learning_rate": 1.8353714258699185e-05,
      "loss": 1.4647,
      "step": 3920
    },
    {
      "epoch": 0.630666773650004,
      "grad_norm": 0.3290277421474457,
      "learning_rate": 1.8344177738492797e-05,
      "loss": 1.4444,
      "step": 3930
    },
    {
      "epoch": 0.6322715237101821,
      "grad_norm": 0.35547274351119995,
      "learning_rate": 1.8334616169207414e-05,
      "loss": 1.353,
      "step": 3940
    },
    {
      "epoch": 0.6338762737703603,
      "grad_norm": 0.31548550724983215,
      "learning_rate": 1.8325029579546703e-05,
      "loss": 1.3747,
      "step": 3950
    },
    {
      "epoch": 0.6354810238305384,
      "grad_norm": 0.37399932742118835,
      "learning_rate": 1.831541799828944e-05,
      "loss": 1.222,
      "step": 3960
    },
    {
      "epoch": 0.6370857738907165,
      "grad_norm": 0.696702778339386,
      "learning_rate": 1.8305781454289436e-05,
      "loss": 1.3068,
      "step": 3970
    },
    {
      "epoch": 0.6386905239508947,
      "grad_norm": 0.4077337682247162,
      "learning_rate": 1.8296119976475422e-05,
      "loss": 1.3649,
      "step": 3980
    },
    {
      "epoch": 0.6402952740110728,
      "grad_norm": 0.5040619373321533,
      "learning_rate": 1.8286433593851e-05,
      "loss": 1.3536,
      "step": 3990
    },
    {
      "epoch": 0.6419000240712509,
      "grad_norm": 0.30516791343688965,
      "learning_rate": 1.827672233549451e-05,
      "loss": 1.3191,
      "step": 4000
    },
    {
      "epoch": 0.643504774131429,
      "grad_norm": 0.4528673589229584,
      "learning_rate": 1.8266986230559e-05,
      "loss": 1.3107,
      "step": 4010
    },
    {
      "epoch": 0.6451095241916072,
      "grad_norm": 0.37125346064567566,
      "learning_rate": 1.8257225308272078e-05,
      "loss": 1.1527,
      "step": 4020
    },
    {
      "epoch": 0.6467142742517853,
      "grad_norm": 0.5220421552658081,
      "learning_rate": 1.8247439597935873e-05,
      "loss": 1.3151,
      "step": 4030
    },
    {
      "epoch": 0.6483190243119634,
      "grad_norm": 0.5411961078643799,
      "learning_rate": 1.8237629128926914e-05,
      "loss": 1.2768,
      "step": 4040
    },
    {
      "epoch": 0.6499237743721415,
      "grad_norm": 0.32979291677474976,
      "learning_rate": 1.8227793930696068e-05,
      "loss": 1.2769,
      "step": 4050
    },
    {
      "epoch": 0.6515285244323197,
      "grad_norm": 0.7464665770530701,
      "learning_rate": 1.821793403276843e-05,
      "loss": 1.3573,
      "step": 4060
    },
    {
      "epoch": 0.6531332744924978,
      "grad_norm": 0.4306504726409912,
      "learning_rate": 1.8208049464743244e-05,
      "loss": 1.2734,
      "step": 4070
    },
    {
      "epoch": 0.6547380245526759,
      "grad_norm": 0.45659545063972473,
      "learning_rate": 1.8198140256293814e-05,
      "loss": 1.2771,
      "step": 4080
    },
    {
      "epoch": 0.6563427746128541,
      "grad_norm": 0.37459665536880493,
      "learning_rate": 1.8188206437167415e-05,
      "loss": 1.3551,
      "step": 4090
    },
    {
      "epoch": 0.6579475246730322,
      "grad_norm": 0.6561301946640015,
      "learning_rate": 1.8178248037185207e-05,
      "loss": 1.4558,
      "step": 4100
    },
    {
      "epoch": 0.6595522747332103,
      "grad_norm": 0.40747231245040894,
      "learning_rate": 1.8168265086242124e-05,
      "loss": 1.2322,
      "step": 4110
    },
    {
      "epoch": 0.6611570247933884,
      "grad_norm": 0.38931646943092346,
      "learning_rate": 1.8158257614306828e-05,
      "loss": 1.3496,
      "step": 4120
    },
    {
      "epoch": 0.6627617748535666,
      "grad_norm": 0.3700779676437378,
      "learning_rate": 1.8148225651421573e-05,
      "loss": 1.2368,
      "step": 4130
    },
    {
      "epoch": 0.6643665249137447,
      "grad_norm": 0.4402172565460205,
      "learning_rate": 1.813816922770214e-05,
      "loss": 1.3536,
      "step": 4140
    },
    {
      "epoch": 0.6659712749739228,
      "grad_norm": 0.3810122311115265,
      "learning_rate": 1.8128088373337738e-05,
      "loss": 1.3272,
      "step": 4150
    },
    {
      "epoch": 0.667576025034101,
      "grad_norm": 0.4991161823272705,
      "learning_rate": 1.8117983118590927e-05,
      "loss": 1.4764,
      "step": 4160
    },
    {
      "epoch": 0.6691807750942791,
      "grad_norm": 0.5510333776473999,
      "learning_rate": 1.8107853493797502e-05,
      "loss": 1.3793,
      "step": 4170
    },
    {
      "epoch": 0.6707855251544572,
      "grad_norm": 0.421845406293869,
      "learning_rate": 1.8097699529366425e-05,
      "loss": 1.3538,
      "step": 4180
    },
    {
      "epoch": 0.6723902752146353,
      "grad_norm": 0.47272321581840515,
      "learning_rate": 1.8087521255779728e-05,
      "loss": 1.3916,
      "step": 4190
    },
    {
      "epoch": 0.6739950252748135,
      "grad_norm": 0.4718872606754303,
      "learning_rate": 1.8077318703592413e-05,
      "loss": 1.4242,
      "step": 4200
    },
    {
      "epoch": 0.6755997753349916,
      "grad_norm": 0.28562572598457336,
      "learning_rate": 1.8067091903432362e-05,
      "loss": 1.3378,
      "step": 4210
    },
    {
      "epoch": 0.6772045253951697,
      "grad_norm": 0.4194621443748474,
      "learning_rate": 1.8056840886000263e-05,
      "loss": 1.3515,
      "step": 4220
    },
    {
      "epoch": 0.6788092754553479,
      "grad_norm": 0.5180320739746094,
      "learning_rate": 1.8046565682069488e-05,
      "loss": 1.3462,
      "step": 4230
    },
    {
      "epoch": 0.680414025515526,
      "grad_norm": 0.5235304832458496,
      "learning_rate": 1.8036266322486025e-05,
      "loss": 1.2596,
      "step": 4240
    },
    {
      "epoch": 0.6820187755757041,
      "grad_norm": 0.5107454061508179,
      "learning_rate": 1.8025942838168376e-05,
      "loss": 1.2346,
      "step": 4250
    },
    {
      "epoch": 0.6836235256358822,
      "grad_norm": 0.4136069118976593,
      "learning_rate": 1.8015595260107467e-05,
      "loss": 1.3906,
      "step": 4260
    },
    {
      "epoch": 0.6852282756960604,
      "grad_norm": 0.3837946653366089,
      "learning_rate": 1.800522361936655e-05,
      "loss": 1.2655,
      "step": 4270
    },
    {
      "epoch": 0.6868330257562385,
      "grad_norm": 0.5983788371086121,
      "learning_rate": 1.7994827947081108e-05,
      "loss": 1.3558,
      "step": 4280
    },
    {
      "epoch": 0.6884377758164166,
      "grad_norm": 0.26743337512016296,
      "learning_rate": 1.798440827445878e-05,
      "loss": 1.2114,
      "step": 4290
    },
    {
      "epoch": 0.6900425258765948,
      "grad_norm": 0.38096147775650024,
      "learning_rate": 1.797396463277924e-05,
      "loss": 1.3249,
      "step": 4300
    },
    {
      "epoch": 0.6916472759367729,
      "grad_norm": 0.5418678522109985,
      "learning_rate": 1.7963497053394118e-05,
      "loss": 1.3708,
      "step": 4310
    },
    {
      "epoch": 0.693252025996951,
      "grad_norm": 0.34386369585990906,
      "learning_rate": 1.795300556772692e-05,
      "loss": 1.5,
      "step": 4320
    },
    {
      "epoch": 0.6948567760571291,
      "grad_norm": 0.44043245911598206,
      "learning_rate": 1.794249020727289e-05,
      "loss": 1.3789,
      "step": 4330
    },
    {
      "epoch": 0.6964615261173073,
      "grad_norm": 0.44178494811058044,
      "learning_rate": 1.793195100359898e-05,
      "loss": 1.2944,
      "step": 4340
    },
    {
      "epoch": 0.6980662761774854,
      "grad_norm": 0.4453251361846924,
      "learning_rate": 1.7921387988343683e-05,
      "loss": 1.235,
      "step": 4350
    },
    {
      "epoch": 0.6996710262376635,
      "grad_norm": 0.4749120771884918,
      "learning_rate": 1.7910801193217e-05,
      "loss": 1.4486,
      "step": 4360
    },
    {
      "epoch": 0.7012757762978417,
      "grad_norm": 0.43452924489974976,
      "learning_rate": 1.7900190650000305e-05,
      "loss": 1.3148,
      "step": 4370
    },
    {
      "epoch": 0.7028805263580198,
      "grad_norm": 0.3879772424697876,
      "learning_rate": 1.7889556390546267e-05,
      "loss": 1.3008,
      "step": 4380
    },
    {
      "epoch": 0.7044852764181979,
      "grad_norm": 0.4637013375759125,
      "learning_rate": 1.7878898446778757e-05,
      "loss": 1.5488,
      "step": 4390
    },
    {
      "epoch": 0.706090026478376,
      "grad_norm": 0.27585384249687195,
      "learning_rate": 1.786821685069273e-05,
      "loss": 1.2449,
      "step": 4400
    },
    {
      "epoch": 0.7076947765385542,
      "grad_norm": 0.5484049916267395,
      "learning_rate": 1.785751163435416e-05,
      "loss": 1.519,
      "step": 4410
    },
    {
      "epoch": 0.7092995265987323,
      "grad_norm": 0.28094980120658875,
      "learning_rate": 1.7846782829899928e-05,
      "loss": 1.396,
      "step": 4420
    },
    {
      "epoch": 0.7109042766589104,
      "grad_norm": 0.5125242471694946,
      "learning_rate": 1.7836030469537717e-05,
      "loss": 1.2525,
      "step": 4430
    },
    {
      "epoch": 0.7125090267190886,
      "grad_norm": 0.39358553290367126,
      "learning_rate": 1.782525458554593e-05,
      "loss": 1.5223,
      "step": 4440
    },
    {
      "epoch": 0.7141137767792667,
      "grad_norm": 0.40496373176574707,
      "learning_rate": 1.7814455210273585e-05,
      "loss": 1.2904,
      "step": 4450
    },
    {
      "epoch": 0.7157185268394447,
      "grad_norm": 0.4594113826751709,
      "learning_rate": 1.7803632376140224e-05,
      "loss": 1.3437,
      "step": 4460
    },
    {
      "epoch": 0.7173232768996228,
      "grad_norm": 0.4577178955078125,
      "learning_rate": 1.7792786115635808e-05,
      "loss": 1.3009,
      "step": 4470
    },
    {
      "epoch": 0.718928026959801,
      "grad_norm": 0.31177616119384766,
      "learning_rate": 1.778191646132063e-05,
      "loss": 1.251,
      "step": 4480
    },
    {
      "epoch": 0.7205327770199791,
      "grad_norm": 0.5064173340797424,
      "learning_rate": 1.7771023445825203e-05,
      "loss": 1.4454,
      "step": 4490
    },
    {
      "epoch": 0.7221375270801572,
      "grad_norm": 0.5016399621963501,
      "learning_rate": 1.776010710185017e-05,
      "loss": 1.4249,
      "step": 4500
    },
    {
      "epoch": 0.7237422771403353,
      "grad_norm": 0.3784669041633606,
      "learning_rate": 1.7749167462166217e-05,
      "loss": 1.3686,
      "step": 4510
    },
    {
      "epoch": 0.7253470272005135,
      "grad_norm": 0.6547024250030518,
      "learning_rate": 1.7738204559613953e-05,
      "loss": 1.3508,
      "step": 4520
    },
    {
      "epoch": 0.7269517772606916,
      "grad_norm": 0.41100314259529114,
      "learning_rate": 1.7727218427103822e-05,
      "loss": 1.4067,
      "step": 4530
    },
    {
      "epoch": 0.7285565273208697,
      "grad_norm": 0.3411480188369751,
      "learning_rate": 1.7716209097616003e-05,
      "loss": 1.3073,
      "step": 4540
    },
    {
      "epoch": 0.7301612773810479,
      "grad_norm": 0.44814205169677734,
      "learning_rate": 1.7705176604200323e-05,
      "loss": 1.3775,
      "step": 4550
    },
    {
      "epoch": 0.731766027441226,
      "grad_norm": 0.4059377610683441,
      "learning_rate": 1.7694120979976132e-05,
      "loss": 1.4588,
      "step": 4560
    },
    {
      "epoch": 0.7333707775014041,
      "grad_norm": 0.44208186864852905,
      "learning_rate": 1.7683042258132232e-05,
      "loss": 1.3924,
      "step": 4570
    },
    {
      "epoch": 0.7349755275615822,
      "grad_norm": 0.4174703359603882,
      "learning_rate": 1.7671940471926746e-05,
      "loss": 1.2993,
      "step": 4580
    },
    {
      "epoch": 0.7365802776217604,
      "grad_norm": 0.4241965413093567,
      "learning_rate": 1.7660815654687055e-05,
      "loss": 1.363,
      "step": 4590
    },
    {
      "epoch": 0.7381850276819385,
      "grad_norm": 0.43813377618789673,
      "learning_rate": 1.764966783980967e-05,
      "loss": 1.1295,
      "step": 4600
    },
    {
      "epoch": 0.7397897777421166,
      "grad_norm": 0.4926077723503113,
      "learning_rate": 1.763849706076014e-05,
      "loss": 1.3185,
      "step": 4610
    },
    {
      "epoch": 0.7413945278022948,
      "grad_norm": 0.44836950302124023,
      "learning_rate": 1.762730335107295e-05,
      "loss": 1.3133,
      "step": 4620
    },
    {
      "epoch": 0.7429992778624729,
      "grad_norm": 0.3741202652454376,
      "learning_rate": 1.7616086744351423e-05,
      "loss": 1.3731,
      "step": 4630
    },
    {
      "epoch": 0.744604027922651,
      "grad_norm": 0.5487156510353088,
      "learning_rate": 1.7604847274267628e-05,
      "loss": 1.3456,
      "step": 4640
    },
    {
      "epoch": 0.7462087779828291,
      "grad_norm": 0.4102467894554138,
      "learning_rate": 1.7593584974562258e-05,
      "loss": 1.2209,
      "step": 4650
    },
    {
      "epoch": 0.7478135280430073,
      "grad_norm": 0.319533109664917,
      "learning_rate": 1.7582299879044548e-05,
      "loss": 1.4936,
      "step": 4660
    },
    {
      "epoch": 0.7494182781031854,
      "grad_norm": 0.3102225065231323,
      "learning_rate": 1.7570992021592154e-05,
      "loss": 1.2653,
      "step": 4670
    },
    {
      "epoch": 0.7510230281633635,
      "grad_norm": 0.40316125750541687,
      "learning_rate": 1.7559661436151082e-05,
      "loss": 1.2612,
      "step": 4680
    },
    {
      "epoch": 0.7526277782235417,
      "grad_norm": 0.27865055203437805,
      "learning_rate": 1.7548308156735544e-05,
      "loss": 1.4759,
      "step": 4690
    },
    {
      "epoch": 0.7542325282837198,
      "grad_norm": 0.37750911712646484,
      "learning_rate": 1.7536932217427898e-05,
      "loss": 1.2777,
      "step": 4700
    },
    {
      "epoch": 0.7558372783438979,
      "grad_norm": 0.5535004138946533,
      "learning_rate": 1.752553365237852e-05,
      "loss": 1.438,
      "step": 4710
    },
    {
      "epoch": 0.757442028404076,
      "grad_norm": 0.4563482999801636,
      "learning_rate": 1.7514112495805703e-05,
      "loss": 1.2838,
      "step": 4720
    },
    {
      "epoch": 0.7590467784642542,
      "grad_norm": 0.3439822793006897,
      "learning_rate": 1.7502668781995566e-05,
      "loss": 1.3012,
      "step": 4730
    },
    {
      "epoch": 0.7606515285244323,
      "grad_norm": 0.4144960939884186,
      "learning_rate": 1.7491202545301944e-05,
      "loss": 1.4224,
      "step": 4740
    },
    {
      "epoch": 0.7622562785846104,
      "grad_norm": 0.41869184374809265,
      "learning_rate": 1.7479713820146282e-05,
      "loss": 1.2688,
      "step": 4750
    },
    {
      "epoch": 0.7638610286447886,
      "grad_norm": 0.5026125907897949,
      "learning_rate": 1.7468202641017538e-05,
      "loss": 1.3464,
      "step": 4760
    },
    {
      "epoch": 0.7654657787049667,
      "grad_norm": 0.5020219087600708,
      "learning_rate": 1.7456669042472072e-05,
      "loss": 1.0694,
      "step": 4770
    },
    {
      "epoch": 0.7670705287651448,
      "grad_norm": 0.2963052988052368,
      "learning_rate": 1.7445113059133554e-05,
      "loss": 1.2345,
      "step": 4780
    },
    {
      "epoch": 0.7686752788253229,
      "grad_norm": 0.44063690304756165,
      "learning_rate": 1.7433534725692843e-05,
      "loss": 1.2226,
      "step": 4790
    },
    {
      "epoch": 0.7702800288855011,
      "grad_norm": 0.4285176992416382,
      "learning_rate": 1.7421934076907905e-05,
      "loss": 1.4068,
      "step": 4800
    },
    {
      "epoch": 0.7718847789456792,
      "grad_norm": 0.37804731726646423,
      "learning_rate": 1.7410311147603682e-05,
      "loss": 1.2514,
      "step": 4810
    },
    {
      "epoch": 0.7734895290058573,
      "grad_norm": 0.4041316509246826,
      "learning_rate": 1.7398665972672014e-05,
      "loss": 1.4336,
      "step": 4820
    },
    {
      "epoch": 0.7750942790660355,
      "grad_norm": 0.3655970096588135,
      "learning_rate": 1.7386998587071518e-05,
      "loss": 1.1238,
      "step": 4830
    },
    {
      "epoch": 0.7766990291262136,
      "grad_norm": 0.4350801408290863,
      "learning_rate": 1.7375309025827482e-05,
      "loss": 1.5054,
      "step": 4840
    },
    {
      "epoch": 0.7783037791863917,
      "grad_norm": 0.49292856454849243,
      "learning_rate": 1.7363597324031772e-05,
      "loss": 1.3391,
      "step": 4850
    },
    {
      "epoch": 0.7799085292465698,
      "grad_norm": 0.37051087617874146,
      "learning_rate": 1.7351863516842717e-05,
      "loss": 1.3629,
      "step": 4860
    },
    {
      "epoch": 0.781513279306748,
      "grad_norm": 0.4974030554294586,
      "learning_rate": 1.7340107639485006e-05,
      "loss": 1.2496,
      "step": 4870
    },
    {
      "epoch": 0.7831180293669261,
      "grad_norm": 0.4464688301086426,
      "learning_rate": 1.7328329727249577e-05,
      "loss": 1.3146,
      "step": 4880
    },
    {
      "epoch": 0.7847227794271042,
      "grad_norm": 0.3693033456802368,
      "learning_rate": 1.7316529815493532e-05,
      "loss": 1.3152,
      "step": 4890
    },
    {
      "epoch": 0.7863275294872824,
      "grad_norm": 0.4830872714519501,
      "learning_rate": 1.730470793963999e-05,
      "loss": 1.3206,
      "step": 4900
    },
    {
      "epoch": 0.7879322795474605,
      "grad_norm": 0.34010177850723267,
      "learning_rate": 1.7292864135178032e-05,
      "loss": 1.4691,
      "step": 4910
    },
    {
      "epoch": 0.7895370296076386,
      "grad_norm": 0.5918980836868286,
      "learning_rate": 1.728099843766255e-05,
      "loss": 1.2618,
      "step": 4920
    },
    {
      "epoch": 0.7911417796678167,
      "grad_norm": 0.3285658359527588,
      "learning_rate": 1.7269110882714167e-05,
      "loss": 1.3736,
      "step": 4930
    },
    {
      "epoch": 0.7927465297279949,
      "grad_norm": 0.44096946716308594,
      "learning_rate": 1.7257201506019127e-05,
      "loss": 1.3606,
      "step": 4940
    },
    {
      "epoch": 0.794351279788173,
      "grad_norm": 0.584700345993042,
      "learning_rate": 1.7245270343329162e-05,
      "loss": 1.2994,
      "step": 4950
    },
    {
      "epoch": 0.7959560298483511,
      "grad_norm": 0.47238895297050476,
      "learning_rate": 1.723331743046143e-05,
      "loss": 1.3257,
      "step": 4960
    },
    {
      "epoch": 0.7975607799085292,
      "grad_norm": 0.3651365637779236,
      "learning_rate": 1.722134280329836e-05,
      "loss": 1.2081,
      "step": 4970
    },
    {
      "epoch": 0.7991655299687074,
      "grad_norm": 0.49651721119880676,
      "learning_rate": 1.7209346497787593e-05,
      "loss": 1.2263,
      "step": 4980
    },
    {
      "epoch": 0.8007702800288855,
      "grad_norm": 0.3314235806465149,
      "learning_rate": 1.7197328549941818e-05,
      "loss": 1.2219,
      "step": 4990
    },
    {
      "epoch": 0.8023750300890636,
      "grad_norm": 0.384847491979599,
      "learning_rate": 1.718528899583872e-05,
      "loss": 1.1697,
      "step": 5000
    },
    {
      "epoch": 0.8039797801492418,
      "grad_norm": 0.5230315923690796,
      "learning_rate": 1.7173227871620827e-05,
      "loss": 1.5142,
      "step": 5010
    },
    {
      "epoch": 0.8055845302094199,
      "grad_norm": 0.42152613401412964,
      "learning_rate": 1.7161145213495434e-05,
      "loss": 1.357,
      "step": 5020
    },
    {
      "epoch": 0.807189280269598,
      "grad_norm": 0.41914135217666626,
      "learning_rate": 1.7149041057734474e-05,
      "loss": 1.2972,
      "step": 5030
    },
    {
      "epoch": 0.8087940303297761,
      "grad_norm": 0.4114096164703369,
      "learning_rate": 1.713691544067441e-05,
      "loss": 1.5291,
      "step": 5040
    },
    {
      "epoch": 0.8103987803899543,
      "grad_norm": 0.7391153573989868,
      "learning_rate": 1.712476839871614e-05,
      "loss": 1.4112,
      "step": 5050
    },
    {
      "epoch": 0.8120035304501324,
      "grad_norm": 0.4324323236942291,
      "learning_rate": 1.711259996832488e-05,
      "loss": 1.3118,
      "step": 5060
    },
    {
      "epoch": 0.8136082805103105,
      "grad_norm": 0.5286919474601746,
      "learning_rate": 1.7100410186030045e-05,
      "loss": 1.4575,
      "step": 5070
    },
    {
      "epoch": 0.8152130305704887,
      "grad_norm": 0.4221147298812866,
      "learning_rate": 1.7088199088425158e-05,
      "loss": 1.3758,
      "step": 5080
    },
    {
      "epoch": 0.8168177806306668,
      "grad_norm": 0.5743736624717712,
      "learning_rate": 1.707596671216772e-05,
      "loss": 1.4401,
      "step": 5090
    },
    {
      "epoch": 0.8184225306908449,
      "grad_norm": 0.3428259789943695,
      "learning_rate": 1.7063713093979118e-05,
      "loss": 1.3772,
      "step": 5100
    },
    {
      "epoch": 0.820027280751023,
      "grad_norm": 0.40434467792510986,
      "learning_rate": 1.7051438270644504e-05,
      "loss": 1.4265,
      "step": 5110
    },
    {
      "epoch": 0.8216320308112012,
      "grad_norm": 0.3157801628112793,
      "learning_rate": 1.7039142279012693e-05,
      "loss": 1.3497,
      "step": 5120
    },
    {
      "epoch": 0.8232367808713793,
      "grad_norm": 0.3540233075618744,
      "learning_rate": 1.7026825155996035e-05,
      "loss": 1.1807,
      "step": 5130
    },
    {
      "epoch": 0.8248415309315574,
      "grad_norm": 0.44661834836006165,
      "learning_rate": 1.7014486938570324e-05,
      "loss": 1.3529,
      "step": 5140
    },
    {
      "epoch": 0.8264462809917356,
      "grad_norm": 0.4605134129524231,
      "learning_rate": 1.700212766377468e-05,
      "loss": 1.3941,
      "step": 5150
    },
    {
      "epoch": 0.8280510310519137,
      "grad_norm": 0.44499823451042175,
      "learning_rate": 1.6989747368711432e-05,
      "loss": 1.3659,
      "step": 5160
    },
    {
      "epoch": 0.8296557811120918,
      "grad_norm": 0.7105720043182373,
      "learning_rate": 1.697734609054602e-05,
      "loss": 1.3627,
      "step": 5170
    },
    {
      "epoch": 0.83126053117227,
      "grad_norm": 0.3888545036315918,
      "learning_rate": 1.6964923866506865e-05,
      "loss": 1.3088,
      "step": 5180
    },
    {
      "epoch": 0.8328652812324481,
      "grad_norm": 0.4209115207195282,
      "learning_rate": 1.695248073388527e-05,
      "loss": 1.2464,
      "step": 5190
    },
    {
      "epoch": 0.8344700312926262,
      "grad_norm": 0.48704811930656433,
      "learning_rate": 1.6940016730035306e-05,
      "loss": 1.3356,
      "step": 5200
    },
    {
      "epoch": 0.8360747813528043,
      "grad_norm": 0.5031517148017883,
      "learning_rate": 1.69275318923737e-05,
      "loss": 1.3395,
      "step": 5210
    },
    {
      "epoch": 0.8376795314129825,
      "grad_norm": 0.43179312348365784,
      "learning_rate": 1.691502625837973e-05,
      "loss": 1.3407,
      "step": 5220
    },
    {
      "epoch": 0.8392842814731606,
      "grad_norm": 0.2716063857078552,
      "learning_rate": 1.690249986559508e-05,
      "loss": 1.3156,
      "step": 5230
    },
    {
      "epoch": 0.8408890315333387,
      "grad_norm": 0.42406779527664185,
      "learning_rate": 1.688995275162378e-05,
      "loss": 1.3187,
      "step": 5240
    },
    {
      "epoch": 0.8424937815935168,
      "grad_norm": 0.3839367926120758,
      "learning_rate": 1.6877384954132052e-05,
      "loss": 1.1937,
      "step": 5250
    },
    {
      "epoch": 0.844098531653695,
      "grad_norm": 0.49437153339385986,
      "learning_rate": 1.6864796510848197e-05,
      "loss": 1.2126,
      "step": 5260
    },
    {
      "epoch": 0.8457032817138731,
      "grad_norm": 0.287202924489975,
      "learning_rate": 1.685218745956252e-05,
      "loss": 1.3967,
      "step": 5270
    },
    {
      "epoch": 0.8473080317740512,
      "grad_norm": 0.3923187851905823,
      "learning_rate": 1.683955783812717e-05,
      "loss": 1.1879,
      "step": 5280
    },
    {
      "epoch": 0.8489127818342294,
      "grad_norm": 0.443006306886673,
      "learning_rate": 1.6826907684456055e-05,
      "loss": 1.447,
      "step": 5290
    },
    {
      "epoch": 0.8505175318944075,
      "grad_norm": 0.3437308371067047,
      "learning_rate": 1.6814237036524725e-05,
      "loss": 1.3741,
      "step": 5300
    },
    {
      "epoch": 0.8521222819545856,
      "grad_norm": 0.4169386327266693,
      "learning_rate": 1.6801545932370244e-05,
      "loss": 1.3657,
      "step": 5310
    },
    {
      "epoch": 0.8537270320147637,
      "grad_norm": 0.3671761453151703,
      "learning_rate": 1.6788834410091094e-05,
      "loss": 1.2365,
      "step": 5320
    },
    {
      "epoch": 0.8553317820749419,
      "grad_norm": 0.42742592096328735,
      "learning_rate": 1.677610250784704e-05,
      "loss": 1.3898,
      "step": 5330
    },
    {
      "epoch": 0.85693653213512,
      "grad_norm": 0.39841219782829285,
      "learning_rate": 1.6763350263859043e-05,
      "loss": 1.39,
      "step": 5340
    },
    {
      "epoch": 0.8585412821952981,
      "grad_norm": 0.6718493700027466,
      "learning_rate": 1.6750577716409115e-05,
      "loss": 1.3272,
      "step": 5350
    },
    {
      "epoch": 0.8601460322554763,
      "grad_norm": 0.43465569615364075,
      "learning_rate": 1.673778490384023e-05,
      "loss": 1.519,
      "step": 5360
    },
    {
      "epoch": 0.8617507823156544,
      "grad_norm": 0.526240348815918,
      "learning_rate": 1.6724971864556193e-05,
      "loss": 1.2687,
      "step": 5370
    },
    {
      "epoch": 0.8633555323758325,
      "grad_norm": 0.45785650610923767,
      "learning_rate": 1.6712138637021526e-05,
      "loss": 1.3477,
      "step": 5380
    },
    {
      "epoch": 0.8649602824360106,
      "grad_norm": 0.389891117811203,
      "learning_rate": 1.669928525976136e-05,
      "loss": 1.2942,
      "step": 5390
    },
    {
      "epoch": 0.8665650324961888,
      "grad_norm": 0.3290356993675232,
      "learning_rate": 1.6686411771361314e-05,
      "loss": 1.2516,
      "step": 5400
    },
    {
      "epoch": 0.8681697825563669,
      "grad_norm": 0.4714694023132324,
      "learning_rate": 1.667351821046738e-05,
      "loss": 1.3926,
      "step": 5410
    },
    {
      "epoch": 0.869774532616545,
      "grad_norm": 0.41370436549186707,
      "learning_rate": 1.666060461578581e-05,
      "loss": 1.1895,
      "step": 5420
    },
    {
      "epoch": 0.8713792826767232,
      "grad_norm": 0.3037577271461487,
      "learning_rate": 1.664767102608299e-05,
      "loss": 1.4118,
      "step": 5430
    },
    {
      "epoch": 0.8729840327369013,
      "grad_norm": 0.4214405417442322,
      "learning_rate": 1.6634717480185338e-05,
      "loss": 1.455,
      "step": 5440
    },
    {
      "epoch": 0.8745887827970793,
      "grad_norm": 0.5915951132774353,
      "learning_rate": 1.6621744016979176e-05,
      "loss": 1.4248,
      "step": 5450
    },
    {
      "epoch": 0.8761935328572574,
      "grad_norm": 0.45366013050079346,
      "learning_rate": 1.6608750675410624e-05,
      "loss": 1.3524,
      "step": 5460
    },
    {
      "epoch": 0.8777982829174356,
      "grad_norm": 0.3286932408809662,
      "learning_rate": 1.6595737494485463e-05,
      "loss": 1.2665,
      "step": 5470
    },
    {
      "epoch": 0.8794030329776137,
      "grad_norm": 0.39342668652534485,
      "learning_rate": 1.6582704513269048e-05,
      "loss": 1.2325,
      "step": 5480
    },
    {
      "epoch": 0.8810077830377918,
      "grad_norm": 0.5058841109275818,
      "learning_rate": 1.656965177088616e-05,
      "loss": 1.2939,
      "step": 5490
    },
    {
      "epoch": 0.8826125330979699,
      "grad_norm": 0.37228599190711975,
      "learning_rate": 1.655657930652091e-05,
      "loss": 1.2751,
      "step": 5500
    },
    {
      "epoch": 0.8842172831581481,
      "grad_norm": 0.32073843479156494,
      "learning_rate": 1.6543487159416615e-05,
      "loss": 1.308,
      "step": 5510
    },
    {
      "epoch": 0.8858220332183262,
      "grad_norm": 0.40155431628227234,
      "learning_rate": 1.6530375368875674e-05,
      "loss": 1.3307,
      "step": 5520
    },
    {
      "epoch": 0.8874267832785043,
      "grad_norm": 0.5695632100105286,
      "learning_rate": 1.6517243974259457e-05,
      "loss": 1.393,
      "step": 5530
    },
    {
      "epoch": 0.8890315333386825,
      "grad_norm": 0.48095759749412537,
      "learning_rate": 1.650409301498819e-05,
      "loss": 1.3773,
      "step": 5540
    },
    {
      "epoch": 0.8906362833988606,
      "grad_norm": 0.46708589792251587,
      "learning_rate": 1.649092253054083e-05,
      "loss": 1.4356,
      "step": 5550
    },
    {
      "epoch": 0.8922410334590387,
      "grad_norm": 0.29170000553131104,
      "learning_rate": 1.647773256045494e-05,
      "loss": 1.3835,
      "step": 5560
    },
    {
      "epoch": 0.8938457835192168,
      "grad_norm": 0.36648043990135193,
      "learning_rate": 1.646452314432659e-05,
      "loss": 1.2114,
      "step": 5570
    },
    {
      "epoch": 0.895450533579395,
      "grad_norm": 0.343148797750473,
      "learning_rate": 1.6451294321810215e-05,
      "loss": 1.2981,
      "step": 5580
    },
    {
      "epoch": 0.8970552836395731,
      "grad_norm": 0.4367126226425171,
      "learning_rate": 1.6438046132618522e-05,
      "loss": 1.1562,
      "step": 5590
    },
    {
      "epoch": 0.8986600336997512,
      "grad_norm": 0.4520427882671356,
      "learning_rate": 1.6424778616522345e-05,
      "loss": 1.4461,
      "step": 5600
    },
    {
      "epoch": 0.9002647837599294,
      "grad_norm": 0.49677371978759766,
      "learning_rate": 1.641149181335054e-05,
      "loss": 1.4926,
      "step": 5610
    },
    {
      "epoch": 0.9018695338201075,
      "grad_norm": 0.6070894002914429,
      "learning_rate": 1.639818576298986e-05,
      "loss": 1.3126,
      "step": 5620
    },
    {
      "epoch": 0.9034742838802856,
      "grad_norm": 0.3122079074382782,
      "learning_rate": 1.638486050538485e-05,
      "loss": 1.2197,
      "step": 5630
    },
    {
      "epoch": 0.9050790339404637,
      "grad_norm": 0.4353470504283905,
      "learning_rate": 1.6371516080537696e-05,
      "loss": 1.3893,
      "step": 5640
    },
    {
      "epoch": 0.9066837840006419,
      "grad_norm": 0.3354873061180115,
      "learning_rate": 1.635815252850814e-05,
      "loss": 1.4285,
      "step": 5650
    },
    {
      "epoch": 0.90828853406082,
      "grad_norm": 0.3952888250350952,
      "learning_rate": 1.634476988941333e-05,
      "loss": 1.1802,
      "step": 5660
    },
    {
      "epoch": 0.9098932841209981,
      "grad_norm": 0.4878421425819397,
      "learning_rate": 1.6331368203427732e-05,
      "loss": 1.239,
      "step": 5670
    },
    {
      "epoch": 0.9114980341811763,
      "grad_norm": 0.48459434509277344,
      "learning_rate": 1.6317947510782968e-05,
      "loss": 1.3353,
      "step": 5680
    },
    {
      "epoch": 0.9131027842413544,
      "grad_norm": 0.5296024680137634,
      "learning_rate": 1.6304507851767728e-05,
      "loss": 1.3325,
      "step": 5690
    },
    {
      "epoch": 0.9147075343015325,
      "grad_norm": 0.45877718925476074,
      "learning_rate": 1.6291049266727644e-05,
      "loss": 1.3223,
      "step": 5700
    },
    {
      "epoch": 0.9163122843617106,
      "grad_norm": 0.36395880579948425,
      "learning_rate": 1.627757179606515e-05,
      "loss": 1.3718,
      "step": 5710
    },
    {
      "epoch": 0.9179170344218888,
      "grad_norm": 0.6373873949050903,
      "learning_rate": 1.6264075480239394e-05,
      "loss": 1.3696,
      "step": 5720
    },
    {
      "epoch": 0.9195217844820669,
      "grad_norm": 0.5325868129730225,
      "learning_rate": 1.6250560359766078e-05,
      "loss": 1.4063,
      "step": 5730
    },
    {
      "epoch": 0.921126534542245,
      "grad_norm": 0.47153016924858093,
      "learning_rate": 1.6237026475217364e-05,
      "loss": 1.3867,
      "step": 5740
    },
    {
      "epoch": 0.9227312846024232,
      "grad_norm": 0.41277918219566345,
      "learning_rate": 1.6223473867221745e-05,
      "loss": 1.446,
      "step": 5750
    },
    {
      "epoch": 0.9243360346626013,
      "grad_norm": 0.5519567728042603,
      "learning_rate": 1.6209902576463913e-05,
      "loss": 1.3142,
      "step": 5760
    },
    {
      "epoch": 0.9259407847227794,
      "grad_norm": 0.6120237112045288,
      "learning_rate": 1.619631264368466e-05,
      "loss": 1.1825,
      "step": 5770
    },
    {
      "epoch": 0.9275455347829575,
      "grad_norm": 0.6467441916465759,
      "learning_rate": 1.6182704109680722e-05,
      "loss": 1.3095,
      "step": 5780
    },
    {
      "epoch": 0.9291502848431357,
      "grad_norm": 0.45790350437164307,
      "learning_rate": 1.6169077015304687e-05,
      "loss": 1.2986,
      "step": 5790
    },
    {
      "epoch": 0.9307550349033138,
      "grad_norm": 0.4265598952770233,
      "learning_rate": 1.6155431401464868e-05,
      "loss": 1.2973,
      "step": 5800
    },
    {
      "epoch": 0.9323597849634919,
      "grad_norm": 0.38908231258392334,
      "learning_rate": 1.6141767309125157e-05,
      "loss": 1.2323,
      "step": 5810
    },
    {
      "epoch": 0.93396453502367,
      "grad_norm": 0.8979561924934387,
      "learning_rate": 1.6128084779304923e-05,
      "loss": 1.282,
      "step": 5820
    },
    {
      "epoch": 0.9355692850838482,
      "grad_norm": 0.5202555060386658,
      "learning_rate": 1.61143838530789e-05,
      "loss": 1.4424,
      "step": 5830
    },
    {
      "epoch": 0.9371740351440263,
      "grad_norm": 0.666973888874054,
      "learning_rate": 1.6100664571577018e-05,
      "loss": 1.3073,
      "step": 5840
    },
    {
      "epoch": 0.9387787852042044,
      "grad_norm": 0.825140118598938,
      "learning_rate": 1.608692697598434e-05,
      "loss": 1.3351,
      "step": 5850
    },
    {
      "epoch": 0.9403835352643826,
      "grad_norm": 0.4914003610610962,
      "learning_rate": 1.6073171107540883e-05,
      "loss": 1.2839,
      "step": 5860
    },
    {
      "epoch": 0.9419882853245607,
      "grad_norm": 0.40694382786750793,
      "learning_rate": 1.6059397007541533e-05,
      "loss": 1.3518,
      "step": 5870
    },
    {
      "epoch": 0.9435930353847388,
      "grad_norm": 0.621853232383728,
      "learning_rate": 1.60456047173359e-05,
      "loss": 1.3419,
      "step": 5880
    },
    {
      "epoch": 0.945197785444917,
      "grad_norm": 0.40042200684547424,
      "learning_rate": 1.603179427832821e-05,
      "loss": 1.4809,
      "step": 5890
    },
    {
      "epoch": 0.9468025355050951,
      "grad_norm": 0.5344259142875671,
      "learning_rate": 1.6017965731977158e-05,
      "loss": 1.3183,
      "step": 5900
    },
    {
      "epoch": 0.9484072855652732,
      "grad_norm": 0.816536545753479,
      "learning_rate": 1.600411911979581e-05,
      "loss": 1.3356,
      "step": 5910
    },
    {
      "epoch": 0.9500120356254513,
      "grad_norm": 0.48372316360473633,
      "learning_rate": 1.5990254483351453e-05,
      "loss": 1.3854,
      "step": 5920
    },
    {
      "epoch": 0.9516167856856295,
      "grad_norm": 0.49060237407684326,
      "learning_rate": 1.5976371864265493e-05,
      "loss": 1.4454,
      "step": 5930
    },
    {
      "epoch": 0.9532215357458076,
      "grad_norm": 0.4114634394645691,
      "learning_rate": 1.5962471304213315e-05,
      "loss": 1.2264,
      "step": 5940
    },
    {
      "epoch": 0.9548262858059857,
      "grad_norm": 0.4883602559566498,
      "learning_rate": 1.5948552844924158e-05,
      "loss": 1.2849,
      "step": 5950
    },
    {
      "epoch": 0.9564310358661638,
      "grad_norm": 0.393923819065094,
      "learning_rate": 1.593461652818101e-05,
      "loss": 1.2245,
      "step": 5960
    },
    {
      "epoch": 0.958035785926342,
      "grad_norm": 0.4913346767425537,
      "learning_rate": 1.5920662395820442e-05,
      "loss": 1.0894,
      "step": 5970
    },
    {
      "epoch": 0.9596405359865201,
      "grad_norm": 0.3558640778064728,
      "learning_rate": 1.590669048973253e-05,
      "loss": 1.3691,
      "step": 5980
    },
    {
      "epoch": 0.9612452860466982,
      "grad_norm": 0.39837709069252014,
      "learning_rate": 1.5892700851860694e-05,
      "loss": 1.2236,
      "step": 5990
    },
    {
      "epoch": 0.9628500361068764,
      "grad_norm": 0.40076902508735657,
      "learning_rate": 1.5878693524201588e-05,
      "loss": 1.2526,
      "step": 6000
    },
    {
      "epoch": 0.9644547861670545,
      "grad_norm": 0.4735957384109497,
      "learning_rate": 1.5864668548804972e-05,
      "loss": 1.4731,
      "step": 6010
    },
    {
      "epoch": 0.9660595362272326,
      "grad_norm": 0.473957359790802,
      "learning_rate": 1.585062596777358e-05,
      "loss": 1.5207,
      "step": 6020
    },
    {
      "epoch": 0.9676642862874107,
      "grad_norm": 0.4437260925769806,
      "learning_rate": 1.5836565823263e-05,
      "loss": 1.2809,
      "step": 6030
    },
    {
      "epoch": 0.9692690363475889,
      "grad_norm": 0.3869938552379608,
      "learning_rate": 1.582248815748155e-05,
      "loss": 1.4852,
      "step": 6040
    },
    {
      "epoch": 0.970873786407767,
      "grad_norm": 0.35249489545822144,
      "learning_rate": 1.5808393012690143e-05,
      "loss": 1.2395,
      "step": 6050
    },
    {
      "epoch": 0.9724785364679451,
      "grad_norm": 0.5415266752243042,
      "learning_rate": 1.5794280431202153e-05,
      "loss": 1.2244,
      "step": 6060
    },
    {
      "epoch": 0.9740832865281233,
      "grad_norm": 0.450879842042923,
      "learning_rate": 1.5780150455383315e-05,
      "loss": 1.2263,
      "step": 6070
    },
    {
      "epoch": 0.9756880365883014,
      "grad_norm": 0.37794679403305054,
      "learning_rate": 1.5766003127651578e-05,
      "loss": 1.4547,
      "step": 6080
    },
    {
      "epoch": 0.9772927866484795,
      "grad_norm": 0.42578089237213135,
      "learning_rate": 1.5751838490476977e-05,
      "loss": 1.2277,
      "step": 6090
    },
    {
      "epoch": 0.9788975367086576,
      "grad_norm": 0.3842075765132904,
      "learning_rate": 1.5737656586381502e-05,
      "loss": 1.2639,
      "step": 6100
    },
    {
      "epoch": 0.9805022867688358,
      "grad_norm": 0.8705730438232422,
      "learning_rate": 1.5723457457938997e-05,
      "loss": 1.4355,
      "step": 6110
    },
    {
      "epoch": 0.9821070368290139,
      "grad_norm": 0.3839168846607208,
      "learning_rate": 1.5709241147775e-05,
      "loss": 1.2519,
      "step": 6120
    },
    {
      "epoch": 0.983711786889192,
      "grad_norm": 0.6457141637802124,
      "learning_rate": 1.5695007698566625e-05,
      "loss": 1.3103,
      "step": 6130
    },
    {
      "epoch": 0.9853165369493702,
      "grad_norm": 0.44989705085754395,
      "learning_rate": 1.5680757153042455e-05,
      "loss": 1.2612,
      "step": 6140
    },
    {
      "epoch": 0.9869212870095483,
      "grad_norm": 0.3752959072589874,
      "learning_rate": 1.5666489553982376e-05,
      "loss": 1.2201,
      "step": 6150
    },
    {
      "epoch": 0.9885260370697264,
      "grad_norm": 0.3515862226486206,
      "learning_rate": 1.565220494421748e-05,
      "loss": 1.2803,
      "step": 6160
    },
    {
      "epoch": 0.9901307871299045,
      "grad_norm": 0.3068527579307556,
      "learning_rate": 1.563790336662992e-05,
      "loss": 1.1936,
      "step": 6170
    },
    {
      "epoch": 0.9917355371900827,
      "grad_norm": 0.4431750178337097,
      "learning_rate": 1.5623584864152784e-05,
      "loss": 1.2306,
      "step": 6180
    },
    {
      "epoch": 0.9933402872502608,
      "grad_norm": 0.6194785833358765,
      "learning_rate": 1.560924947976998e-05,
      "loss": 1.3348,
      "step": 6190
    },
    {
      "epoch": 0.9949450373104389,
      "grad_norm": 0.5039888620376587,
      "learning_rate": 1.5594897256516075e-05,
      "loss": 1.3855,
      "step": 6200
    },
    {
      "epoch": 0.9965497873706171,
      "grad_norm": 0.6017631888389587,
      "learning_rate": 1.5580528237476208e-05,
      "loss": 1.2783,
      "step": 6210
    },
    {
      "epoch": 0.9981545374307952,
      "grad_norm": 0.574985146522522,
      "learning_rate": 1.556614246578593e-05,
      "loss": 1.2288,
      "step": 6220
    },
    {
      "epoch": 0.9997592874909733,
      "grad_norm": 0.4361003637313843,
      "learning_rate": 1.5551739984631073e-05,
      "loss": 1.3075,
      "step": 6230
    },
    {
      "epoch": 1.0012838000481425,
      "grad_norm": 0.3601723909378052,
      "learning_rate": 1.5537320837247646e-05,
      "loss": 1.2991,
      "step": 6240
    },
    {
      "epoch": 1.0028885501083207,
      "grad_norm": 0.4324955940246582,
      "learning_rate": 1.5522885066921684e-05,
      "loss": 1.3122,
      "step": 6250
    },
    {
      "epoch": 1.0044933001684988,
      "grad_norm": 0.33751097321510315,
      "learning_rate": 1.5508432716989117e-05,
      "loss": 1.4255,
      "step": 6260
    },
    {
      "epoch": 1.006098050228677,
      "grad_norm": 0.4428054690361023,
      "learning_rate": 1.5493963830835658e-05,
      "loss": 1.3473,
      "step": 6270
    },
    {
      "epoch": 1.007702800288855,
      "grad_norm": 0.5338355302810669,
      "learning_rate": 1.5479478451896648e-05,
      "loss": 1.3517,
      "step": 6280
    },
    {
      "epoch": 1.0093075503490332,
      "grad_norm": 0.425942987203598,
      "learning_rate": 1.546497662365696e-05,
      "loss": 1.3183,
      "step": 6290
    },
    {
      "epoch": 1.0109123004092113,
      "grad_norm": 0.5304164290428162,
      "learning_rate": 1.545045838965082e-05,
      "loss": 1.3874,
      "step": 6300
    },
    {
      "epoch": 1.0125170504693894,
      "grad_norm": 0.4224250316619873,
      "learning_rate": 1.543592379346173e-05,
      "loss": 1.2632,
      "step": 6310
    },
    {
      "epoch": 1.0141218005295676,
      "grad_norm": 0.3829323947429657,
      "learning_rate": 1.542137287872229e-05,
      "loss": 1.2622,
      "step": 6320
    },
    {
      "epoch": 1.0157265505897457,
      "grad_norm": 0.35731014609336853,
      "learning_rate": 1.5406805689114107e-05,
      "loss": 1.3027,
      "step": 6330
    },
    {
      "epoch": 1.0173313006499238,
      "grad_norm": 0.4790075421333313,
      "learning_rate": 1.539222226836763e-05,
      "loss": 1.4207,
      "step": 6340
    },
    {
      "epoch": 1.018936050710102,
      "grad_norm": 0.6849955916404724,
      "learning_rate": 1.537762266026204e-05,
      "loss": 1.3475,
      "step": 6350
    },
    {
      "epoch": 1.02054080077028,
      "grad_norm": 0.31947189569473267,
      "learning_rate": 1.536300690862511e-05,
      "loss": 1.3133,
      "step": 6360
    },
    {
      "epoch": 1.0221455508304582,
      "grad_norm": 0.41740599274635315,
      "learning_rate": 1.5348375057333077e-05,
      "loss": 1.2148,
      "step": 6370
    },
    {
      "epoch": 1.0237503008906363,
      "grad_norm": 0.7626890540122986,
      "learning_rate": 1.533372715031051e-05,
      "loss": 1.2917,
      "step": 6380
    },
    {
      "epoch": 1.0253550509508145,
      "grad_norm": 0.4480336904525757,
      "learning_rate": 1.531906323153017e-05,
      "loss": 1.227,
      "step": 6390
    },
    {
      "epoch": 1.0269598010109926,
      "grad_norm": 0.4390794336795807,
      "learning_rate": 1.53043833450129e-05,
      "loss": 1.2819,
      "step": 6400
    },
    {
      "epoch": 1.0285645510711707,
      "grad_norm": 0.3649744689464569,
      "learning_rate": 1.5289687534827463e-05,
      "loss": 1.2173,
      "step": 6410
    },
    {
      "epoch": 1.0301693011313489,
      "grad_norm": 0.5818130970001221,
      "learning_rate": 1.5274975845090433e-05,
      "loss": 1.4234,
      "step": 6420
    },
    {
      "epoch": 1.031774051191527,
      "grad_norm": 0.4034980535507202,
      "learning_rate": 1.5260248319966047e-05,
      "loss": 1.3347,
      "step": 6430
    },
    {
      "epoch": 1.0333788012517051,
      "grad_norm": 0.48886194825172424,
      "learning_rate": 1.5245505003666083e-05,
      "loss": 1.3065,
      "step": 6440
    },
    {
      "epoch": 1.0349835513118832,
      "grad_norm": 0.37109822034835815,
      "learning_rate": 1.5230745940449729e-05,
      "loss": 1.2236,
      "step": 6450
    },
    {
      "epoch": 1.0365883013720614,
      "grad_norm": 0.3626192808151245,
      "learning_rate": 1.5215971174623439e-05,
      "loss": 1.2796,
      "step": 6460
    },
    {
      "epoch": 1.0381930514322395,
      "grad_norm": 0.5692756175994873,
      "learning_rate": 1.5201180750540807e-05,
      "loss": 1.3854,
      "step": 6470
    },
    {
      "epoch": 1.0397978014924176,
      "grad_norm": 0.3761465847492218,
      "learning_rate": 1.5186374712602433e-05,
      "loss": 1.3716,
      "step": 6480
    },
    {
      "epoch": 1.0414025515525958,
      "grad_norm": 0.48764920234680176,
      "learning_rate": 1.5171553105255794e-05,
      "loss": 1.3049,
      "step": 6490
    },
    {
      "epoch": 1.0430073016127739,
      "grad_norm": 0.5362102389335632,
      "learning_rate": 1.5156715972995096e-05,
      "loss": 1.3683,
      "step": 6500
    },
    {
      "epoch": 1.044612051672952,
      "grad_norm": 0.4372890889644623,
      "learning_rate": 1.514186336036116e-05,
      "loss": 1.3398,
      "step": 6510
    },
    {
      "epoch": 1.0462168017331301,
      "grad_norm": 0.645440936088562,
      "learning_rate": 1.5126995311941274e-05,
      "loss": 1.243,
      "step": 6520
    },
    {
      "epoch": 1.0478215517933083,
      "grad_norm": 0.45468226075172424,
      "learning_rate": 1.5112111872369067e-05,
      "loss": 1.4602,
      "step": 6530
    },
    {
      "epoch": 1.0494263018534864,
      "grad_norm": 0.459573358297348,
      "learning_rate": 1.5097213086324367e-05,
      "loss": 1.2885,
      "step": 6540
    },
    {
      "epoch": 1.0510310519136645,
      "grad_norm": 0.4216645061969757,
      "learning_rate": 1.508229899853308e-05,
      "loss": 1.1765,
      "step": 6550
    },
    {
      "epoch": 1.0526358019738427,
      "grad_norm": 0.4805035889148712,
      "learning_rate": 1.5067369653767036e-05,
      "loss": 1.2619,
      "step": 6560
    },
    {
      "epoch": 1.0542405520340208,
      "grad_norm": 0.42158639430999756,
      "learning_rate": 1.505242509684388e-05,
      "loss": 1.3451,
      "step": 6570
    },
    {
      "epoch": 1.055845302094199,
      "grad_norm": 0.4415193200111389,
      "learning_rate": 1.5037465372626914e-05,
      "loss": 1.2924,
      "step": 6580
    },
    {
      "epoch": 1.057450052154377,
      "grad_norm": 0.4799317419528961,
      "learning_rate": 1.5022490526024973e-05,
      "loss": 1.3174,
      "step": 6590
    },
    {
      "epoch": 1.0590548022145552,
      "grad_norm": 0.4865780770778656,
      "learning_rate": 1.5007500601992294e-05,
      "loss": 1.3613,
      "step": 6600
    },
    {
      "epoch": 1.0606595522747333,
      "grad_norm": 0.5198495984077454,
      "learning_rate": 1.4992495645528365e-05,
      "loss": 1.2907,
      "step": 6610
    },
    {
      "epoch": 1.0622643023349114,
      "grad_norm": 0.5259076952934265,
      "learning_rate": 1.4977475701677818e-05,
      "loss": 1.3438,
      "step": 6620
    },
    {
      "epoch": 1.0638690523950896,
      "grad_norm": 0.391799658536911,
      "learning_rate": 1.4962440815530261e-05,
      "loss": 1.2975,
      "step": 6630
    },
    {
      "epoch": 1.0654738024552677,
      "grad_norm": 0.3793030381202698,
      "learning_rate": 1.4947391032220173e-05,
      "loss": 1.2415,
      "step": 6640
    },
    {
      "epoch": 1.0670785525154458,
      "grad_norm": 0.5111414790153503,
      "learning_rate": 1.493232639692674e-05,
      "loss": 1.3922,
      "step": 6650
    },
    {
      "epoch": 1.068683302575624,
      "grad_norm": 0.5521757006645203,
      "learning_rate": 1.4917246954873739e-05,
      "loss": 1.4384,
      "step": 6660
    },
    {
      "epoch": 1.070288052635802,
      "grad_norm": 0.6244081258773804,
      "learning_rate": 1.4902152751329401e-05,
      "loss": 1.3634,
      "step": 6670
    },
    {
      "epoch": 1.0718928026959802,
      "grad_norm": 0.484292596578598,
      "learning_rate": 1.4887043831606267e-05,
      "loss": 1.1858,
      "step": 6680
    },
    {
      "epoch": 1.0734975527561583,
      "grad_norm": 0.36072757840156555,
      "learning_rate": 1.4871920241061056e-05,
      "loss": 1.3018,
      "step": 6690
    },
    {
      "epoch": 1.0751023028163365,
      "grad_norm": 0.4387272894382477,
      "learning_rate": 1.4856782025094527e-05,
      "loss": 1.2818,
      "step": 6700
    },
    {
      "epoch": 1.0767070528765146,
      "grad_norm": 0.38968297839164734,
      "learning_rate": 1.4841629229151345e-05,
      "loss": 1.2832,
      "step": 6710
    },
    {
      "epoch": 1.0783118029366927,
      "grad_norm": 0.6619718074798584,
      "learning_rate": 1.4826461898719944e-05,
      "loss": 1.3435,
      "step": 6720
    },
    {
      "epoch": 1.0799165529968708,
      "grad_norm": 0.5053989887237549,
      "learning_rate": 1.4811280079332393e-05,
      "loss": 1.2909,
      "step": 6730
    },
    {
      "epoch": 1.081521303057049,
      "grad_norm": 0.4098435044288635,
      "learning_rate": 1.4796083816564255e-05,
      "loss": 1.2031,
      "step": 6740
    },
    {
      "epoch": 1.083126053117227,
      "grad_norm": 0.4420308768749237,
      "learning_rate": 1.4780873156034447e-05,
      "loss": 1.4824,
      "step": 6750
    },
    {
      "epoch": 1.0847308031774052,
      "grad_norm": 0.3314979374408722,
      "learning_rate": 1.4765648143405118e-05,
      "loss": 1.3183,
      "step": 6760
    },
    {
      "epoch": 1.0863355532375834,
      "grad_norm": 0.3510865569114685,
      "learning_rate": 1.4750408824381491e-05,
      "loss": 1.2142,
      "step": 6770
    },
    {
      "epoch": 1.0879403032977613,
      "grad_norm": 0.7053998112678528,
      "learning_rate": 1.4735155244711741e-05,
      "loss": 1.315,
      "step": 6780
    },
    {
      "epoch": 1.0895450533579396,
      "grad_norm": 0.5080541968345642,
      "learning_rate": 1.4719887450186858e-05,
      "loss": 1.3033,
      "step": 6790
    },
    {
      "epoch": 1.0911498034181175,
      "grad_norm": 0.47557002305984497,
      "learning_rate": 1.4704605486640495e-05,
      "loss": 1.2938,
      "step": 6800
    },
    {
      "epoch": 1.0927545534782959,
      "grad_norm": 0.4491935968399048,
      "learning_rate": 1.468930939994885e-05,
      "loss": 1.3345,
      "step": 6810
    },
    {
      "epoch": 1.0943593035384738,
      "grad_norm": 0.44362714886665344,
      "learning_rate": 1.4673999236030514e-05,
      "loss": 1.1347,
      "step": 6820
    },
    {
      "epoch": 1.0959640535986521,
      "grad_norm": 0.4518422782421112,
      "learning_rate": 1.4658675040846333e-05,
      "loss": 1.3781,
      "step": 6830
    },
    {
      "epoch": 1.09756880365883,
      "grad_norm": 0.41093572974205017,
      "learning_rate": 1.4643336860399282e-05,
      "loss": 1.3406,
      "step": 6840
    },
    {
      "epoch": 1.0991735537190084,
      "grad_norm": 0.6091871857643127,
      "learning_rate": 1.4627984740734316e-05,
      "loss": 1.3216,
      "step": 6850
    },
    {
      "epoch": 1.1007783037791863,
      "grad_norm": 0.7222230434417725,
      "learning_rate": 1.4612618727938235e-05,
      "loss": 1.3419,
      "step": 6860
    },
    {
      "epoch": 1.1023830538393646,
      "grad_norm": 0.5202787518501282,
      "learning_rate": 1.459723886813955e-05,
      "loss": 1.2601,
      "step": 6870
    },
    {
      "epoch": 1.1039878038995425,
      "grad_norm": 0.3911527395248413,
      "learning_rate": 1.4581845207508333e-05,
      "loss": 1.2106,
      "step": 6880
    },
    {
      "epoch": 1.1055925539597207,
      "grad_norm": 0.41159531474113464,
      "learning_rate": 1.4566437792256097e-05,
      "loss": 1.1649,
      "step": 6890
    },
    {
      "epoch": 1.1071973040198988,
      "grad_norm": 0.5409526228904724,
      "learning_rate": 1.4551016668635632e-05,
      "loss": 1.2035,
      "step": 6900
    },
    {
      "epoch": 1.108802054080077,
      "grad_norm": 0.30948135256767273,
      "learning_rate": 1.4535581882940888e-05,
      "loss": 1.2175,
      "step": 6910
    },
    {
      "epoch": 1.110406804140255,
      "grad_norm": 0.4835200607776642,
      "learning_rate": 1.4520133481506833e-05,
      "loss": 1.3498,
      "step": 6920
    },
    {
      "epoch": 1.1120115542004332,
      "grad_norm": 0.4115942716598511,
      "learning_rate": 1.45046715107093e-05,
      "loss": 1.2632,
      "step": 6930
    },
    {
      "epoch": 1.1136163042606113,
      "grad_norm": 0.8460289835929871,
      "learning_rate": 1.4489196016964865e-05,
      "loss": 1.3076,
      "step": 6940
    },
    {
      "epoch": 1.1152210543207894,
      "grad_norm": 0.2751508057117462,
      "learning_rate": 1.4473707046730692e-05,
      "loss": 1.3567,
      "step": 6950
    },
    {
      "epoch": 1.1168258043809676,
      "grad_norm": 0.6270081996917725,
      "learning_rate": 1.4458204646504406e-05,
      "loss": 1.2648,
      "step": 6960
    },
    {
      "epoch": 1.1184305544411457,
      "grad_norm": 0.339470773935318,
      "learning_rate": 1.4442688862823947e-05,
      "loss": 1.2423,
      "step": 6970
    },
    {
      "epoch": 1.1200353045013238,
      "grad_norm": 0.3658987879753113,
      "learning_rate": 1.4427159742267436e-05,
      "loss": 1.3307,
      "step": 6980
    },
    {
      "epoch": 1.121640054561502,
      "grad_norm": 0.5582230091094971,
      "learning_rate": 1.4411617331453024e-05,
      "loss": 1.2053,
      "step": 6990
    },
    {
      "epoch": 1.12324480462168,
      "grad_norm": 0.37663304805755615,
      "learning_rate": 1.4396061677038764e-05,
      "loss": 1.1812,
      "step": 7000
    },
    {
      "epoch": 1.1248495546818582,
      "grad_norm": 0.38784775137901306,
      "learning_rate": 1.4380492825722464e-05,
      "loss": 1.4165,
      "step": 7010
    },
    {
      "epoch": 1.1264543047420363,
      "grad_norm": 0.3521536588668823,
      "learning_rate": 1.436491082424155e-05,
      "loss": 1.3488,
      "step": 7020
    },
    {
      "epoch": 1.1280590548022145,
      "grad_norm": 0.3474646508693695,
      "learning_rate": 1.4349315719372925e-05,
      "loss": 1.3054,
      "step": 7030
    },
    {
      "epoch": 1.1296638048623926,
      "grad_norm": 0.3786047399044037,
      "learning_rate": 1.4333707557932825e-05,
      "loss": 1.0053,
      "step": 7040
    },
    {
      "epoch": 1.1312685549225707,
      "grad_norm": 0.4184497594833374,
      "learning_rate": 1.4318086386776688e-05,
      "loss": 1.4007,
      "step": 7050
    },
    {
      "epoch": 1.1328733049827489,
      "grad_norm": 0.43812254071235657,
      "learning_rate": 1.4302452252799e-05,
      "loss": 1.4642,
      "step": 7060
    },
    {
      "epoch": 1.134478055042927,
      "grad_norm": 0.49178361892700195,
      "learning_rate": 1.4286805202933162e-05,
      "loss": 1.3949,
      "step": 7070
    },
    {
      "epoch": 1.136082805103105,
      "grad_norm": 0.4279196560382843,
      "learning_rate": 1.4271145284151355e-05,
      "loss": 1.4526,
      "step": 7080
    },
    {
      "epoch": 1.1376875551632832,
      "grad_norm": 0.5261572599411011,
      "learning_rate": 1.4255472543464385e-05,
      "loss": 1.4703,
      "step": 7090
    },
    {
      "epoch": 1.1392923052234614,
      "grad_norm": 0.6121659874916077,
      "learning_rate": 1.4239787027921555e-05,
      "loss": 1.4138,
      "step": 7100
    },
    {
      "epoch": 1.1408970552836395,
      "grad_norm": 0.3736075460910797,
      "learning_rate": 1.4224088784610507e-05,
      "loss": 1.4209,
      "step": 7110
    },
    {
      "epoch": 1.1425018053438176,
      "grad_norm": 0.5189878344535828,
      "learning_rate": 1.4208377860657108e-05,
      "loss": 1.4667,
      "step": 7120
    },
    {
      "epoch": 1.1441065554039958,
      "grad_norm": 0.40697869658470154,
      "learning_rate": 1.4192654303225277e-05,
      "loss": 1.2793,
      "step": 7130
    },
    {
      "epoch": 1.1457113054641739,
      "grad_norm": 0.396034300327301,
      "learning_rate": 1.4176918159516869e-05,
      "loss": 1.42,
      "step": 7140
    },
    {
      "epoch": 1.147316055524352,
      "grad_norm": 0.5505780577659607,
      "learning_rate": 1.4161169476771515e-05,
      "loss": 1.4229,
      "step": 7150
    },
    {
      "epoch": 1.1489208055845301,
      "grad_norm": 0.4597626030445099,
      "learning_rate": 1.4145408302266494e-05,
      "loss": 1.3511,
      "step": 7160
    },
    {
      "epoch": 1.1505255556447083,
      "grad_norm": 0.46073710918426514,
      "learning_rate": 1.412963468331658e-05,
      "loss": 1.2401,
      "step": 7170
    },
    {
      "epoch": 1.1521303057048864,
      "grad_norm": 0.8317685723304749,
      "learning_rate": 1.4113848667273913e-05,
      "loss": 1.2792,
      "step": 7180
    },
    {
      "epoch": 1.1537350557650645,
      "grad_norm": 0.6350285410881042,
      "learning_rate": 1.4098050301527835e-05,
      "loss": 1.3959,
      "step": 7190
    },
    {
      "epoch": 1.1553398058252426,
      "grad_norm": 0.6157631874084473,
      "learning_rate": 1.4082239633504778e-05,
      "loss": 1.3762,
      "step": 7200
    },
    {
      "epoch": 1.1569445558854208,
      "grad_norm": 0.5284351706504822,
      "learning_rate": 1.4066416710668093e-05,
      "loss": 1.3333,
      "step": 7210
    },
    {
      "epoch": 1.158549305945599,
      "grad_norm": 0.6936942338943481,
      "learning_rate": 1.4050581580517925e-05,
      "loss": 1.409,
      "step": 7220
    },
    {
      "epoch": 1.160154056005777,
      "grad_norm": 0.6341999173164368,
      "learning_rate": 1.4034734290591064e-05,
      "loss": 1.4347,
      "step": 7230
    },
    {
      "epoch": 1.1617588060659552,
      "grad_norm": 0.29309824109077454,
      "learning_rate": 1.4018874888460803e-05,
      "loss": 1.2808,
      "step": 7240
    },
    {
      "epoch": 1.1633635561261333,
      "grad_norm": 0.6630398631095886,
      "learning_rate": 1.4003003421736798e-05,
      "loss": 1.2953,
      "step": 7250
    },
    {
      "epoch": 1.1649683061863114,
      "grad_norm": 0.3834102749824524,
      "learning_rate": 1.3987119938064919e-05,
      "loss": 1.173,
      "step": 7260
    },
    {
      "epoch": 1.1665730562464895,
      "grad_norm": 0.5507009029388428,
      "learning_rate": 1.3971224485127116e-05,
      "loss": 1.4009,
      "step": 7270
    },
    {
      "epoch": 1.1681778063066677,
      "grad_norm": 0.49438032507896423,
      "learning_rate": 1.395531711064126e-05,
      "loss": 1.2463,
      "step": 7280
    },
    {
      "epoch": 1.1697825563668458,
      "grad_norm": 0.4966919720172882,
      "learning_rate": 1.3939397862361026e-05,
      "loss": 1.3507,
      "step": 7290
    },
    {
      "epoch": 1.171387306427024,
      "grad_norm": 0.41381627321243286,
      "learning_rate": 1.3923466788075719e-05,
      "loss": 1.3816,
      "step": 7300
    },
    {
      "epoch": 1.172992056487202,
      "grad_norm": 0.40047743916511536,
      "learning_rate": 1.3907523935610154e-05,
      "loss": 1.286,
      "step": 7310
    },
    {
      "epoch": 1.1745968065473802,
      "grad_norm": 0.45795977115631104,
      "learning_rate": 1.3891569352824501e-05,
      "loss": 1.2834,
      "step": 7320
    },
    {
      "epoch": 1.1762015566075583,
      "grad_norm": 0.42646050453186035,
      "learning_rate": 1.3875603087614147e-05,
      "loss": 1.3431,
      "step": 7330
    },
    {
      "epoch": 1.1778063066677364,
      "grad_norm": 0.5583655834197998,
      "learning_rate": 1.3859625187909545e-05,
      "loss": 1.3837,
      "step": 7340
    },
    {
      "epoch": 1.1794110567279146,
      "grad_norm": 0.7619643211364746,
      "learning_rate": 1.3843635701676078e-05,
      "loss": 1.3277,
      "step": 7350
    },
    {
      "epoch": 1.1810158067880927,
      "grad_norm": 0.4550929665565491,
      "learning_rate": 1.382763467691391e-05,
      "loss": 1.1936,
      "step": 7360
    },
    {
      "epoch": 1.1826205568482708,
      "grad_norm": 0.39123114943504333,
      "learning_rate": 1.3811622161657844e-05,
      "loss": 1.2075,
      "step": 7370
    },
    {
      "epoch": 1.184225306908449,
      "grad_norm": 0.5276375412940979,
      "learning_rate": 1.3795598203977179e-05,
      "loss": 1.2623,
      "step": 7380
    },
    {
      "epoch": 1.185830056968627,
      "grad_norm": 0.34813618659973145,
      "learning_rate": 1.377956285197556e-05,
      "loss": 1.3291,
      "step": 7390
    },
    {
      "epoch": 1.1874348070288052,
      "grad_norm": 0.5610764026641846,
      "learning_rate": 1.3763516153790839e-05,
      "loss": 1.4134,
      "step": 7400
    },
    {
      "epoch": 1.1890395570889833,
      "grad_norm": 0.41077131032943726,
      "learning_rate": 1.3747458157594932e-05,
      "loss": 1.2891,
      "step": 7410
    },
    {
      "epoch": 1.1906443071491615,
      "grad_norm": 0.3441135585308075,
      "learning_rate": 1.3731388911593669e-05,
      "loss": 1.4564,
      "step": 7420
    },
    {
      "epoch": 1.1922490572093396,
      "grad_norm": 0.4646686613559723,
      "learning_rate": 1.3715308464026649e-05,
      "loss": 1.41,
      "step": 7430
    },
    {
      "epoch": 1.1938538072695177,
      "grad_norm": 0.38971275091171265,
      "learning_rate": 1.3699216863167103e-05,
      "loss": 1.3341,
      "step": 7440
    },
    {
      "epoch": 1.1954585573296959,
      "grad_norm": 0.38166874647140503,
      "learning_rate": 1.3683114157321745e-05,
      "loss": 1.1631,
      "step": 7450
    },
    {
      "epoch": 1.197063307389874,
      "grad_norm": 0.35580551624298096,
      "learning_rate": 1.3667000394830617e-05,
      "loss": 1.248,
      "step": 7460
    },
    {
      "epoch": 1.1986680574500521,
      "grad_norm": 0.43652981519699097,
      "learning_rate": 1.3650875624066964e-05,
      "loss": 1.2756,
      "step": 7470
    },
    {
      "epoch": 1.2002728075102302,
      "grad_norm": 0.42601877450942993,
      "learning_rate": 1.3634739893437066e-05,
      "loss": 1.3872,
      "step": 7480
    },
    {
      "epoch": 1.2018775575704084,
      "grad_norm": 0.3693404197692871,
      "learning_rate": 1.3618593251380117e-05,
      "loss": 1.3187,
      "step": 7490
    },
    {
      "epoch": 1.2034823076305865,
      "grad_norm": 0.41958141326904297,
      "learning_rate": 1.3602435746368055e-05,
      "loss": 1.2432,
      "step": 7500
    },
    {
      "epoch": 1.2050870576907646,
      "grad_norm": 0.3722675144672394,
      "learning_rate": 1.358626742690544e-05,
      "loss": 1.2077,
      "step": 7510
    },
    {
      "epoch": 1.2066918077509428,
      "grad_norm": 0.38971027731895447,
      "learning_rate": 1.3570088341529286e-05,
      "loss": 1.3264,
      "step": 7520
    },
    {
      "epoch": 1.2082965578111209,
      "grad_norm": 0.433913916349411,
      "learning_rate": 1.3553898538808932e-05,
      "loss": 1.3777,
      "step": 7530
    },
    {
      "epoch": 1.209901307871299,
      "grad_norm": 0.3125605285167694,
      "learning_rate": 1.3537698067345891e-05,
      "loss": 1.1755,
      "step": 7540
    },
    {
      "epoch": 1.2115060579314771,
      "grad_norm": 0.6322104930877686,
      "learning_rate": 1.35214869757737e-05,
      "loss": 1.3435,
      "step": 7550
    },
    {
      "epoch": 1.2131108079916553,
      "grad_norm": 0.33701908588409424,
      "learning_rate": 1.3505265312757776e-05,
      "loss": 1.3467,
      "step": 7560
    },
    {
      "epoch": 1.2147155580518334,
      "grad_norm": 0.5904881358146667,
      "learning_rate": 1.348903312699528e-05,
      "loss": 1.269,
      "step": 7570
    },
    {
      "epoch": 1.2163203081120115,
      "grad_norm": 0.34604138135910034,
      "learning_rate": 1.3472790467214951e-05,
      "loss": 1.3224,
      "step": 7580
    },
    {
      "epoch": 1.2179250581721897,
      "grad_norm": 0.42672184109687805,
      "learning_rate": 1.3456537382176982e-05,
      "loss": 1.4349,
      "step": 7590
    },
    {
      "epoch": 1.2195298082323678,
      "grad_norm": 0.5021414756774902,
      "learning_rate": 1.344027392067285e-05,
      "loss": 1.2085,
      "step": 7600
    },
    {
      "epoch": 1.221134558292546,
      "grad_norm": 0.40774115920066833,
      "learning_rate": 1.3424000131525193e-05,
      "loss": 1.3823,
      "step": 7610
    },
    {
      "epoch": 1.222739308352724,
      "grad_norm": 0.6307203769683838,
      "learning_rate": 1.3407716063587645e-05,
      "loss": 1.3097,
      "step": 7620
    },
    {
      "epoch": 1.2243440584129022,
      "grad_norm": 0.7373653650283813,
      "learning_rate": 1.3391421765744706e-05,
      "loss": 1.3351,
      "step": 7630
    },
    {
      "epoch": 1.2259488084730803,
      "grad_norm": 0.8907294869422913,
      "learning_rate": 1.337511728691157e-05,
      "loss": 1.3109,
      "step": 7640
    },
    {
      "epoch": 1.2275535585332584,
      "grad_norm": 0.4963296055793762,
      "learning_rate": 1.335880267603401e-05,
      "loss": 1.3072,
      "step": 7650
    },
    {
      "epoch": 1.2291583085934366,
      "grad_norm": 0.3151059150695801,
      "learning_rate": 1.3342477982088209e-05,
      "loss": 1.341,
      "step": 7660
    },
    {
      "epoch": 1.2307630586536147,
      "grad_norm": 0.613586962223053,
      "learning_rate": 1.332614325408062e-05,
      "loss": 1.4259,
      "step": 7670
    },
    {
      "epoch": 1.2323678087137928,
      "grad_norm": 0.42419764399528503,
      "learning_rate": 1.3309798541047813e-05,
      "loss": 1.2646,
      "step": 7680
    },
    {
      "epoch": 1.233972558773971,
      "grad_norm": 0.5329625606536865,
      "learning_rate": 1.329344389205634e-05,
      "loss": 1.3818,
      "step": 7690
    },
    {
      "epoch": 1.235577308834149,
      "grad_norm": 0.46859243512153625,
      "learning_rate": 1.3277079356202578e-05,
      "loss": 1.3215,
      "step": 7700
    },
    {
      "epoch": 1.2371820588943272,
      "grad_norm": 0.4753597378730774,
      "learning_rate": 1.3260704982612584e-05,
      "loss": 1.2435,
      "step": 7710
    },
    {
      "epoch": 1.2387868089545053,
      "grad_norm": 0.44442829489707947,
      "learning_rate": 1.3244320820441946e-05,
      "loss": 1.3199,
      "step": 7720
    },
    {
      "epoch": 1.2403915590146835,
      "grad_norm": 0.47191905975341797,
      "learning_rate": 1.3227926918875644e-05,
      "loss": 1.2851,
      "step": 7730
    },
    {
      "epoch": 1.2419963090748616,
      "grad_norm": 0.45169103145599365,
      "learning_rate": 1.3211523327127883e-05,
      "loss": 1.1977,
      "step": 7740
    },
    {
      "epoch": 1.2436010591350397,
      "grad_norm": 0.4244837164878845,
      "learning_rate": 1.3195110094441972e-05,
      "loss": 1.3071,
      "step": 7750
    },
    {
      "epoch": 1.2452058091952178,
      "grad_norm": 0.4697318375110626,
      "learning_rate": 1.317868727009015e-05,
      "loss": 1.1962,
      "step": 7760
    },
    {
      "epoch": 1.246810559255396,
      "grad_norm": 0.6292215585708618,
      "learning_rate": 1.3162254903373457e-05,
      "loss": 1.5945,
      "step": 7770
    },
    {
      "epoch": 1.248415309315574,
      "grad_norm": 0.6986608505249023,
      "learning_rate": 1.3145813043621578e-05,
      "loss": 1.3643,
      "step": 7780
    },
    {
      "epoch": 1.2500200593757522,
      "grad_norm": 0.4346173405647278,
      "learning_rate": 1.3129361740192694e-05,
      "loss": 1.1721,
      "step": 7790
    },
    {
      "epoch": 1.2516248094359304,
      "grad_norm": 0.3822895586490631,
      "learning_rate": 1.3112901042473336e-05,
      "loss": 1.2735,
      "step": 7800
    },
    {
      "epoch": 1.2532295594961085,
      "grad_norm": 0.5531186461448669,
      "learning_rate": 1.3096430999878238e-05,
      "loss": 1.2641,
      "step": 7810
    },
    {
      "epoch": 1.2548343095562866,
      "grad_norm": 0.439873069524765,
      "learning_rate": 1.3079951661850187e-05,
      "loss": 1.276,
      "step": 7820
    },
    {
      "epoch": 1.2564390596164647,
      "grad_norm": 0.6348855495452881,
      "learning_rate": 1.306346307785988e-05,
      "loss": 1.2874,
      "step": 7830
    },
    {
      "epoch": 1.2580438096766429,
      "grad_norm": 0.48618176579475403,
      "learning_rate": 1.3046965297405751e-05,
      "loss": 1.3119,
      "step": 7840
    },
    {
      "epoch": 1.259648559736821,
      "grad_norm": 0.47632744908332825,
      "learning_rate": 1.3030458370013863e-05,
      "loss": 1.2751,
      "step": 7850
    },
    {
      "epoch": 1.2612533097969991,
      "grad_norm": 0.6458721160888672,
      "learning_rate": 1.3013942345237731e-05,
      "loss": 1.252,
      "step": 7860
    },
    {
      "epoch": 1.2628580598571773,
      "grad_norm": 0.376121461391449,
      "learning_rate": 1.2997417272658178e-05,
      "loss": 1.2583,
      "step": 7870
    },
    {
      "epoch": 1.2644628099173554,
      "grad_norm": 0.3605959415435791,
      "learning_rate": 1.2980883201883186e-05,
      "loss": 1.1997,
      "step": 7880
    },
    {
      "epoch": 1.2660675599775335,
      "grad_norm": 0.5622216463088989,
      "learning_rate": 1.2964340182547758e-05,
      "loss": 1.3531,
      "step": 7890
    },
    {
      "epoch": 1.2676723100377116,
      "grad_norm": 0.9512320756912231,
      "learning_rate": 1.2947788264313753e-05,
      "loss": 1.2665,
      "step": 7900
    },
    {
      "epoch": 1.2692770600978898,
      "grad_norm": 0.5057741403579712,
      "learning_rate": 1.2931227496869747e-05,
      "loss": 1.2972,
      "step": 7910
    },
    {
      "epoch": 1.270881810158068,
      "grad_norm": 0.2990984320640564,
      "learning_rate": 1.291465792993088e-05,
      "loss": 1.349,
      "step": 7920
    },
    {
      "epoch": 1.272486560218246,
      "grad_norm": 0.45057356357574463,
      "learning_rate": 1.2898079613238712e-05,
      "loss": 1.2777,
      "step": 7930
    },
    {
      "epoch": 1.2740913102784241,
      "grad_norm": 0.5843234658241272,
      "learning_rate": 1.2881492596561063e-05,
      "loss": 1.2019,
      "step": 7940
    },
    {
      "epoch": 1.2756960603386023,
      "grad_norm": 0.4822700619697571,
      "learning_rate": 1.2864896929691874e-05,
      "loss": 1.2658,
      "step": 7950
    },
    {
      "epoch": 1.2773008103987804,
      "grad_norm": 0.5484413504600525,
      "learning_rate": 1.2848292662451055e-05,
      "loss": 1.2573,
      "step": 7960
    },
    {
      "epoch": 1.2789055604589585,
      "grad_norm": 0.28138402104377747,
      "learning_rate": 1.2831679844684332e-05,
      "loss": 1.3079,
      "step": 7970
    },
    {
      "epoch": 1.2805103105191367,
      "grad_norm": 0.645535945892334,
      "learning_rate": 1.28150585262631e-05,
      "loss": 1.5523,
      "step": 7980
    },
    {
      "epoch": 1.2821150605793148,
      "grad_norm": 0.41554227471351624,
      "learning_rate": 1.2798428757084271e-05,
      "loss": 1.1431,
      "step": 7990
    },
    {
      "epoch": 1.283719810639493,
      "grad_norm": 0.6188637614250183,
      "learning_rate": 1.2781790587070131e-05,
      "loss": 1.2114,
      "step": 8000
    },
    {
      "epoch": 1.285324560699671,
      "grad_norm": 0.39754635095596313,
      "learning_rate": 1.276514406616818e-05,
      "loss": 1.2338,
      "step": 8010
    },
    {
      "epoch": 1.2869293107598492,
      "grad_norm": 0.6637294888496399,
      "learning_rate": 1.2748489244350992e-05,
      "loss": 1.1554,
      "step": 8020
    },
    {
      "epoch": 1.2885340608200273,
      "grad_norm": 0.47590771317481995,
      "learning_rate": 1.2731826171616053e-05,
      "loss": 1.3423,
      "step": 8030
    },
    {
      "epoch": 1.2901388108802054,
      "grad_norm": 0.45480504631996155,
      "learning_rate": 1.2715154897985627e-05,
      "loss": 1.4462,
      "step": 8040
    },
    {
      "epoch": 1.2917435609403836,
      "grad_norm": 0.60245281457901,
      "learning_rate": 1.269847547350659e-05,
      "loss": 1.3435,
      "step": 8050
    },
    {
      "epoch": 1.2933483110005617,
      "grad_norm": 0.3781774938106537,
      "learning_rate": 1.268178794825029e-05,
      "loss": 1.226,
      "step": 8060
    },
    {
      "epoch": 1.2949530610607398,
      "grad_norm": 0.5589027404785156,
      "learning_rate": 1.2665092372312393e-05,
      "loss": 1.3327,
      "step": 8070
    },
    {
      "epoch": 1.296557811120918,
      "grad_norm": 0.3527999222278595,
      "learning_rate": 1.2648388795812732e-05,
      "loss": 1.2599,
      "step": 8080
    },
    {
      "epoch": 1.298162561181096,
      "grad_norm": 0.36413395404815674,
      "learning_rate": 1.263167726889516e-05,
      "loss": 1.3486,
      "step": 8090
    },
    {
      "epoch": 1.2997673112412742,
      "grad_norm": 0.588789701461792,
      "learning_rate": 1.2614957841727391e-05,
      "loss": 1.2093,
      "step": 8100
    },
    {
      "epoch": 1.3013720613014523,
      "grad_norm": 0.7329758405685425,
      "learning_rate": 1.2598230564500869e-05,
      "loss": 1.1892,
      "step": 8110
    },
    {
      "epoch": 1.3029768113616305,
      "grad_norm": 0.33255577087402344,
      "learning_rate": 1.2581495487430579e-05,
      "loss": 1.3614,
      "step": 8120
    },
    {
      "epoch": 1.3045815614218086,
      "grad_norm": 0.4325100779533386,
      "learning_rate": 1.2564752660754951e-05,
      "loss": 1.3827,
      "step": 8130
    },
    {
      "epoch": 1.3061863114819867,
      "grad_norm": 0.46744903922080994,
      "learning_rate": 1.2548002134735657e-05,
      "loss": 1.1451,
      "step": 8140
    },
    {
      "epoch": 1.3077910615421648,
      "grad_norm": 0.36994969844818115,
      "learning_rate": 1.2531243959657493e-05,
      "loss": 1.3015,
      "step": 8150
    },
    {
      "epoch": 1.309395811602343,
      "grad_norm": 0.41128799319267273,
      "learning_rate": 1.2514478185828214e-05,
      "loss": 1.2192,
      "step": 8160
    },
    {
      "epoch": 1.311000561662521,
      "grad_norm": 0.4450591802597046,
      "learning_rate": 1.2497704863578384e-05,
      "loss": 1.3085,
      "step": 8170
    },
    {
      "epoch": 1.3126053117226992,
      "grad_norm": 0.48298510909080505,
      "learning_rate": 1.248092404326123e-05,
      "loss": 1.3365,
      "step": 8180
    },
    {
      "epoch": 1.3142100617828774,
      "grad_norm": 0.3929745852947235,
      "learning_rate": 1.2464135775252493e-05,
      "loss": 1.2866,
      "step": 8190
    },
    {
      "epoch": 1.3158148118430555,
      "grad_norm": 0.34664878249168396,
      "learning_rate": 1.2447340109950262e-05,
      "loss": 1.1876,
      "step": 8200
    },
    {
      "epoch": 1.3174195619032336,
      "grad_norm": 0.6476680040359497,
      "learning_rate": 1.2430537097774838e-05,
      "loss": 1.2697,
      "step": 8210
    },
    {
      "epoch": 1.3190243119634117,
      "grad_norm": 0.517227292060852,
      "learning_rate": 1.2413726789168576e-05,
      "loss": 1.347,
      "step": 8220
    },
    {
      "epoch": 1.3206290620235899,
      "grad_norm": 0.49171507358551025,
      "learning_rate": 1.2396909234595737e-05,
      "loss": 1.3026,
      "step": 8230
    },
    {
      "epoch": 1.322233812083768,
      "grad_norm": 0.4261479675769806,
      "learning_rate": 1.2380084484542333e-05,
      "loss": 1.4349,
      "step": 8240
    },
    {
      "epoch": 1.3238385621439461,
      "grad_norm": 0.5554542541503906,
      "learning_rate": 1.236325258951597e-05,
      "loss": 1.1946,
      "step": 8250
    },
    {
      "epoch": 1.3254433122041243,
      "grad_norm": 0.3498288691043854,
      "learning_rate": 1.2346413600045717e-05,
      "loss": 1.4893,
      "step": 8260
    },
    {
      "epoch": 1.3270480622643024,
      "grad_norm": 0.34357771277427673,
      "learning_rate": 1.2329567566681931e-05,
      "loss": 1.3772,
      "step": 8270
    },
    {
      "epoch": 1.3286528123244805,
      "grad_norm": 0.5095021724700928,
      "learning_rate": 1.2312714539996111e-05,
      "loss": 1.2766,
      "step": 8280
    },
    {
      "epoch": 1.3302575623846586,
      "grad_norm": 0.7153652906417847,
      "learning_rate": 1.2295854570580761e-05,
      "loss": 1.5027,
      "step": 8290
    },
    {
      "epoch": 1.3318623124448368,
      "grad_norm": 0.3345617949962616,
      "learning_rate": 1.227898770904922e-05,
      "loss": 1.1703,
      "step": 8300
    },
    {
      "epoch": 1.333467062505015,
      "grad_norm": 0.5505359172821045,
      "learning_rate": 1.2262114006035517e-05,
      "loss": 1.2998,
      "step": 8310
    },
    {
      "epoch": 1.335071812565193,
      "grad_norm": 0.4179230034351349,
      "learning_rate": 1.224523351219422e-05,
      "loss": 1.2123,
      "step": 8320
    },
    {
      "epoch": 1.3366765626253712,
      "grad_norm": 0.47714290022850037,
      "learning_rate": 1.2228346278200282e-05,
      "loss": 1.3336,
      "step": 8330
    },
    {
      "epoch": 1.3382813126855493,
      "grad_norm": 0.5441111326217651,
      "learning_rate": 1.2211452354748894e-05,
      "loss": 1.428,
      "step": 8340
    },
    {
      "epoch": 1.3398860627457274,
      "grad_norm": 0.48453477025032043,
      "learning_rate": 1.2194551792555323e-05,
      "loss": 1.283,
      "step": 8350
    },
    {
      "epoch": 1.3414908128059055,
      "grad_norm": 0.34199202060699463,
      "learning_rate": 1.217764464235477e-05,
      "loss": 1.212,
      "step": 8360
    },
    {
      "epoch": 1.3430955628660837,
      "grad_norm": 0.4048543870449066,
      "learning_rate": 1.2160730954902208e-05,
      "loss": 1.2824,
      "step": 8370
    },
    {
      "epoch": 1.3447003129262618,
      "grad_norm": 0.5567260980606079,
      "learning_rate": 1.214381078097224e-05,
      "loss": 1.1986,
      "step": 8380
    },
    {
      "epoch": 1.34630506298644,
      "grad_norm": 0.6571035981178284,
      "learning_rate": 1.2126884171358941e-05,
      "loss": 1.2567,
      "step": 8390
    },
    {
      "epoch": 1.347909813046618,
      "grad_norm": 0.6787120699882507,
      "learning_rate": 1.21099511768757e-05,
      "loss": 1.4128,
      "step": 8400
    },
    {
      "epoch": 1.3495145631067962,
      "grad_norm": 0.4723717272281647,
      "learning_rate": 1.2093011848355077e-05,
      "loss": 1.3208,
      "step": 8410
    },
    {
      "epoch": 1.3511193131669743,
      "grad_norm": 0.4018743336200714,
      "learning_rate": 1.2076066236648649e-05,
      "loss": 1.3074,
      "step": 8420
    },
    {
      "epoch": 1.3527240632271524,
      "grad_norm": 0.37755438685417175,
      "learning_rate": 1.2059114392626852e-05,
      "loss": 1.3438,
      "step": 8430
    },
    {
      "epoch": 1.3543288132873306,
      "grad_norm": 0.45434141159057617,
      "learning_rate": 1.2042156367178833e-05,
      "loss": 1.32,
      "step": 8440
    },
    {
      "epoch": 1.3559335633475087,
      "grad_norm": 0.4530329704284668,
      "learning_rate": 1.2025192211212293e-05,
      "loss": 1.2147,
      "step": 8450
    },
    {
      "epoch": 1.3575383134076868,
      "grad_norm": 0.4891365170478821,
      "learning_rate": 1.2008221975653339e-05,
      "loss": 1.3763,
      "step": 8460
    },
    {
      "epoch": 1.359143063467865,
      "grad_norm": 0.5381444096565247,
      "learning_rate": 1.199124571144633e-05,
      "loss": 1.425,
      "step": 8470
    },
    {
      "epoch": 1.360747813528043,
      "grad_norm": 0.42470335960388184,
      "learning_rate": 1.1974263469553718e-05,
      "loss": 1.4415,
      "step": 8480
    },
    {
      "epoch": 1.3623525635882212,
      "grad_norm": 0.645168125629425,
      "learning_rate": 1.1957275300955903e-05,
      "loss": 1.274,
      "step": 8490
    },
    {
      "epoch": 1.3639573136483993,
      "grad_norm": 0.4900779128074646,
      "learning_rate": 1.194028125665108e-05,
      "loss": 1.3773,
      "step": 8500
    },
    {
      "epoch": 1.3655620637085775,
      "grad_norm": 0.4401026666164398,
      "learning_rate": 1.1923281387655077e-05,
      "loss": 1.33,
      "step": 8510
    },
    {
      "epoch": 1.3671668137687556,
      "grad_norm": 0.4375276565551758,
      "learning_rate": 1.1906275745001212e-05,
      "loss": 1.2678,
      "step": 8520
    },
    {
      "epoch": 1.3687715638289337,
      "grad_norm": 0.38777750730514526,
      "learning_rate": 1.188926437974013e-05,
      "loss": 1.2551,
      "step": 8530
    },
    {
      "epoch": 1.3703763138891119,
      "grad_norm": 0.6465415954589844,
      "learning_rate": 1.1872247342939667e-05,
      "loss": 1.3257,
      "step": 8540
    },
    {
      "epoch": 1.37198106394929,
      "grad_norm": 0.5299310684204102,
      "learning_rate": 1.1855224685684667e-05,
      "loss": 1.2168,
      "step": 8550
    },
    {
      "epoch": 1.373585814009468,
      "grad_norm": 0.4115622341632843,
      "learning_rate": 1.1838196459076864e-05,
      "loss": 1.3731,
      "step": 8560
    },
    {
      "epoch": 1.375190564069646,
      "grad_norm": 0.4625302851200104,
      "learning_rate": 1.1821162714234697e-05,
      "loss": 1.3381,
      "step": 8570
    },
    {
      "epoch": 1.3767953141298244,
      "grad_norm": 0.5291891098022461,
      "learning_rate": 1.1804123502293181e-05,
      "loss": 1.3669,
      "step": 8580
    },
    {
      "epoch": 1.3784000641900023,
      "grad_norm": 0.6452414393424988,
      "learning_rate": 1.1787078874403737e-05,
      "loss": 1.3412,
      "step": 8590
    },
    {
      "epoch": 1.3800048142501806,
      "grad_norm": 0.36883407831192017,
      "learning_rate": 1.1770028881734048e-05,
      "loss": 1.4255,
      "step": 8600
    },
    {
      "epoch": 1.3816095643103585,
      "grad_norm": 0.5101768374443054,
      "learning_rate": 1.1752973575467901e-05,
      "loss": 1.3352,
      "step": 8610
    },
    {
      "epoch": 1.3832143143705369,
      "grad_norm": 0.39968395233154297,
      "learning_rate": 1.1735913006805033e-05,
      "loss": 1.2118,
      "step": 8620
    },
    {
      "epoch": 1.3848190644307148,
      "grad_norm": 0.3971270024776459,
      "learning_rate": 1.171884722696098e-05,
      "loss": 1.3322,
      "step": 8630
    },
    {
      "epoch": 1.3864238144908931,
      "grad_norm": 0.5070668458938599,
      "learning_rate": 1.170177628716692e-05,
      "loss": 1.3286,
      "step": 8640
    },
    {
      "epoch": 1.388028564551071,
      "grad_norm": 0.40874800086021423,
      "learning_rate": 1.1684700238669526e-05,
      "loss": 1.3987,
      "step": 8650
    },
    {
      "epoch": 1.3896333146112494,
      "grad_norm": 0.4954782724380493,
      "learning_rate": 1.1667619132730802e-05,
      "loss": 1.2074,
      "step": 8660
    },
    {
      "epoch": 1.3912380646714273,
      "grad_norm": 0.36395907402038574,
      "learning_rate": 1.1650533020627937e-05,
      "loss": 1.3012,
      "step": 8670
    },
    {
      "epoch": 1.3928428147316056,
      "grad_norm": 0.44027063250541687,
      "learning_rate": 1.1633441953653143e-05,
      "loss": 1.348,
      "step": 8680
    },
    {
      "epoch": 1.3944475647917836,
      "grad_norm": 0.3093055188655853,
      "learning_rate": 1.1616345983113516e-05,
      "loss": 1.3614,
      "step": 8690
    },
    {
      "epoch": 1.396052314851962,
      "grad_norm": 0.5862752199172974,
      "learning_rate": 1.1599245160330866e-05,
      "loss": 1.2803,
      "step": 8700
    },
    {
      "epoch": 1.3976570649121398,
      "grad_norm": 0.40818434953689575,
      "learning_rate": 1.158213953664157e-05,
      "loss": 1.2492,
      "step": 8710
    },
    {
      "epoch": 1.3992618149723182,
      "grad_norm": 0.5676238536834717,
      "learning_rate": 1.156502916339642e-05,
      "loss": 1.2,
      "step": 8720
    },
    {
      "epoch": 1.400866565032496,
      "grad_norm": 0.4251767694950104,
      "learning_rate": 1.1547914091960463e-05,
      "loss": 1.341,
      "step": 8730
    },
    {
      "epoch": 1.4024713150926744,
      "grad_norm": 0.3724423050880432,
      "learning_rate": 1.153079437371285e-05,
      "loss": 1.3924,
      "step": 8740
    },
    {
      "epoch": 1.4040760651528523,
      "grad_norm": 0.5566845536231995,
      "learning_rate": 1.1513670060046685e-05,
      "loss": 1.3988,
      "step": 8750
    },
    {
      "epoch": 1.4056808152130307,
      "grad_norm": 0.6623894572257996,
      "learning_rate": 1.1496541202368863e-05,
      "loss": 1.2676,
      "step": 8760
    },
    {
      "epoch": 1.4072855652732086,
      "grad_norm": 0.5004273056983948,
      "learning_rate": 1.1479407852099922e-05,
      "loss": 1.2287,
      "step": 8770
    },
    {
      "epoch": 1.408890315333387,
      "grad_norm": 0.4145958125591278,
      "learning_rate": 1.1462270060673888e-05,
      "loss": 1.2321,
      "step": 8780
    },
    {
      "epoch": 1.4104950653935648,
      "grad_norm": 0.397914856672287,
      "learning_rate": 1.1445127879538117e-05,
      "loss": 1.2859,
      "step": 8790
    },
    {
      "epoch": 1.4120998154537432,
      "grad_norm": 0.5796698927879333,
      "learning_rate": 1.1427981360153143e-05,
      "loss": 1.5296,
      "step": 8800
    },
    {
      "epoch": 1.413704565513921,
      "grad_norm": 0.2995744049549103,
      "learning_rate": 1.1410830553992527e-05,
      "loss": 1.2787,
      "step": 8810
    },
    {
      "epoch": 1.4153093155740994,
      "grad_norm": 0.43969741463661194,
      "learning_rate": 1.1393675512542695e-05,
      "loss": 1.1841,
      "step": 8820
    },
    {
      "epoch": 1.4169140656342774,
      "grad_norm": 0.6251104474067688,
      "learning_rate": 1.1376516287302789e-05,
      "loss": 1.4875,
      "step": 8830
    },
    {
      "epoch": 1.4185188156944557,
      "grad_norm": 0.6077860593795776,
      "learning_rate": 1.1359352929784506e-05,
      "loss": 1.3382,
      "step": 8840
    },
    {
      "epoch": 1.4201235657546336,
      "grad_norm": 0.6382263898849487,
      "learning_rate": 1.1342185491511957e-05,
      "loss": 1.4388,
      "step": 8850
    },
    {
      "epoch": 1.421728315814812,
      "grad_norm": 0.5587853789329529,
      "learning_rate": 1.1325014024021494e-05,
      "loss": 1.2698,
      "step": 8860
    },
    {
      "epoch": 1.4233330658749899,
      "grad_norm": 0.44043320417404175,
      "learning_rate": 1.1307838578861573e-05,
      "loss": 1.259,
      "step": 8870
    },
    {
      "epoch": 1.4249378159351682,
      "grad_norm": 0.4108255207538605,
      "learning_rate": 1.1290659207592588e-05,
      "loss": 1.4734,
      "step": 8880
    },
    {
      "epoch": 1.4265425659953461,
      "grad_norm": 0.38001808524131775,
      "learning_rate": 1.127347596178671e-05,
      "loss": 1.2766,
      "step": 8890
    },
    {
      "epoch": 1.4281473160555245,
      "grad_norm": 0.7457751035690308,
      "learning_rate": 1.1256288893027758e-05,
      "loss": 1.1252,
      "step": 8900
    },
    {
      "epoch": 1.4297520661157024,
      "grad_norm": 0.49614080786705017,
      "learning_rate": 1.1239098052911013e-05,
      "loss": 1.4077,
      "step": 8910
    },
    {
      "epoch": 1.4313568161758807,
      "grad_norm": 0.32776719331741333,
      "learning_rate": 1.1221903493043088e-05,
      "loss": 1.2703,
      "step": 8920
    },
    {
      "epoch": 1.4329615662360586,
      "grad_norm": 0.5177673697471619,
      "learning_rate": 1.120470526504175e-05,
      "loss": 1.2345,
      "step": 8930
    },
    {
      "epoch": 1.434566316296237,
      "grad_norm": 0.6331548690795898,
      "learning_rate": 1.1187503420535798e-05,
      "loss": 1.4242,
      "step": 8940
    },
    {
      "epoch": 1.436171066356415,
      "grad_norm": 0.4373107850551605,
      "learning_rate": 1.1170298011164863e-05,
      "loss": 1.1219,
      "step": 8950
    },
    {
      "epoch": 1.4377758164165932,
      "grad_norm": 0.5678044557571411,
      "learning_rate": 1.11530890885793e-05,
      "loss": 1.2895,
      "step": 8960
    },
    {
      "epoch": 1.4393805664767712,
      "grad_norm": 0.4400615692138672,
      "learning_rate": 1.1135876704439994e-05,
      "loss": 1.3671,
      "step": 8970
    },
    {
      "epoch": 1.4409853165369495,
      "grad_norm": 0.6141665577888489,
      "learning_rate": 1.1118660910418228e-05,
      "loss": 1.2818,
      "step": 8980
    },
    {
      "epoch": 1.4425900665971274,
      "grad_norm": 0.6309098601341248,
      "learning_rate": 1.1101441758195524e-05,
      "loss": 1.2054,
      "step": 8990
    },
    {
      "epoch": 1.4441948166573058,
      "grad_norm": 0.5055993795394897,
      "learning_rate": 1.1084219299463484e-05,
      "loss": 1.4019,
      "step": 9000
    },
    {
      "epoch": 1.4457995667174837,
      "grad_norm": 0.5752190947532654,
      "learning_rate": 1.1066993585923628e-05,
      "loss": 1.1936,
      "step": 9010
    },
    {
      "epoch": 1.447404316777662,
      "grad_norm": 0.28613463044166565,
      "learning_rate": 1.1049764669287265e-05,
      "loss": 1.3169,
      "step": 9020
    },
    {
      "epoch": 1.44900906683784,
      "grad_norm": 0.2837562561035156,
      "learning_rate": 1.1032532601275298e-05,
      "loss": 1.3299,
      "step": 9030
    },
    {
      "epoch": 1.4506138168980183,
      "grad_norm": 0.5696951746940613,
      "learning_rate": 1.1015297433618107e-05,
      "loss": 1.4582,
      "step": 9040
    },
    {
      "epoch": 1.4522185669581962,
      "grad_norm": 0.38746750354766846,
      "learning_rate": 1.0998059218055366e-05,
      "loss": 1.2116,
      "step": 9050
    },
    {
      "epoch": 1.4538233170183743,
      "grad_norm": 0.42330387234687805,
      "learning_rate": 1.0980818006335907e-05,
      "loss": 1.1413,
      "step": 9060
    },
    {
      "epoch": 1.4554280670785524,
      "grad_norm": 0.7380138635635376,
      "learning_rate": 1.0963573850217553e-05,
      "loss": 1.4244,
      "step": 9070
    },
    {
      "epoch": 1.4570328171387306,
      "grad_norm": 0.2893231511116028,
      "learning_rate": 1.0946326801466964e-05,
      "loss": 1.5662,
      "step": 9080
    },
    {
      "epoch": 1.4586375671989087,
      "grad_norm": 0.4452526867389679,
      "learning_rate": 1.0929076911859487e-05,
      "loss": 1.3258,
      "step": 9090
    },
    {
      "epoch": 1.4602423172590868,
      "grad_norm": 0.47603633999824524,
      "learning_rate": 1.0911824233178995e-05,
      "loss": 1.2894,
      "step": 9100
    },
    {
      "epoch": 1.461847067319265,
      "grad_norm": 0.45050227642059326,
      "learning_rate": 1.0894568817217736e-05,
      "loss": 1.1209,
      "step": 9110
    },
    {
      "epoch": 1.463451817379443,
      "grad_norm": 0.6243150234222412,
      "learning_rate": 1.0877310715776175e-05,
      "loss": 1.2994,
      "step": 9120
    },
    {
      "epoch": 1.4650565674396212,
      "grad_norm": 0.28925564885139465,
      "learning_rate": 1.0860049980662832e-05,
      "loss": 1.3688,
      "step": 9130
    },
    {
      "epoch": 1.4666613174997993,
      "grad_norm": 0.47817665338516235,
      "learning_rate": 1.0842786663694145e-05,
      "loss": 1.3101,
      "step": 9140
    },
    {
      "epoch": 1.4682660675599775,
      "grad_norm": 0.43149423599243164,
      "learning_rate": 1.0825520816694295e-05,
      "loss": 1.2572,
      "step": 9150
    },
    {
      "epoch": 1.4698708176201556,
      "grad_norm": 0.4288705587387085,
      "learning_rate": 1.0808252491495057e-05,
      "loss": 1.1729,
      "step": 9160
    },
    {
      "epoch": 1.4714755676803337,
      "grad_norm": 0.4268832802772522,
      "learning_rate": 1.0790981739935652e-05,
      "loss": 1.2318,
      "step": 9170
    },
    {
      "epoch": 1.4730803177405118,
      "grad_norm": 0.6261513829231262,
      "learning_rate": 1.0773708613862577e-05,
      "loss": 1.4976,
      "step": 9180
    },
    {
      "epoch": 1.47468506780069,
      "grad_norm": 0.4662007689476013,
      "learning_rate": 1.0756433165129462e-05,
      "loss": 1.3217,
      "step": 9190
    },
    {
      "epoch": 1.476289817860868,
      "grad_norm": 0.5503376722335815,
      "learning_rate": 1.0739155445596914e-05,
      "loss": 1.3551,
      "step": 9200
    },
    {
      "epoch": 1.4778945679210462,
      "grad_norm": 0.6103018522262573,
      "learning_rate": 1.0721875507132347e-05,
      "loss": 1.3378,
      "step": 9210
    },
    {
      "epoch": 1.4794993179812244,
      "grad_norm": 0.4730494022369385,
      "learning_rate": 1.0704593401609842e-05,
      "loss": 1.3869,
      "step": 9220
    },
    {
      "epoch": 1.4811040680414025,
      "grad_norm": 0.45912331342697144,
      "learning_rate": 1.0687309180909984e-05,
      "loss": 1.3005,
      "step": 9230
    },
    {
      "epoch": 1.4827088181015806,
      "grad_norm": 0.7722640037536621,
      "learning_rate": 1.0670022896919711e-05,
      "loss": 1.2601,
      "step": 9240
    },
    {
      "epoch": 1.4843135681617587,
      "grad_norm": 0.4240722954273224,
      "learning_rate": 1.0652734601532149e-05,
      "loss": 1.2971,
      "step": 9250
    },
    {
      "epoch": 1.4859183182219369,
      "grad_norm": 0.34611833095550537,
      "learning_rate": 1.0635444346646467e-05,
      "loss": 1.2676,
      "step": 9260
    },
    {
      "epoch": 1.487523068282115,
      "grad_norm": 0.4019455909729004,
      "learning_rate": 1.0618152184167712e-05,
      "loss": 1.5207,
      "step": 9270
    },
    {
      "epoch": 1.4891278183422931,
      "grad_norm": 0.4068852961063385,
      "learning_rate": 1.0600858166006664e-05,
      "loss": 1.2538,
      "step": 9280
    },
    {
      "epoch": 1.4907325684024713,
      "grad_norm": 0.6425724625587463,
      "learning_rate": 1.0583562344079668e-05,
      "loss": 1.3007,
      "step": 9290
    },
    {
      "epoch": 1.4923373184626494,
      "grad_norm": 0.48550939559936523,
      "learning_rate": 1.0566264770308485e-05,
      "loss": 1.4296,
      "step": 9300
    },
    {
      "epoch": 1.4939420685228275,
      "grad_norm": 0.4583524167537689,
      "learning_rate": 1.0548965496620138e-05,
      "loss": 1.3346,
      "step": 9310
    },
    {
      "epoch": 1.4955468185830056,
      "grad_norm": 0.5861687064170837,
      "learning_rate": 1.0531664574946752e-05,
      "loss": 1.4121,
      "step": 9320
    },
    {
      "epoch": 1.4971515686431838,
      "grad_norm": 0.5131075382232666,
      "learning_rate": 1.0514362057225395e-05,
      "loss": 1.2911,
      "step": 9330
    },
    {
      "epoch": 1.498756318703362,
      "grad_norm": 0.6177898049354553,
      "learning_rate": 1.049705799539793e-05,
      "loss": 1.3211,
      "step": 9340
    },
    {
      "epoch": 1.50036106876354,
      "grad_norm": 0.4633405804634094,
      "learning_rate": 1.0479752441410857e-05,
      "loss": 1.2506,
      "step": 9350
    },
    {
      "epoch": 1.5019658188237182,
      "grad_norm": 0.5027555227279663,
      "learning_rate": 1.0462445447215154e-05,
      "loss": 1.384,
      "step": 9360
    },
    {
      "epoch": 1.5035705688838963,
      "grad_norm": 0.7161389589309692,
      "learning_rate": 1.044513706476612e-05,
      "loss": 1.4567,
      "step": 9370
    },
    {
      "epoch": 1.5051753189440744,
      "grad_norm": 0.5154787302017212,
      "learning_rate": 1.042782734602322e-05,
      "loss": 1.2602,
      "step": 9380
    },
    {
      "epoch": 1.5067800690042525,
      "grad_norm": 0.5886952877044678,
      "learning_rate": 1.041051634294994e-05,
      "loss": 1.263,
      "step": 9390
    },
    {
      "epoch": 1.5083848190644307,
      "grad_norm": 0.4104962646961212,
      "learning_rate": 1.0393204107513614e-05,
      "loss": 1.3834,
      "step": 9400
    },
    {
      "epoch": 1.5099895691246088,
      "grad_norm": 0.6174566149711609,
      "learning_rate": 1.0375890691685274e-05,
      "loss": 1.4073,
      "step": 9410
    },
    {
      "epoch": 1.511594319184787,
      "grad_norm": 0.4865262508392334,
      "learning_rate": 1.03585761474395e-05,
      "loss": 1.366,
      "step": 9420
    },
    {
      "epoch": 1.513199069244965,
      "grad_norm": 0.5779155492782593,
      "learning_rate": 1.034126052675426e-05,
      "loss": 1.1822,
      "step": 9430
    },
    {
      "epoch": 1.5148038193051432,
      "grad_norm": 0.4840981960296631,
      "learning_rate": 1.032394388161075e-05,
      "loss": 1.1629,
      "step": 9440
    },
    {
      "epoch": 1.5164085693653213,
      "grad_norm": 0.36747467517852783,
      "learning_rate": 1.0306626263993242e-05,
      "loss": 1.3736,
      "step": 9450
    },
    {
      "epoch": 1.5180133194254994,
      "grad_norm": 0.43062517046928406,
      "learning_rate": 1.0289307725888929e-05,
      "loss": 1.3422,
      "step": 9460
    },
    {
      "epoch": 1.5196180694856776,
      "grad_norm": 0.5593737363815308,
      "learning_rate": 1.0271988319287765e-05,
      "loss": 1.3549,
      "step": 9470
    },
    {
      "epoch": 1.5212228195458557,
      "grad_norm": 0.5526537299156189,
      "learning_rate": 1.0254668096182313e-05,
      "loss": 1.3362,
      "step": 9480
    },
    {
      "epoch": 1.5228275696060338,
      "grad_norm": 0.5397584438323975,
      "learning_rate": 1.023734710856759e-05,
      "loss": 1.2868,
      "step": 9490
    },
    {
      "epoch": 1.524432319666212,
      "grad_norm": 0.5956773161888123,
      "learning_rate": 1.02200254084409e-05,
      "loss": 1.2728,
      "step": 9500
    },
    {
      "epoch": 1.52603706972639,
      "grad_norm": 0.49894410371780396,
      "learning_rate": 1.0202703047801696e-05,
      "loss": 1.2865,
      "step": 9510
    },
    {
      "epoch": 1.5276418197865682,
      "grad_norm": 0.6718289256095886,
      "learning_rate": 1.0185380078651408e-05,
      "loss": 1.3946,
      "step": 9520
    },
    {
      "epoch": 1.5292465698467463,
      "grad_norm": 0.5137860774993896,
      "learning_rate": 1.0168056552993288e-05,
      "loss": 1.3255,
      "step": 9530
    },
    {
      "epoch": 1.5308513199069245,
      "grad_norm": 0.5104056596755981,
      "learning_rate": 1.0150732522832268e-05,
      "loss": 1.2502,
      "step": 9540
    },
    {
      "epoch": 1.5324560699671026,
      "grad_norm": 0.3354523777961731,
      "learning_rate": 1.013340804017479e-05,
      "loss": 1.1625,
      "step": 9550
    },
    {
      "epoch": 1.5340608200272807,
      "grad_norm": 0.5526414513587952,
      "learning_rate": 1.0116083157028656e-05,
      "loss": 1.3795,
      "step": 9560
    },
    {
      "epoch": 1.5356655700874589,
      "grad_norm": 0.6103456616401672,
      "learning_rate": 1.0098757925402866e-05,
      "loss": 1.2743,
      "step": 9570
    },
    {
      "epoch": 1.537270320147637,
      "grad_norm": 0.5660024285316467,
      "learning_rate": 1.0081432397307474e-05,
      "loss": 1.305,
      "step": 9580
    },
    {
      "epoch": 1.538875070207815,
      "grad_norm": 0.4005955457687378,
      "learning_rate": 1.0064106624753417e-05,
      "loss": 1.2612,
      "step": 9590
    },
    {
      "epoch": 1.5404798202679932,
      "grad_norm": 0.37001246213912964,
      "learning_rate": 1.0046780659752364e-05,
      "loss": 1.2803,
      "step": 9600
    },
    {
      "epoch": 1.5420845703281714,
      "grad_norm": 0.6065647602081299,
      "learning_rate": 1.0029454554316574e-05,
      "loss": 1.2171,
      "step": 9610
    },
    {
      "epoch": 1.5436893203883495,
      "grad_norm": 0.34911027550697327,
      "learning_rate": 1.0012128360458713e-05,
      "loss": 1.1811,
      "step": 9620
    },
    {
      "epoch": 1.5452940704485276,
      "grad_norm": 0.5313422083854675,
      "learning_rate": 9.994802130191719e-06,
      "loss": 1.2787,
      "step": 9630
    },
    {
      "epoch": 1.5468988205087058,
      "grad_norm": 0.6233901381492615,
      "learning_rate": 9.977475915528647e-06,
      "loss": 1.433,
      "step": 9640
    },
    {
      "epoch": 1.5485035705688839,
      "grad_norm": 0.6291518211364746,
      "learning_rate": 9.960149768482485e-06,
      "loss": 1.3126,
      "step": 9650
    },
    {
      "epoch": 1.550108320629062,
      "grad_norm": 0.5162368416786194,
      "learning_rate": 9.942823741066043e-06,
      "loss": 1.2563,
      "step": 9660
    },
    {
      "epoch": 1.5517130706892401,
      "grad_norm": 0.42715251445770264,
      "learning_rate": 9.925497885291752e-06,
      "loss": 1.4011,
      "step": 9670
    },
    {
      "epoch": 1.5533178207494183,
      "grad_norm": 0.5093327164649963,
      "learning_rate": 9.908172253171534e-06,
      "loss": 1.3438,
      "step": 9680
    },
    {
      "epoch": 1.5549225708095964,
      "grad_norm": 0.31581199169158936,
      "learning_rate": 9.890846896716647e-06,
      "loss": 1.3662,
      "step": 9690
    },
    {
      "epoch": 1.5565273208697745,
      "grad_norm": 0.5133839845657349,
      "learning_rate": 9.873521867937506e-06,
      "loss": 1.3054,
      "step": 9700
    },
    {
      "epoch": 1.5581320709299527,
      "grad_norm": 0.43646156787872314,
      "learning_rate": 9.856197218843561e-06,
      "loss": 1.2362,
      "step": 9710
    },
    {
      "epoch": 1.5597368209901308,
      "grad_norm": 0.5532973408699036,
      "learning_rate": 9.838873001443103e-06,
      "loss": 1.3932,
      "step": 9720
    },
    {
      "epoch": 1.561341571050309,
      "grad_norm": 0.6536970138549805,
      "learning_rate": 9.821549267743145e-06,
      "loss": 1.2072,
      "step": 9730
    },
    {
      "epoch": 1.562946321110487,
      "grad_norm": 0.464897096157074,
      "learning_rate": 9.804226069749234e-06,
      "loss": 1.5062,
      "step": 9740
    },
    {
      "epoch": 1.5645510711706652,
      "grad_norm": 0.4531822204589844,
      "learning_rate": 9.78690345946532e-06,
      "loss": 1.3122,
      "step": 9750
    },
    {
      "epoch": 1.5661558212308433,
      "grad_norm": 0.5209041237831116,
      "learning_rate": 9.769581488893575e-06,
      "loss": 1.3426,
      "step": 9760
    },
    {
      "epoch": 1.5677605712910214,
      "grad_norm": 0.7090029716491699,
      "learning_rate": 9.752260210034266e-06,
      "loss": 1.3979,
      "step": 9770
    },
    {
      "epoch": 1.5693653213511995,
      "grad_norm": 0.5189130902290344,
      "learning_rate": 9.73493967488557e-06,
      "loss": 1.1663,
      "step": 9780
    },
    {
      "epoch": 1.5709700714113777,
      "grad_norm": 0.5736579298973083,
      "learning_rate": 9.717619935443448e-06,
      "loss": 1.3169,
      "step": 9790
    },
    {
      "epoch": 1.5725748214715558,
      "grad_norm": 0.550003170967102,
      "learning_rate": 9.700301043701447e-06,
      "loss": 1.4483,
      "step": 9800
    },
    {
      "epoch": 1.574179571531734,
      "grad_norm": 0.38397595286369324,
      "learning_rate": 9.682983051650597e-06,
      "loss": 1.149,
      "step": 9810
    },
    {
      "epoch": 1.575784321591912,
      "grad_norm": 0.4317736327648163,
      "learning_rate": 9.665666011279206e-06,
      "loss": 1.4116,
      "step": 9820
    },
    {
      "epoch": 1.5773890716520902,
      "grad_norm": 0.8652298450469971,
      "learning_rate": 9.648349974572739e-06,
      "loss": 1.3851,
      "step": 9830
    },
    {
      "epoch": 1.5789938217122683,
      "grad_norm": 0.6816331744194031,
      "learning_rate": 9.631034993513637e-06,
      "loss": 1.1529,
      "step": 9840
    },
    {
      "epoch": 1.5805985717724464,
      "grad_norm": 0.4067188501358032,
      "learning_rate": 9.613721120081184e-06,
      "loss": 1.107,
      "step": 9850
    },
    {
      "epoch": 1.5822033218326246,
      "grad_norm": 0.6432110071182251,
      "learning_rate": 9.596408406251323e-06,
      "loss": 1.2874,
      "step": 9860
    },
    {
      "epoch": 1.5838080718928027,
      "grad_norm": 0.5622227787971497,
      "learning_rate": 9.579096903996535e-06,
      "loss": 1.249,
      "step": 9870
    },
    {
      "epoch": 1.5854128219529808,
      "grad_norm": 0.36109793186187744,
      "learning_rate": 9.561786665285646e-06,
      "loss": 1.1388,
      "step": 9880
    },
    {
      "epoch": 1.587017572013159,
      "grad_norm": 0.7643912434577942,
      "learning_rate": 9.544477742083704e-06,
      "loss": 1.3524,
      "step": 9890
    },
    {
      "epoch": 1.588622322073337,
      "grad_norm": 0.4878406524658203,
      "learning_rate": 9.527170186351792e-06,
      "loss": 1.2164,
      "step": 9900
    },
    {
      "epoch": 1.5902270721335152,
      "grad_norm": 0.8255204558372498,
      "learning_rate": 9.509864050046904e-06,
      "loss": 1.5354,
      "step": 9910
    },
    {
      "epoch": 1.5918318221936933,
      "grad_norm": 0.7140220403671265,
      "learning_rate": 9.492559385121762e-06,
      "loss": 1.4168,
      "step": 9920
    },
    {
      "epoch": 1.5934365722538715,
      "grad_norm": 0.43024447560310364,
      "learning_rate": 9.475256243524678e-06,
      "loss": 1.0933,
      "step": 9930
    },
    {
      "epoch": 1.5950413223140496,
      "grad_norm": 0.5931713581085205,
      "learning_rate": 9.457954677199383e-06,
      "loss": 1.4246,
      "step": 9940
    },
    {
      "epoch": 1.5966460723742277,
      "grad_norm": 0.6254360675811768,
      "learning_rate": 9.440654738084885e-06,
      "loss": 1.3693,
      "step": 9950
    },
    {
      "epoch": 1.5982508224344059,
      "grad_norm": 0.7071413397789001,
      "learning_rate": 9.423356478115304e-06,
      "loss": 1.4853,
      "step": 9960
    },
    {
      "epoch": 1.599855572494584,
      "grad_norm": 0.6861327290534973,
      "learning_rate": 9.406059949219724e-06,
      "loss": 1.3482,
      "step": 9970
    },
    {
      "epoch": 1.6014603225547621,
      "grad_norm": 0.4743536412715912,
      "learning_rate": 9.388765203322029e-06,
      "loss": 1.1261,
      "step": 9980
    },
    {
      "epoch": 1.6030650726149402,
      "grad_norm": 0.5346304178237915,
      "learning_rate": 9.371472292340748e-06,
      "loss": 1.4086,
      "step": 9990
    },
    {
      "epoch": 1.6046698226751184,
      "grad_norm": 0.5557130575180054,
      "learning_rate": 9.354181268188907e-06,
      "loss": 1.2709,
      "step": 10000
    },
    {
      "epoch": 1.6062745727352965,
      "grad_norm": 0.611912190914154,
      "learning_rate": 9.336892182773864e-06,
      "loss": 1.2891,
      "step": 10010
    },
    {
      "epoch": 1.6078793227954746,
      "grad_norm": 0.5284813642501831,
      "learning_rate": 9.319605087997157e-06,
      "loss": 1.354,
      "step": 10020
    },
    {
      "epoch": 1.6094840728556528,
      "grad_norm": 0.30901047587394714,
      "learning_rate": 9.30232003575435e-06,
      "loss": 1.37,
      "step": 10030
    },
    {
      "epoch": 1.6110888229158309,
      "grad_norm": 0.4922613799571991,
      "learning_rate": 9.285037077934874e-06,
      "loss": 1.2974,
      "step": 10040
    },
    {
      "epoch": 1.612693572976009,
      "grad_norm": 0.4004596471786499,
      "learning_rate": 9.267756266421875e-06,
      "loss": 1.2456,
      "step": 10050
    },
    {
      "epoch": 1.6142983230361871,
      "grad_norm": 0.4462622404098511,
      "learning_rate": 9.250477653092051e-06,
      "loss": 1.38,
      "step": 10060
    },
    {
      "epoch": 1.6159030730963653,
      "grad_norm": 0.4766418933868408,
      "learning_rate": 9.233201289815505e-06,
      "loss": 1.3339,
      "step": 10070
    },
    {
      "epoch": 1.6175078231565434,
      "grad_norm": 0.5451598763465881,
      "learning_rate": 9.215927228455587e-06,
      "loss": 1.2365,
      "step": 10080
    },
    {
      "epoch": 1.6191125732167215,
      "grad_norm": 0.44418027997016907,
      "learning_rate": 9.19865552086873e-06,
      "loss": 1.2417,
      "step": 10090
    },
    {
      "epoch": 1.6207173232768997,
      "grad_norm": 0.40806153416633606,
      "learning_rate": 9.181386218904308e-06,
      "loss": 1.1334,
      "step": 10100
    },
    {
      "epoch": 1.6223220733370778,
      "grad_norm": 0.5471625328063965,
      "learning_rate": 9.16411937440447e-06,
      "loss": 1.1424,
      "step": 10110
    },
    {
      "epoch": 1.623926823397256,
      "grad_norm": 0.5932746529579163,
      "learning_rate": 9.146855039203986e-06,
      "loss": 1.284,
      "step": 10120
    },
    {
      "epoch": 1.625531573457434,
      "grad_norm": 0.47057968378067017,
      "learning_rate": 9.129593265130097e-06,
      "loss": 1.3882,
      "step": 10130
    },
    {
      "epoch": 1.6271363235176122,
      "grad_norm": 0.3208356201648712,
      "learning_rate": 9.112334104002353e-06,
      "loss": 1.2611,
      "step": 10140
    },
    {
      "epoch": 1.6287410735777903,
      "grad_norm": 0.5072731971740723,
      "learning_rate": 9.095077607632462e-06,
      "loss": 1.2939,
      "step": 10150
    },
    {
      "epoch": 1.6303458236379684,
      "grad_norm": 0.4697054922580719,
      "learning_rate": 9.07782382782413e-06,
      "loss": 1.281,
      "step": 10160
    },
    {
      "epoch": 1.6319505736981466,
      "grad_norm": 0.6101133823394775,
      "learning_rate": 9.06057281637291e-06,
      "loss": 1.3214,
      "step": 10170
    },
    {
      "epoch": 1.6335553237583247,
      "grad_norm": 0.9278947114944458,
      "learning_rate": 9.043324625066045e-06,
      "loss": 1.4435,
      "step": 10180
    },
    {
      "epoch": 1.6351600738185028,
      "grad_norm": 0.7842887043952942,
      "learning_rate": 9.026079305682304e-06,
      "loss": 1.2535,
      "step": 10190
    },
    {
      "epoch": 1.636764823878681,
      "grad_norm": 0.8392810225486755,
      "learning_rate": 9.008836909991852e-06,
      "loss": 1.3297,
      "step": 10200
    },
    {
      "epoch": 1.638369573938859,
      "grad_norm": 0.6077150106430054,
      "learning_rate": 8.991597489756057e-06,
      "loss": 1.3909,
      "step": 10210
    },
    {
      "epoch": 1.6399743239990372,
      "grad_norm": 0.6001397967338562,
      "learning_rate": 8.97436109672737e-06,
      "loss": 1.3027,
      "step": 10220
    },
    {
      "epoch": 1.6415790740592153,
      "grad_norm": 0.5625351071357727,
      "learning_rate": 8.957127782649146e-06,
      "loss": 1.4741,
      "step": 10230
    },
    {
      "epoch": 1.6431838241193935,
      "grad_norm": 0.3739023804664612,
      "learning_rate": 8.939897599255496e-06,
      "loss": 1.1699,
      "step": 10240
    },
    {
      "epoch": 1.6447885741795716,
      "grad_norm": 0.4957106411457062,
      "learning_rate": 8.92267059827114e-06,
      "loss": 1.3978,
      "step": 10250
    },
    {
      "epoch": 1.6463933242397497,
      "grad_norm": 0.45849132537841797,
      "learning_rate": 8.90544683141124e-06,
      "loss": 1.293,
      "step": 10260
    },
    {
      "epoch": 1.6479980742999278,
      "grad_norm": 0.3547096848487854,
      "learning_rate": 8.88822635038125e-06,
      "loss": 1.2681,
      "step": 10270
    },
    {
      "epoch": 1.649602824360106,
      "grad_norm": 0.41155973076820374,
      "learning_rate": 8.871009206876757e-06,
      "loss": 1.2996,
      "step": 10280
    },
    {
      "epoch": 1.651207574420284,
      "grad_norm": 0.41697436571121216,
      "learning_rate": 8.853795452583339e-06,
      "loss": 1.1865,
      "step": 10290
    },
    {
      "epoch": 1.652812324480462,
      "grad_norm": 0.43161270022392273,
      "learning_rate": 8.836585139176382e-06,
      "loss": 1.2051,
      "step": 10300
    },
    {
      "epoch": 1.6544170745406404,
      "grad_norm": 0.5570599436759949,
      "learning_rate": 8.819378318320962e-06,
      "loss": 1.4229,
      "step": 10310
    },
    {
      "epoch": 1.6560218246008183,
      "grad_norm": 0.40513068437576294,
      "learning_rate": 8.802175041671655e-06,
      "loss": 1.1828,
      "step": 10320
    },
    {
      "epoch": 1.6576265746609966,
      "grad_norm": 0.589361846446991,
      "learning_rate": 8.784975360872407e-06,
      "loss": 1.3967,
      "step": 10330
    },
    {
      "epoch": 1.6592313247211745,
      "grad_norm": 0.4880925416946411,
      "learning_rate": 8.767779327556362e-06,
      "loss": 1.3754,
      "step": 10340
    },
    {
      "epoch": 1.6608360747813529,
      "grad_norm": 0.6069830656051636,
      "learning_rate": 8.750586993345726e-06,
      "loss": 1.3158,
      "step": 10350
    },
    {
      "epoch": 1.6624408248415308,
      "grad_norm": 0.5541645884513855,
      "learning_rate": 8.733398409851582e-06,
      "loss": 1.3009,
      "step": 10360
    },
    {
      "epoch": 1.6640455749017091,
      "grad_norm": 0.47428232431411743,
      "learning_rate": 8.716213628673776e-06,
      "loss": 1.2403,
      "step": 10370
    },
    {
      "epoch": 1.665650324961887,
      "grad_norm": 0.35994043946266174,
      "learning_rate": 8.699032701400714e-06,
      "loss": 1.3217,
      "step": 10380
    },
    {
      "epoch": 1.6672550750220654,
      "grad_norm": 0.6396415829658508,
      "learning_rate": 8.681855679609256e-06,
      "loss": 1.2925,
      "step": 10390
    },
    {
      "epoch": 1.6688598250822433,
      "grad_norm": 0.3323845863342285,
      "learning_rate": 8.664682614864524e-06,
      "loss": 1.3322,
      "step": 10400
    },
    {
      "epoch": 1.6704645751424216,
      "grad_norm": 0.37678366899490356,
      "learning_rate": 8.647513558719768e-06,
      "loss": 1.2973,
      "step": 10410
    },
    {
      "epoch": 1.6720693252025995,
      "grad_norm": 0.6612173318862915,
      "learning_rate": 8.630348562716193e-06,
      "loss": 1.3613,
      "step": 10420
    },
    {
      "epoch": 1.673674075262778,
      "grad_norm": 0.4672214090824127,
      "learning_rate": 8.613187678382836e-06,
      "loss": 1.4009,
      "step": 10430
    },
    {
      "epoch": 1.6752788253229558,
      "grad_norm": 0.4266873002052307,
      "learning_rate": 8.59603095723637e-06,
      "loss": 1.4341,
      "step": 10440
    },
    {
      "epoch": 1.6768835753831342,
      "grad_norm": 0.4876948595046997,
      "learning_rate": 8.578878450780984e-06,
      "loss": 1.2497,
      "step": 10450
    },
    {
      "epoch": 1.678488325443312,
      "grad_norm": 0.34636998176574707,
      "learning_rate": 8.5617302105082e-06,
      "loss": 1.2475,
      "step": 10460
    },
    {
      "epoch": 1.6800930755034904,
      "grad_norm": 0.45646658539772034,
      "learning_rate": 8.544586287896754e-06,
      "loss": 1.3532,
      "step": 10470
    },
    {
      "epoch": 1.6816978255636683,
      "grad_norm": 0.5559931993484497,
      "learning_rate": 8.527446734412399e-06,
      "loss": 1.416,
      "step": 10480
    },
    {
      "epoch": 1.6833025756238467,
      "grad_norm": 0.6177324056625366,
      "learning_rate": 8.51031160150779e-06,
      "loss": 1.4708,
      "step": 10490
    },
    {
      "epoch": 1.6849073256840246,
      "grad_norm": 0.4872027337551117,
      "learning_rate": 8.4931809406223e-06,
      "loss": 1.3138,
      "step": 10500
    },
    {
      "epoch": 1.686512075744203,
      "grad_norm": 0.6699690222740173,
      "learning_rate": 8.476054803181873e-06,
      "loss": 1.5105,
      "step": 10510
    },
    {
      "epoch": 1.6881168258043808,
      "grad_norm": 0.4383772015571594,
      "learning_rate": 8.458933240598894e-06,
      "loss": 1.2915,
      "step": 10520
    },
    {
      "epoch": 1.6897215758645592,
      "grad_norm": 0.42415520548820496,
      "learning_rate": 8.441816304271988e-06,
      "loss": 1.3777,
      "step": 10530
    },
    {
      "epoch": 1.691326325924737,
      "grad_norm": 0.5191252827644348,
      "learning_rate": 8.424704045585919e-06,
      "loss": 1.2759,
      "step": 10540
    },
    {
      "epoch": 1.6929310759849154,
      "grad_norm": 0.41795119643211365,
      "learning_rate": 8.407596515911383e-06,
      "loss": 1.229,
      "step": 10550
    },
    {
      "epoch": 1.6945358260450933,
      "grad_norm": 0.616546094417572,
      "learning_rate": 8.3904937666049e-06,
      "loss": 1.3616,
      "step": 10560
    },
    {
      "epoch": 1.6961405761052717,
      "grad_norm": 0.4757571816444397,
      "learning_rate": 8.373395849008626e-06,
      "loss": 1.3373,
      "step": 10570
    },
    {
      "epoch": 1.6977453261654496,
      "grad_norm": 0.5635285377502441,
      "learning_rate": 8.356302814450223e-06,
      "loss": 1.3847,
      "step": 10580
    },
    {
      "epoch": 1.699350076225628,
      "grad_norm": 0.48056450486183167,
      "learning_rate": 8.33921471424268e-06,
      "loss": 1.3416,
      "step": 10590
    },
    {
      "epoch": 1.7009548262858059,
      "grad_norm": 0.5515750646591187,
      "learning_rate": 8.322131599684193e-06,
      "loss": 1.3848,
      "step": 10600
    },
    {
      "epoch": 1.7025595763459842,
      "grad_norm": 0.5110709071159363,
      "learning_rate": 8.30505352205797e-06,
      "loss": 1.3409,
      "step": 10610
    },
    {
      "epoch": 1.7041643264061621,
      "grad_norm": 0.5171849727630615,
      "learning_rate": 8.287980532632115e-06,
      "loss": 1.3358,
      "step": 10620
    },
    {
      "epoch": 1.7057690764663405,
      "grad_norm": 0.3165454864501953,
      "learning_rate": 8.270912682659446e-06,
      "loss": 1.3347,
      "step": 10630
    },
    {
      "epoch": 1.7073738265265184,
      "grad_norm": 0.42589443922042847,
      "learning_rate": 8.25385002337736e-06,
      "loss": 1.3777,
      "step": 10640
    },
    {
      "epoch": 1.7089785765866967,
      "grad_norm": 0.5552883744239807,
      "learning_rate": 8.23679260600767e-06,
      "loss": 1.2111,
      "step": 10650
    },
    {
      "epoch": 1.7105833266468746,
      "grad_norm": 0.5080781579017639,
      "learning_rate": 8.219740481756445e-06,
      "loss": 1.373,
      "step": 10660
    },
    {
      "epoch": 1.712188076707053,
      "grad_norm": 0.45718783140182495,
      "learning_rate": 8.202693701813876e-06,
      "loss": 1.1633,
      "step": 10670
    },
    {
      "epoch": 1.7137928267672309,
      "grad_norm": 0.502780556678772,
      "learning_rate": 8.185652317354105e-06,
      "loss": 1.2779,
      "step": 10680
    },
    {
      "epoch": 1.7153975768274092,
      "grad_norm": 0.4601869583129883,
      "learning_rate": 8.168616379535074e-06,
      "loss": 1.4262,
      "step": 10690
    },
    {
      "epoch": 1.7170023268875871,
      "grad_norm": 0.530651867389679,
      "learning_rate": 8.15158593949838e-06,
      "loss": 1.3103,
      "step": 10700
    },
    {
      "epoch": 1.7186070769477655,
      "grad_norm": 0.5618245005607605,
      "learning_rate": 8.134561048369112e-06,
      "loss": 1.2285,
      "step": 10710
    },
    {
      "epoch": 1.7202118270079434,
      "grad_norm": 0.5721569061279297,
      "learning_rate": 8.117541757255702e-06,
      "loss": 1.3778,
      "step": 10720
    },
    {
      "epoch": 1.7218165770681217,
      "grad_norm": 0.6814433336257935,
      "learning_rate": 8.100528117249771e-06,
      "loss": 1.3417,
      "step": 10730
    },
    {
      "epoch": 1.7234213271282997,
      "grad_norm": 0.6132250428199768,
      "learning_rate": 8.083520179425977e-06,
      "loss": 1.2133,
      "step": 10740
    },
    {
      "epoch": 1.725026077188478,
      "grad_norm": 0.5856893658638,
      "learning_rate": 8.066517994841858e-06,
      "loss": 1.1279,
      "step": 10750
    },
    {
      "epoch": 1.726630827248656,
      "grad_norm": 0.42318642139434814,
      "learning_rate": 8.049521614537681e-06,
      "loss": 1.3051,
      "step": 10760
    },
    {
      "epoch": 1.7282355773088343,
      "grad_norm": 0.5308985114097595,
      "learning_rate": 8.032531089536288e-06,
      "loss": 1.4259,
      "step": 10770
    },
    {
      "epoch": 1.7298403273690122,
      "grad_norm": 0.8518968224525452,
      "learning_rate": 8.01554647084295e-06,
      "loss": 1.3305,
      "step": 10780
    },
    {
      "epoch": 1.7314450774291905,
      "grad_norm": 0.49992072582244873,
      "learning_rate": 7.998567809445198e-06,
      "loss": 1.3028,
      "step": 10790
    },
    {
      "epoch": 1.7330498274893684,
      "grad_norm": 0.4143707752227783,
      "learning_rate": 7.981595156312678e-06,
      "loss": 1.3308,
      "step": 10800
    },
    {
      "epoch": 1.7346545775495468,
      "grad_norm": 0.5473443269729614,
      "learning_rate": 7.964628562397017e-06,
      "loss": 1.2825,
      "step": 10810
    },
    {
      "epoch": 1.7362593276097247,
      "grad_norm": 0.6666749119758606,
      "learning_rate": 7.947668078631628e-06,
      "loss": 1.271,
      "step": 10820
    },
    {
      "epoch": 1.737864077669903,
      "grad_norm": 0.4105549156665802,
      "learning_rate": 7.930713755931605e-06,
      "loss": 1.1689,
      "step": 10830
    },
    {
      "epoch": 1.739468827730081,
      "grad_norm": 0.40751364827156067,
      "learning_rate": 7.913765645193523e-06,
      "loss": 1.3475,
      "step": 10840
    },
    {
      "epoch": 1.7410735777902593,
      "grad_norm": 0.5400144457817078,
      "learning_rate": 7.896823797295332e-06,
      "loss": 1.4013,
      "step": 10850
    },
    {
      "epoch": 1.7426783278504372,
      "grad_norm": 0.5191487669944763,
      "learning_rate": 7.879888263096161e-06,
      "loss": 1.2742,
      "step": 10860
    },
    {
      "epoch": 1.7442830779106155,
      "grad_norm": 0.489709734916687,
      "learning_rate": 7.862959093436201e-06,
      "loss": 1.2378,
      "step": 10870
    },
    {
      "epoch": 1.7458878279707934,
      "grad_norm": 0.7321893572807312,
      "learning_rate": 7.846036339136524e-06,
      "loss": 1.3028,
      "step": 10880
    },
    {
      "epoch": 1.7474925780309718,
      "grad_norm": 0.49724358320236206,
      "learning_rate": 7.829120050998957e-06,
      "loss": 1.423,
      "step": 10890
    },
    {
      "epoch": 1.7490973280911497,
      "grad_norm": 0.5561866760253906,
      "learning_rate": 7.812210279805898e-06,
      "loss": 1.3103,
      "step": 10900
    },
    {
      "epoch": 1.750702078151328,
      "grad_norm": 0.4211002290248871,
      "learning_rate": 7.795307076320202e-06,
      "loss": 1.2827,
      "step": 10910
    },
    {
      "epoch": 1.752306828211506,
      "grad_norm": 0.6866394877433777,
      "learning_rate": 7.778410491284987e-06,
      "loss": 1.3347,
      "step": 10920
    },
    {
      "epoch": 1.7539115782716843,
      "grad_norm": 0.4786531925201416,
      "learning_rate": 7.76152057542352e-06,
      "loss": 1.3696,
      "step": 10930
    },
    {
      "epoch": 1.7555163283318622,
      "grad_norm": 0.8172760009765625,
      "learning_rate": 7.744637379439034e-06,
      "loss": 1.2188,
      "step": 10940
    },
    {
      "epoch": 1.7571210783920406,
      "grad_norm": 0.3819113075733185,
      "learning_rate": 7.7277609540146e-06,
      "loss": 1.2037,
      "step": 10950
    },
    {
      "epoch": 1.7587258284522185,
      "grad_norm": 0.35200268030166626,
      "learning_rate": 7.710891349812955e-06,
      "loss": 1.1671,
      "step": 10960
    },
    {
      "epoch": 1.7603305785123968,
      "grad_norm": 0.3769153952598572,
      "learning_rate": 7.694028617476367e-06,
      "loss": 1.2848,
      "step": 10970
    },
    {
      "epoch": 1.7619353285725747,
      "grad_norm": 0.6316832304000854,
      "learning_rate": 7.677172807626466e-06,
      "loss": 1.4366,
      "step": 10980
    },
    {
      "epoch": 1.763540078632753,
      "grad_norm": 0.3961732089519501,
      "learning_rate": 7.660323970864107e-06,
      "loss": 1.3316,
      "step": 10990
    },
    {
      "epoch": 1.765144828692931,
      "grad_norm": 0.6283671855926514,
      "learning_rate": 7.643482157769207e-06,
      "loss": 1.2858,
      "step": 11000
    },
    {
      "epoch": 1.7667495787531093,
      "grad_norm": 0.6095404028892517,
      "learning_rate": 7.626647418900609e-06,
      "loss": 1.4104,
      "step": 11010
    },
    {
      "epoch": 1.7683543288132872,
      "grad_norm": 0.5168508887290955,
      "learning_rate": 7.609819804795902e-06,
      "loss": 1.4231,
      "step": 11020
    },
    {
      "epoch": 1.7699590788734656,
      "grad_norm": 0.47013169527053833,
      "learning_rate": 7.592999365971303e-06,
      "loss": 1.3148,
      "step": 11030
    },
    {
      "epoch": 1.7715638289336435,
      "grad_norm": 0.46109142899513245,
      "learning_rate": 7.576186152921476e-06,
      "loss": 1.2762,
      "step": 11040
    },
    {
      "epoch": 1.7731685789938219,
      "grad_norm": 0.7143042683601379,
      "learning_rate": 7.5593802161194065e-06,
      "loss": 1.2455,
      "step": 11050
    },
    {
      "epoch": 1.7747733290539998,
      "grad_norm": 0.4510422945022583,
      "learning_rate": 7.5425816060162215e-06,
      "loss": 1.2338,
      "step": 11060
    },
    {
      "epoch": 1.776378079114178,
      "grad_norm": 0.6079107522964478,
      "learning_rate": 7.52579037304107e-06,
      "loss": 1.2181,
      "step": 11070
    },
    {
      "epoch": 1.777982829174356,
      "grad_norm": 0.6734358668327332,
      "learning_rate": 7.509006567600941e-06,
      "loss": 1.3018,
      "step": 11080
    },
    {
      "epoch": 1.7795875792345344,
      "grad_norm": 0.41367560625076294,
      "learning_rate": 7.49223024008053e-06,
      "loss": 1.2004,
      "step": 11090
    },
    {
      "epoch": 1.7811923292947123,
      "grad_norm": 0.5479249954223633,
      "learning_rate": 7.475461440842092e-06,
      "loss": 1.4317,
      "step": 11100
    },
    {
      "epoch": 1.7827970793548906,
      "grad_norm": 0.8279992341995239,
      "learning_rate": 7.458700220225268e-06,
      "loss": 1.2973,
      "step": 11110
    },
    {
      "epoch": 1.7844018294150685,
      "grad_norm": 0.5492921471595764,
      "learning_rate": 7.441946628546965e-06,
      "loss": 1.3057,
      "step": 11120
    },
    {
      "epoch": 1.7860065794752469,
      "grad_norm": 0.525579571723938,
      "learning_rate": 7.425200716101169e-06,
      "loss": 1.1552,
      "step": 11130
    },
    {
      "epoch": 1.7876113295354248,
      "grad_norm": 0.5571953654289246,
      "learning_rate": 7.408462533158831e-06,
      "loss": 1.3348,
      "step": 11140
    },
    {
      "epoch": 1.7892160795956031,
      "grad_norm": 0.44689229130744934,
      "learning_rate": 7.391732129967683e-06,
      "loss": 1.214,
      "step": 11150
    },
    {
      "epoch": 1.790820829655781,
      "grad_norm": 0.32095175981521606,
      "learning_rate": 7.3750095567521196e-06,
      "loss": 1.2817,
      "step": 11160
    },
    {
      "epoch": 1.7924255797159594,
      "grad_norm": 0.6287011504173279,
      "learning_rate": 7.3582948637130095e-06,
      "loss": 1.2687,
      "step": 11170
    },
    {
      "epoch": 1.7940303297761373,
      "grad_norm": 0.4316258728504181,
      "learning_rate": 7.341588101027584e-06,
      "loss": 1.2326,
      "step": 11180
    },
    {
      "epoch": 1.7956350798363157,
      "grad_norm": 0.36761370301246643,
      "learning_rate": 7.324889318849251e-06,
      "loss": 1.393,
      "step": 11190
    },
    {
      "epoch": 1.7972398298964936,
      "grad_norm": 0.3727651536464691,
      "learning_rate": 7.308198567307477e-06,
      "loss": 1.2269,
      "step": 11200
    },
    {
      "epoch": 1.798844579956672,
      "grad_norm": 0.5051815509796143,
      "learning_rate": 7.291515896507606e-06,
      "loss": 1.2457,
      "step": 11210
    },
    {
      "epoch": 1.8004493300168498,
      "grad_norm": 0.4418484568595886,
      "learning_rate": 7.274841356530739e-06,
      "loss": 1.2191,
      "step": 11220
    },
    {
      "epoch": 1.8020540800770282,
      "grad_norm": 0.4545564651489258,
      "learning_rate": 7.258174997433549e-06,
      "loss": 1.1564,
      "step": 11230
    },
    {
      "epoch": 1.803658830137206,
      "grad_norm": 0.6294673085212708,
      "learning_rate": 7.241516869248174e-06,
      "loss": 1.3425,
      "step": 11240
    },
    {
      "epoch": 1.8052635801973844,
      "grad_norm": 0.4129815101623535,
      "learning_rate": 7.224867021982018e-06,
      "loss": 1.2557,
      "step": 11250
    },
    {
      "epoch": 1.8068683302575623,
      "grad_norm": 0.371139794588089,
      "learning_rate": 7.208225505617648e-06,
      "loss": 1.3069,
      "step": 11260
    },
    {
      "epoch": 1.8084730803177407,
      "grad_norm": 0.48662349581718445,
      "learning_rate": 7.191592370112604e-06,
      "loss": 1.1918,
      "step": 11270
    },
    {
      "epoch": 1.8100778303779186,
      "grad_norm": 0.6558789014816284,
      "learning_rate": 7.174967665399282e-06,
      "loss": 1.1918,
      "step": 11280
    },
    {
      "epoch": 1.8116825804380967,
      "grad_norm": 0.4134206771850586,
      "learning_rate": 7.158351441384752e-06,
      "loss": 1.2686,
      "step": 11290
    },
    {
      "epoch": 1.8132873304982748,
      "grad_norm": 0.6505715847015381,
      "learning_rate": 7.141743747950647e-06,
      "loss": 1.3168,
      "step": 11300
    },
    {
      "epoch": 1.814892080558453,
      "grad_norm": 0.5894173383712769,
      "learning_rate": 7.125144634952965e-06,
      "loss": 1.2122,
      "step": 11310
    },
    {
      "epoch": 1.816496830618631,
      "grad_norm": 0.6591395139694214,
      "learning_rate": 7.108554152221969e-06,
      "loss": 1.2684,
      "step": 11320
    },
    {
      "epoch": 1.8181015806788092,
      "grad_norm": 0.5875928401947021,
      "learning_rate": 7.091972349562004e-06,
      "loss": 1.3269,
      "step": 11330
    },
    {
      "epoch": 1.8197063307389874,
      "grad_norm": 0.739879846572876,
      "learning_rate": 7.075399276751356e-06,
      "loss": 1.5329,
      "step": 11340
    },
    {
      "epoch": 1.8213110807991655,
      "grad_norm": 0.358539342880249,
      "learning_rate": 7.058834983542109e-06,
      "loss": 1.296,
      "step": 11350
    },
    {
      "epoch": 1.8229158308593436,
      "grad_norm": 0.5323147773742676,
      "learning_rate": 7.04227951965998e-06,
      "loss": 1.2891,
      "step": 11360
    },
    {
      "epoch": 1.8245205809195217,
      "grad_norm": 0.5676724314689636,
      "learning_rate": 7.025732934804202e-06,
      "loss": 1.3604,
      "step": 11370
    },
    {
      "epoch": 1.8261253309796999,
      "grad_norm": 0.5993578433990479,
      "learning_rate": 7.009195278647325e-06,
      "loss": 1.3605,
      "step": 11380
    },
    {
      "epoch": 1.827730081039878,
      "grad_norm": 0.4432379901409149,
      "learning_rate": 6.992666600835125e-06,
      "loss": 1.4691,
      "step": 11390
    },
    {
      "epoch": 1.8293348311000561,
      "grad_norm": 0.45297855138778687,
      "learning_rate": 6.976146950986398e-06,
      "loss": 1.2448,
      "step": 11400
    },
    {
      "epoch": 1.8309395811602343,
      "grad_norm": 0.4373244643211365,
      "learning_rate": 6.95963637869286e-06,
      "loss": 1.1384,
      "step": 11410
    },
    {
      "epoch": 1.8325443312204124,
      "grad_norm": 0.48302578926086426,
      "learning_rate": 6.94313493351896e-06,
      "loss": 1.312,
      "step": 11420
    },
    {
      "epoch": 1.8341490812805905,
      "grad_norm": 0.5795308947563171,
      "learning_rate": 6.92664266500176e-06,
      "loss": 1.2456,
      "step": 11430
    },
    {
      "epoch": 1.8357538313407686,
      "grad_norm": 0.5570203065872192,
      "learning_rate": 6.9101596226507625e-06,
      "loss": 1.2277,
      "step": 11440
    },
    {
      "epoch": 1.8373585814009468,
      "grad_norm": 0.4106871485710144,
      "learning_rate": 6.893685855947786e-06,
      "loss": 1.2248,
      "step": 11450
    },
    {
      "epoch": 1.838963331461125,
      "grad_norm": 0.3800872266292572,
      "learning_rate": 6.877221414346789e-06,
      "loss": 1.2717,
      "step": 11460
    },
    {
      "epoch": 1.840568081521303,
      "grad_norm": 0.49055901169776917,
      "learning_rate": 6.8607663472737505e-06,
      "loss": 1.225,
      "step": 11470
    },
    {
      "epoch": 1.8421728315814812,
      "grad_norm": 0.5742685198783875,
      "learning_rate": 6.844320704126494e-06,
      "loss": 1.285,
      "step": 11480
    },
    {
      "epoch": 1.8437775816416593,
      "grad_norm": 0.7320507168769836,
      "learning_rate": 6.827884534274567e-06,
      "loss": 1.3414,
      "step": 11490
    },
    {
      "epoch": 1.8453823317018374,
      "grad_norm": 0.2874715030193329,
      "learning_rate": 6.811457887059063e-06,
      "loss": 1.4825,
      "step": 11500
    },
    {
      "epoch": 1.8469870817620155,
      "grad_norm": 0.5561821460723877,
      "learning_rate": 6.795040811792504e-06,
      "loss": 1.3335,
      "step": 11510
    },
    {
      "epoch": 1.8485918318221937,
      "grad_norm": 0.5846020579338074,
      "learning_rate": 6.7786333577586615e-06,
      "loss": 1.2156,
      "step": 11520
    },
    {
      "epoch": 1.8501965818823718,
      "grad_norm": 0.5528136491775513,
      "learning_rate": 6.762235574212439e-06,
      "loss": 1.1673,
      "step": 11530
    },
    {
      "epoch": 1.85180133194255,
      "grad_norm": 0.3541504740715027,
      "learning_rate": 6.745847510379695e-06,
      "loss": 1.4387,
      "step": 11540
    },
    {
      "epoch": 1.853406082002728,
      "grad_norm": 0.34789222478866577,
      "learning_rate": 6.729469215457127e-06,
      "loss": 1.372,
      "step": 11550
    },
    {
      "epoch": 1.8550108320629062,
      "grad_norm": 0.5380761623382568,
      "learning_rate": 6.713100738612086e-06,
      "loss": 1.1702,
      "step": 11560
    },
    {
      "epoch": 1.8566155821230843,
      "grad_norm": 0.5303881764411926,
      "learning_rate": 6.696742128982469e-06,
      "loss": 1.2164,
      "step": 11570
    },
    {
      "epoch": 1.8582203321832624,
      "grad_norm": 0.5854052901268005,
      "learning_rate": 6.680393435676536e-06,
      "loss": 1.4423,
      "step": 11580
    },
    {
      "epoch": 1.8598250822434406,
      "grad_norm": 0.3337753415107727,
      "learning_rate": 6.664054707772789e-06,
      "loss": 1.3979,
      "step": 11590
    },
    {
      "epoch": 1.8614298323036187,
      "grad_norm": 0.7466180920600891,
      "learning_rate": 6.647725994319806e-06,
      "loss": 1.3125,
      "step": 11600
    },
    {
      "epoch": 1.8630345823637968,
      "grad_norm": 0.34337541460990906,
      "learning_rate": 6.631407344336111e-06,
      "loss": 1.2354,
      "step": 11610
    },
    {
      "epoch": 1.864639332423975,
      "grad_norm": 0.4028712809085846,
      "learning_rate": 6.615098806810007e-06,
      "loss": 1.2602,
      "step": 11620
    },
    {
      "epoch": 1.866244082484153,
      "grad_norm": 0.6535418629646301,
      "learning_rate": 6.5988004306994505e-06,
      "loss": 1.3773,
      "step": 11630
    },
    {
      "epoch": 1.8678488325443312,
      "grad_norm": 0.4747299253940582,
      "learning_rate": 6.582512264931886e-06,
      "loss": 1.2729,
      "step": 11640
    },
    {
      "epoch": 1.8694535826045093,
      "grad_norm": 0.5783270001411438,
      "learning_rate": 6.566234358404102e-06,
      "loss": 1.3973,
      "step": 11650
    },
    {
      "epoch": 1.8710583326646875,
      "grad_norm": 0.4734252095222473,
      "learning_rate": 6.549966759982108e-06,
      "loss": 1.2882,
      "step": 11660
    },
    {
      "epoch": 1.8726630827248656,
      "grad_norm": 0.4707244336605072,
      "learning_rate": 6.533709518500946e-06,
      "loss": 1.4396,
      "step": 11670
    },
    {
      "epoch": 1.8742678327850437,
      "grad_norm": 0.4928357005119324,
      "learning_rate": 6.517462682764583e-06,
      "loss": 1.0749,
      "step": 11680
    },
    {
      "epoch": 1.8758725828452218,
      "grad_norm": 0.40285977721214294,
      "learning_rate": 6.50122630154574e-06,
      "loss": 1.2805,
      "step": 11690
    },
    {
      "epoch": 1.8774773329054,
      "grad_norm": 0.4155314266681671,
      "learning_rate": 6.485000423585759e-06,
      "loss": 1.2612,
      "step": 11700
    },
    {
      "epoch": 1.879082082965578,
      "grad_norm": 0.44745147228240967,
      "learning_rate": 6.468785097594442e-06,
      "loss": 1.2906,
      "step": 11710
    },
    {
      "epoch": 1.8806868330257562,
      "grad_norm": 0.49821171164512634,
      "learning_rate": 6.452580372249929e-06,
      "loss": 1.1497,
      "step": 11720
    },
    {
      "epoch": 1.8822915830859344,
      "grad_norm": 0.5866856575012207,
      "learning_rate": 6.43638629619852e-06,
      "loss": 1.405,
      "step": 11730
    },
    {
      "epoch": 1.8838963331461125,
      "grad_norm": 0.5094614624977112,
      "learning_rate": 6.420202918054563e-06,
      "loss": 1.1851,
      "step": 11740
    },
    {
      "epoch": 1.8855010832062906,
      "grad_norm": 0.5514557957649231,
      "learning_rate": 6.404030286400274e-06,
      "loss": 1.2924,
      "step": 11750
    },
    {
      "epoch": 1.8871058332664687,
      "grad_norm": 0.5219709277153015,
      "learning_rate": 6.387868449785626e-06,
      "loss": 1.2223,
      "step": 11760
    },
    {
      "epoch": 1.8887105833266469,
      "grad_norm": 0.507422924041748,
      "learning_rate": 6.3717174567281656e-06,
      "loss": 1.3775,
      "step": 11770
    },
    {
      "epoch": 1.890315333386825,
      "grad_norm": 0.40702423453330994,
      "learning_rate": 6.355577355712911e-06,
      "loss": 1.2125,
      "step": 11780
    },
    {
      "epoch": 1.8919200834470031,
      "grad_norm": 0.7126702070236206,
      "learning_rate": 6.339448195192157e-06,
      "loss": 1.3784,
      "step": 11790
    },
    {
      "epoch": 1.8935248335071813,
      "grad_norm": 0.52977454662323,
      "learning_rate": 6.323330023585379e-06,
      "loss": 1.3971,
      "step": 11800
    },
    {
      "epoch": 1.8951295835673594,
      "grad_norm": 0.43204614520072937,
      "learning_rate": 6.307222889279041e-06,
      "loss": 1.1834,
      "step": 11810
    },
    {
      "epoch": 1.8967343336275375,
      "grad_norm": 0.5170060396194458,
      "learning_rate": 6.291126840626498e-06,
      "loss": 1.2533,
      "step": 11820
    },
    {
      "epoch": 1.8983390836877156,
      "grad_norm": 0.5977076888084412,
      "learning_rate": 6.2750419259478e-06,
      "loss": 1.2744,
      "step": 11830
    },
    {
      "epoch": 1.8999438337478938,
      "grad_norm": 0.7378869652748108,
      "learning_rate": 6.258968193529598e-06,
      "loss": 1.2861,
      "step": 11840
    },
    {
      "epoch": 1.901548583808072,
      "grad_norm": 0.39628249406814575,
      "learning_rate": 6.242905691624952e-06,
      "loss": 1.3821,
      "step": 11850
    },
    {
      "epoch": 1.90315333386825,
      "grad_norm": 0.4712900221347809,
      "learning_rate": 6.226854468453227e-06,
      "loss": 1.2107,
      "step": 11860
    },
    {
      "epoch": 1.9047580839284282,
      "grad_norm": 0.8376707434654236,
      "learning_rate": 6.210814572199912e-06,
      "loss": 1.2916,
      "step": 11870
    },
    {
      "epoch": 1.9063628339886063,
      "grad_norm": 0.589645504951477,
      "learning_rate": 6.19478605101651e-06,
      "loss": 1.2471,
      "step": 11880
    },
    {
      "epoch": 1.9079675840487844,
      "grad_norm": 0.5634622573852539,
      "learning_rate": 6.17876895302036e-06,
      "loss": 1.1044,
      "step": 11890
    },
    {
      "epoch": 1.9095723341089625,
      "grad_norm": 0.6380563974380493,
      "learning_rate": 6.162763326294525e-06,
      "loss": 1.1816,
      "step": 11900
    },
    {
      "epoch": 1.9111770841691407,
      "grad_norm": 0.537203311920166,
      "learning_rate": 6.146769218887615e-06,
      "loss": 1.2895,
      "step": 11910
    },
    {
      "epoch": 1.9127818342293188,
      "grad_norm": 0.7563451528549194,
      "learning_rate": 6.130786678813669e-06,
      "loss": 1.4434,
      "step": 11920
    },
    {
      "epoch": 1.914386584289497,
      "grad_norm": 0.39798450469970703,
      "learning_rate": 6.114815754052002e-06,
      "loss": 1.1186,
      "step": 11930
    },
    {
      "epoch": 1.915991334349675,
      "grad_norm": 0.5859978199005127,
      "learning_rate": 6.0988564925470565e-06,
      "loss": 1.2253,
      "step": 11940
    },
    {
      "epoch": 1.9175960844098532,
      "grad_norm": 0.5101495981216431,
      "learning_rate": 6.082908942208261e-06,
      "loss": 1.3134,
      "step": 11950
    },
    {
      "epoch": 1.9192008344700313,
      "grad_norm": 0.6529114842414856,
      "learning_rate": 6.0669731509098895e-06,
      "loss": 1.3555,
      "step": 11960
    },
    {
      "epoch": 1.9208055845302094,
      "grad_norm": 0.7944371700286865,
      "learning_rate": 6.051049166490917e-06,
      "loss": 1.3724,
      "step": 11970
    },
    {
      "epoch": 1.9224103345903876,
      "grad_norm": 0.6208010315895081,
      "learning_rate": 6.03513703675487e-06,
      "loss": 1.3231,
      "step": 11980
    },
    {
      "epoch": 1.9240150846505657,
      "grad_norm": 0.38422515988349915,
      "learning_rate": 6.019236809469693e-06,
      "loss": 1.2217,
      "step": 11990
    },
    {
      "epoch": 1.9256198347107438,
      "grad_norm": 0.3641115725040436,
      "learning_rate": 6.0033485323675946e-06,
      "loss": 1.3817,
      "step": 12000
    },
    {
      "epoch": 1.927224584770922,
      "grad_norm": 0.5047890543937683,
      "learning_rate": 5.987472253144911e-06,
      "loss": 1.4024,
      "step": 12010
    },
    {
      "epoch": 1.9288293348311,
      "grad_norm": 0.5633889436721802,
      "learning_rate": 5.971608019461961e-06,
      "loss": 1.1811,
      "step": 12020
    },
    {
      "epoch": 1.9304340848912782,
      "grad_norm": 0.5705041885375977,
      "learning_rate": 5.955755878942907e-06,
      "loss": 1.3279,
      "step": 12030
    },
    {
      "epoch": 1.9320388349514563,
      "grad_norm": 0.9632352590560913,
      "learning_rate": 5.939915879175597e-06,
      "loss": 1.4205,
      "step": 12040
    },
    {
      "epoch": 1.9336435850116345,
      "grad_norm": 0.4464355707168579,
      "learning_rate": 5.924088067711449e-06,
      "loss": 1.1164,
      "step": 12050
    },
    {
      "epoch": 1.9352483350718126,
      "grad_norm": 0.6112291812896729,
      "learning_rate": 5.908272492065272e-06,
      "loss": 1.2076,
      "step": 12060
    },
    {
      "epoch": 1.9368530851319907,
      "grad_norm": 0.45659881830215454,
      "learning_rate": 5.892469199715163e-06,
      "loss": 1.3557,
      "step": 12070
    },
    {
      "epoch": 1.9384578351921689,
      "grad_norm": 0.38872668147087097,
      "learning_rate": 5.8766782381023275e-06,
      "loss": 1.1482,
      "step": 12080
    },
    {
      "epoch": 1.940062585252347,
      "grad_norm": 0.48537006974220276,
      "learning_rate": 5.860899654630972e-06,
      "loss": 1.2655,
      "step": 12090
    },
    {
      "epoch": 1.9416673353125251,
      "grad_norm": 0.5485859513282776,
      "learning_rate": 5.845133496668123e-06,
      "loss": 1.4044,
      "step": 12100
    },
    {
      "epoch": 1.9432720853727032,
      "grad_norm": 0.49417558312416077,
      "learning_rate": 5.829379811543529e-06,
      "loss": 1.2109,
      "step": 12110
    },
    {
      "epoch": 1.9448768354328814,
      "grad_norm": 0.4521615505218506,
      "learning_rate": 5.813638646549473e-06,
      "loss": 1.1542,
      "step": 12120
    },
    {
      "epoch": 1.9464815854930595,
      "grad_norm": 0.41364559531211853,
      "learning_rate": 5.797910048940673e-06,
      "loss": 1.2462,
      "step": 12130
    },
    {
      "epoch": 1.9480863355532376,
      "grad_norm": 0.5391426086425781,
      "learning_rate": 5.782194065934101e-06,
      "loss": 1.2402,
      "step": 12140
    },
    {
      "epoch": 1.9496910856134158,
      "grad_norm": 0.40889647603034973,
      "learning_rate": 5.766490744708877e-06,
      "loss": 1.3727,
      "step": 12150
    },
    {
      "epoch": 1.9512958356735939,
      "grad_norm": 0.6610537171363831,
      "learning_rate": 5.750800132406096e-06,
      "loss": 1.0631,
      "step": 12160
    },
    {
      "epoch": 1.952900585733772,
      "grad_norm": 0.5043777227401733,
      "learning_rate": 5.735122276128717e-06,
      "loss": 1.2527,
      "step": 12170
    },
    {
      "epoch": 1.9545053357939501,
      "grad_norm": 0.6659494638442993,
      "learning_rate": 5.7194572229413866e-06,
      "loss": 1.1272,
      "step": 12180
    },
    {
      "epoch": 1.9561100858541283,
      "grad_norm": 0.743131697177887,
      "learning_rate": 5.7038050198703365e-06,
      "loss": 1.2838,
      "step": 12190
    },
    {
      "epoch": 1.9577148359143064,
      "grad_norm": 0.4084281623363495,
      "learning_rate": 5.688165713903204e-06,
      "loss": 1.3122,
      "step": 12200
    },
    {
      "epoch": 1.9593195859744845,
      "grad_norm": 0.6288772821426392,
      "learning_rate": 5.6725393519889275e-06,
      "loss": 1.2311,
      "step": 12210
    },
    {
      "epoch": 1.9609243360346627,
      "grad_norm": 0.6304981112480164,
      "learning_rate": 5.6569259810375686e-06,
      "loss": 1.251,
      "step": 12220
    },
    {
      "epoch": 1.9625290860948408,
      "grad_norm": 0.6155768036842346,
      "learning_rate": 5.641325647920208e-06,
      "loss": 1.3336,
      "step": 12230
    },
    {
      "epoch": 1.964133836155019,
      "grad_norm": 0.5068325400352478,
      "learning_rate": 5.6257383994687745e-06,
      "loss": 1.3921,
      "step": 12240
    },
    {
      "epoch": 1.965738586215197,
      "grad_norm": 0.4182339012622833,
      "learning_rate": 5.610164282475925e-06,
      "loss": 1.2467,
      "step": 12250
    },
    {
      "epoch": 1.9673433362753752,
      "grad_norm": 0.6401615738868713,
      "learning_rate": 5.594603343694889e-06,
      "loss": 1.4289,
      "step": 12260
    },
    {
      "epoch": 1.968948086335553,
      "grad_norm": 0.6725279092788696,
      "learning_rate": 5.579055629839344e-06,
      "loss": 1.205,
      "step": 12270
    },
    {
      "epoch": 1.9705528363957314,
      "grad_norm": 0.5423653721809387,
      "learning_rate": 5.563521187583254e-06,
      "loss": 1.2668,
      "step": 12280
    },
    {
      "epoch": 1.9721575864559093,
      "grad_norm": 0.5271467566490173,
      "learning_rate": 5.548000063560757e-06,
      "loss": 1.2236,
      "step": 12290
    },
    {
      "epoch": 1.9737623365160877,
      "grad_norm": 0.36739709973335266,
      "learning_rate": 5.532492304365995e-06,
      "loss": 1.3192,
      "step": 12300
    },
    {
      "epoch": 1.9753670865762656,
      "grad_norm": 0.608930230140686,
      "learning_rate": 5.516997956553003e-06,
      "loss": 1.3134,
      "step": 12310
    },
    {
      "epoch": 1.976971836636444,
      "grad_norm": 0.5082205533981323,
      "learning_rate": 5.501517066635541e-06,
      "loss": 1.1858,
      "step": 12320
    },
    {
      "epoch": 1.9785765866966218,
      "grad_norm": 0.3333296477794647,
      "learning_rate": 5.486049681086984e-06,
      "loss": 1.2533,
      "step": 12330
    },
    {
      "epoch": 1.9801813367568002,
      "grad_norm": 0.4584592878818512,
      "learning_rate": 5.470595846340152e-06,
      "loss": 1.2911,
      "step": 12340
    },
    {
      "epoch": 1.981786086816978,
      "grad_norm": 0.4483330249786377,
      "learning_rate": 5.455155608787195e-06,
      "loss": 1.2942,
      "step": 12350
    },
    {
      "epoch": 1.9833908368771564,
      "grad_norm": 0.6970025897026062,
      "learning_rate": 5.439729014779447e-06,
      "loss": 1.2964,
      "step": 12360
    },
    {
      "epoch": 1.9849955869373344,
      "grad_norm": 0.37287676334381104,
      "learning_rate": 5.424316110627272e-06,
      "loss": 1.2598,
      "step": 12370
    },
    {
      "epoch": 1.9866003369975127,
      "grad_norm": 0.4495922327041626,
      "learning_rate": 5.408916942599953e-06,
      "loss": 1.2951,
      "step": 12380
    },
    {
      "epoch": 1.9882050870576906,
      "grad_norm": 0.6824597120285034,
      "learning_rate": 5.3935315569255205e-06,
      "loss": 1.3447,
      "step": 12390
    },
    {
      "epoch": 1.989809837117869,
      "grad_norm": 0.42082539200782776,
      "learning_rate": 5.378159999790648e-06,
      "loss": 1.2248,
      "step": 12400
    },
    {
      "epoch": 1.9914145871780469,
      "grad_norm": 0.5878294110298157,
      "learning_rate": 5.3628023173404805e-06,
      "loss": 1.3231,
      "step": 12410
    },
    {
      "epoch": 1.9930193372382252,
      "grad_norm": 0.5531538128852844,
      "learning_rate": 5.347458555678524e-06,
      "loss": 1.2489,
      "step": 12420
    },
    {
      "epoch": 1.9946240872984031,
      "grad_norm": 0.45372524857521057,
      "learning_rate": 5.332128760866483e-06,
      "loss": 1.2782,
      "step": 12430
    },
    {
      "epoch": 1.9962288373585815,
      "grad_norm": 0.3124426603317261,
      "learning_rate": 5.316812978924146e-06,
      "loss": 1.2618,
      "step": 12440
    },
    {
      "epoch": 1.9978335874187594,
      "grad_norm": 0.4132457375526428,
      "learning_rate": 5.301511255829221e-06,
      "loss": 1.2182,
      "step": 12450
    },
    {
      "epoch": 1.9994383374789377,
      "grad_norm": 0.5237779021263123,
      "learning_rate": 5.2862236375172275e-06,
      "loss": 1.2882,
      "step": 12460
    },
    {
      "epoch": 2.0009628500361067,
      "grad_norm": 0.789932906627655,
      "learning_rate": 5.270950169881326e-06,
      "loss": 1.3747,
      "step": 12470
    },
    {
      "epoch": 2.002567600096285,
      "grad_norm": 0.4757423996925354,
      "learning_rate": 5.255690898772215e-06,
      "loss": 1.2722,
      "step": 12480
    },
    {
      "epoch": 2.004172350156463,
      "grad_norm": 0.603412926197052,
      "learning_rate": 5.24044586999796e-06,
      "loss": 1.2884,
      "step": 12490
    },
    {
      "epoch": 2.0057771002166414,
      "grad_norm": 0.5154134631156921,
      "learning_rate": 5.225215129323873e-06,
      "loss": 1.3782,
      "step": 12500
    },
    {
      "epoch": 2.0073818502768193,
      "grad_norm": 0.4650868773460388,
      "learning_rate": 5.2099987224723855e-06,
      "loss": 1.1717,
      "step": 12510
    },
    {
      "epoch": 2.0089866003369976,
      "grad_norm": 0.3288191556930542,
      "learning_rate": 5.1947966951228835e-06,
      "loss": 1.1832,
      "step": 12520
    },
    {
      "epoch": 2.0105913503971755,
      "grad_norm": 0.6594498157501221,
      "learning_rate": 5.179609092911601e-06,
      "loss": 1.3006,
      "step": 12530
    },
    {
      "epoch": 2.012196100457354,
      "grad_norm": 0.2897605895996094,
      "learning_rate": 5.164435961431451e-06,
      "loss": 1.1489,
      "step": 12540
    },
    {
      "epoch": 2.0138008505175318,
      "grad_norm": 0.6144582033157349,
      "learning_rate": 5.149277346231926e-06,
      "loss": 1.2576,
      "step": 12550
    },
    {
      "epoch": 2.01540560057771,
      "grad_norm": 0.5127604007720947,
      "learning_rate": 5.134133292818919e-06,
      "loss": 1.3973,
      "step": 12560
    },
    {
      "epoch": 2.017010350637888,
      "grad_norm": 0.4714965224266052,
      "learning_rate": 5.119003846654628e-06,
      "loss": 1.2085,
      "step": 12570
    },
    {
      "epoch": 2.0186151006980664,
      "grad_norm": 0.526009738445282,
      "learning_rate": 5.103889053157385e-06,
      "loss": 1.3322,
      "step": 12580
    },
    {
      "epoch": 2.0202198507582443,
      "grad_norm": 0.44716447591781616,
      "learning_rate": 5.088788957701549e-06,
      "loss": 1.1928,
      "step": 12590
    },
    {
      "epoch": 2.0218246008184226,
      "grad_norm": 0.45488476753234863,
      "learning_rate": 5.073703605617339e-06,
      "loss": 1.2091,
      "step": 12600
    },
    {
      "epoch": 2.0234293508786005,
      "grad_norm": 0.6601904034614563,
      "learning_rate": 5.058633042190737e-06,
      "loss": 1.2625,
      "step": 12610
    },
    {
      "epoch": 2.025034100938779,
      "grad_norm": 0.4816506803035736,
      "learning_rate": 5.043577312663304e-06,
      "loss": 1.3062,
      "step": 12620
    },
    {
      "epoch": 2.026638850998957,
      "grad_norm": 0.38517481088638306,
      "learning_rate": 5.0285364622320935e-06,
      "loss": 1.2336,
      "step": 12630
    },
    {
      "epoch": 2.028243601059135,
      "grad_norm": 0.5105124711990356,
      "learning_rate": 5.0135105360494714e-06,
      "loss": 1.3202,
      "step": 12640
    },
    {
      "epoch": 2.029848351119313,
      "grad_norm": 0.5350769758224487,
      "learning_rate": 4.998499579223022e-06,
      "loss": 1.2289,
      "step": 12650
    },
    {
      "epoch": 2.0314531011794914,
      "grad_norm": 0.6596155762672424,
      "learning_rate": 4.98350363681537e-06,
      "loss": 1.4415,
      "step": 12660
    },
    {
      "epoch": 2.0330578512396693,
      "grad_norm": 0.30045419931411743,
      "learning_rate": 4.968522753844089e-06,
      "loss": 1.313,
      "step": 12670
    },
    {
      "epoch": 2.0346626012998477,
      "grad_norm": 0.5004703402519226,
      "learning_rate": 4.953556975281525e-06,
      "loss": 1.2066,
      "step": 12680
    },
    {
      "epoch": 2.0362673513600256,
      "grad_norm": 0.5507836937904358,
      "learning_rate": 4.9386063460546975e-06,
      "loss": 1.29,
      "step": 12690
    },
    {
      "epoch": 2.037872101420204,
      "grad_norm": 0.582142174243927,
      "learning_rate": 4.923670911045131e-06,
      "loss": 1.4038,
      "step": 12700
    },
    {
      "epoch": 2.039476851480382,
      "grad_norm": 0.739469051361084,
      "learning_rate": 4.908750715088755e-06,
      "loss": 1.2719,
      "step": 12710
    },
    {
      "epoch": 2.04108160154056,
      "grad_norm": 0.4340607523918152,
      "learning_rate": 4.893845802975735e-06,
      "loss": 1.1876,
      "step": 12720
    },
    {
      "epoch": 2.042686351600738,
      "grad_norm": 0.6972562074661255,
      "learning_rate": 4.878956219450371e-06,
      "loss": 1.1809,
      "step": 12730
    },
    {
      "epoch": 2.0442911016609164,
      "grad_norm": 0.49436816573143005,
      "learning_rate": 4.864082009210927e-06,
      "loss": 1.3997,
      "step": 12740
    },
    {
      "epoch": 2.0458958517210943,
      "grad_norm": 0.5425479412078857,
      "learning_rate": 4.849223216909541e-06,
      "loss": 1.2269,
      "step": 12750
    },
    {
      "epoch": 2.0475006017812727,
      "grad_norm": 0.4724436402320862,
      "learning_rate": 4.834379887152046e-06,
      "loss": 1.4167,
      "step": 12760
    },
    {
      "epoch": 2.0491053518414506,
      "grad_norm": 0.5607113242149353,
      "learning_rate": 4.819552064497862e-06,
      "loss": 1.2683,
      "step": 12770
    },
    {
      "epoch": 2.050710101901629,
      "grad_norm": 0.5014428496360779,
      "learning_rate": 4.8047397934598675e-06,
      "loss": 1.2839,
      "step": 12780
    },
    {
      "epoch": 2.052314851961807,
      "grad_norm": 0.4832276403903961,
      "learning_rate": 4.789943118504237e-06,
      "loss": 1.2661,
      "step": 12790
    },
    {
      "epoch": 2.053919602021985,
      "grad_norm": 0.46134960651397705,
      "learning_rate": 4.775162084050346e-06,
      "loss": 1.1052,
      "step": 12800
    },
    {
      "epoch": 2.055524352082163,
      "grad_norm": 0.5230177044868469,
      "learning_rate": 4.760396734470598e-06,
      "loss": 1.3531,
      "step": 12810
    },
    {
      "epoch": 2.0571291021423415,
      "grad_norm": 0.7218384146690369,
      "learning_rate": 4.745647114090328e-06,
      "loss": 1.2969,
      "step": 12820
    },
    {
      "epoch": 2.0587338522025194,
      "grad_norm": 0.45237216353416443,
      "learning_rate": 4.730913267187638e-06,
      "loss": 1.3614,
      "step": 12830
    },
    {
      "epoch": 2.0603386022626977,
      "grad_norm": 0.37226054072380066,
      "learning_rate": 4.716195237993292e-06,
      "loss": 1.3032,
      "step": 12840
    },
    {
      "epoch": 2.0619433523228756,
      "grad_norm": 0.6413739323616028,
      "learning_rate": 4.701493070690552e-06,
      "loss": 1.1911,
      "step": 12850
    },
    {
      "epoch": 2.063548102383054,
      "grad_norm": 0.4918959438800812,
      "learning_rate": 4.686806809415085e-06,
      "loss": 1.332,
      "step": 12860
    },
    {
      "epoch": 2.065152852443232,
      "grad_norm": 0.5002651214599609,
      "learning_rate": 4.6721364982547856e-06,
      "loss": 1.3264,
      "step": 12870
    },
    {
      "epoch": 2.0667576025034102,
      "grad_norm": 0.4507034420967102,
      "learning_rate": 4.657482181249685e-06,
      "loss": 1.2291,
      "step": 12880
    },
    {
      "epoch": 2.068362352563588,
      "grad_norm": 0.5322330594062805,
      "learning_rate": 4.642843902391787e-06,
      "loss": 1.3727,
      "step": 12890
    },
    {
      "epoch": 2.0699671026237665,
      "grad_norm": 0.6268169283866882,
      "learning_rate": 4.628221705624959e-06,
      "loss": 1.2921,
      "step": 12900
    },
    {
      "epoch": 2.0715718526839444,
      "grad_norm": 0.7267871499061584,
      "learning_rate": 4.6136156348447815e-06,
      "loss": 1.2566,
      "step": 12910
    },
    {
      "epoch": 2.0731766027441227,
      "grad_norm": 0.43746107816696167,
      "learning_rate": 4.599025733898435e-06,
      "loss": 1.1958,
      "step": 12920
    },
    {
      "epoch": 2.0747813528043006,
      "grad_norm": 0.7549811005592346,
      "learning_rate": 4.584452046584546e-06,
      "loss": 1.3496,
      "step": 12930
    },
    {
      "epoch": 2.076386102864479,
      "grad_norm": 0.560555636882782,
      "learning_rate": 4.569894616653081e-06,
      "loss": 1.1722,
      "step": 12940
    },
    {
      "epoch": 2.077990852924657,
      "grad_norm": 0.5174969434738159,
      "learning_rate": 4.555353487805191e-06,
      "loss": 1.2133,
      "step": 12950
    },
    {
      "epoch": 2.0795956029848353,
      "grad_norm": 0.4906492233276367,
      "learning_rate": 4.540828703693101e-06,
      "loss": 1.2563,
      "step": 12960
    },
    {
      "epoch": 2.081200353045013,
      "grad_norm": 0.48303502798080444,
      "learning_rate": 4.526320307919959e-06,
      "loss": 1.3202,
      "step": 12970
    },
    {
      "epoch": 2.0828051031051915,
      "grad_norm": 0.42304080724716187,
      "learning_rate": 4.511828344039729e-06,
      "loss": 1.228,
      "step": 12980
    },
    {
      "epoch": 2.0844098531653694,
      "grad_norm": 0.5038099884986877,
      "learning_rate": 4.49735285555703e-06,
      "loss": 1.3146,
      "step": 12990
    },
    {
      "epoch": 2.0860146032255478,
      "grad_norm": 0.552196741104126,
      "learning_rate": 4.48289388592704e-06,
      "loss": 1.2895,
      "step": 13000
    },
    {
      "epoch": 2.0876193532857257,
      "grad_norm": 0.5619484782218933,
      "learning_rate": 4.46845147855533e-06,
      "loss": 1.2821,
      "step": 13010
    },
    {
      "epoch": 2.089224103345904,
      "grad_norm": 0.5327500700950623,
      "learning_rate": 4.454025676797768e-06,
      "loss": 1.3705,
      "step": 13020
    },
    {
      "epoch": 2.090828853406082,
      "grad_norm": 0.6353708505630493,
      "learning_rate": 4.439616523960358e-06,
      "loss": 1.3315,
      "step": 13030
    },
    {
      "epoch": 2.0924336034662603,
      "grad_norm": 0.4748678207397461,
      "learning_rate": 4.4252240632991375e-06,
      "loss": 1.1974,
      "step": 13040
    },
    {
      "epoch": 2.094038353526438,
      "grad_norm": 0.7303181886672974,
      "learning_rate": 4.410848338020023e-06,
      "loss": 1.3937,
      "step": 13050
    },
    {
      "epoch": 2.0956431035866165,
      "grad_norm": 0.3165464997291565,
      "learning_rate": 4.396489391278692e-06,
      "loss": 1.4207,
      "step": 13060
    },
    {
      "epoch": 2.0972478536467944,
      "grad_norm": 0.7755902409553528,
      "learning_rate": 4.382147266180466e-06,
      "loss": 1.3527,
      "step": 13070
    },
    {
      "epoch": 2.098852603706973,
      "grad_norm": 0.3396570086479187,
      "learning_rate": 4.367822005780152e-06,
      "loss": 1.3238,
      "step": 13080
    },
    {
      "epoch": 2.1004573537671507,
      "grad_norm": 0.3574198782444,
      "learning_rate": 4.353513653081945e-06,
      "loss": 1.247,
      "step": 13090
    },
    {
      "epoch": 2.102062103827329,
      "grad_norm": 0.7271413803100586,
      "learning_rate": 4.339222251039265e-06,
      "loss": 1.315,
      "step": 13100
    },
    {
      "epoch": 2.103666853887507,
      "grad_norm": 0.4964822828769684,
      "learning_rate": 4.324947842554667e-06,
      "loss": 1.3134,
      "step": 13110
    },
    {
      "epoch": 2.1052716039476853,
      "grad_norm": 0.45025691390037537,
      "learning_rate": 4.3106904704796735e-06,
      "loss": 1.2077,
      "step": 13120
    },
    {
      "epoch": 2.106876354007863,
      "grad_norm": 0.5630126595497131,
      "learning_rate": 4.2964501776146785e-06,
      "loss": 1.126,
      "step": 13130
    },
    {
      "epoch": 2.1084811040680416,
      "grad_norm": 0.46542200446128845,
      "learning_rate": 4.28222700670879e-06,
      "loss": 1.3332,
      "step": 13140
    },
    {
      "epoch": 2.1100858541282195,
      "grad_norm": 0.4383372366428375,
      "learning_rate": 4.2680210004597335e-06,
      "loss": 1.1897,
      "step": 13150
    },
    {
      "epoch": 2.111690604188398,
      "grad_norm": 0.36567071080207825,
      "learning_rate": 4.253832201513687e-06,
      "loss": 1.3154,
      "step": 13160
    },
    {
      "epoch": 2.1132953542485757,
      "grad_norm": 0.4824807047843933,
      "learning_rate": 4.239660652465192e-06,
      "loss": 1.2974,
      "step": 13170
    },
    {
      "epoch": 2.114900104308754,
      "grad_norm": 0.6025552153587341,
      "learning_rate": 4.225506395856989e-06,
      "loss": 1.1115,
      "step": 13180
    },
    {
      "epoch": 2.116504854368932,
      "grad_norm": 0.49352261424064636,
      "learning_rate": 4.211369474179919e-06,
      "loss": 1.3589,
      "step": 13190
    },
    {
      "epoch": 2.1181096044291103,
      "grad_norm": 0.6306906342506409,
      "learning_rate": 4.197249929872776e-06,
      "loss": 1.3276,
      "step": 13200
    },
    {
      "epoch": 2.1197143544892882,
      "grad_norm": 0.5436266660690308,
      "learning_rate": 4.183147805322196e-06,
      "loss": 1.1367,
      "step": 13210
    },
    {
      "epoch": 2.1213191045494666,
      "grad_norm": 0.28791671991348267,
      "learning_rate": 4.16906314286251e-06,
      "loss": 1.2447,
      "step": 13220
    },
    {
      "epoch": 2.1229238546096445,
      "grad_norm": 1.2385879755020142,
      "learning_rate": 4.154995984775638e-06,
      "loss": 1.3387,
      "step": 13230
    },
    {
      "epoch": 2.124528604669823,
      "grad_norm": 0.6334934830665588,
      "learning_rate": 4.140946373290945e-06,
      "loss": 1.3845,
      "step": 13240
    },
    {
      "epoch": 2.1261333547300008,
      "grad_norm": 0.42568662762641907,
      "learning_rate": 4.1269143505851295e-06,
      "loss": 1.3493,
      "step": 13250
    },
    {
      "epoch": 2.127738104790179,
      "grad_norm": 0.5190733075141907,
      "learning_rate": 4.1128999587820775e-06,
      "loss": 1.3278,
      "step": 13260
    },
    {
      "epoch": 2.129342854850357,
      "grad_norm": 0.7533910870552063,
      "learning_rate": 4.09890323995276e-06,
      "loss": 1.3187,
      "step": 13270
    },
    {
      "epoch": 2.1309476049105354,
      "grad_norm": 0.5329356789588928,
      "learning_rate": 4.084924236115082e-06,
      "loss": 1.2537,
      "step": 13280
    },
    {
      "epoch": 2.1325523549707133,
      "grad_norm": 0.6288428902626038,
      "learning_rate": 4.07096298923378e-06,
      "loss": 1.2277,
      "step": 13290
    },
    {
      "epoch": 2.1341571050308916,
      "grad_norm": 0.8970685601234436,
      "learning_rate": 4.057019541220272e-06,
      "loss": 1.4049,
      "step": 13300
    },
    {
      "epoch": 2.1357618550910695,
      "grad_norm": 0.45294103026390076,
      "learning_rate": 4.043093933932558e-06,
      "loss": 1.3034,
      "step": 13310
    },
    {
      "epoch": 2.137366605151248,
      "grad_norm": 0.38794806599617004,
      "learning_rate": 4.02918620917507e-06,
      "loss": 1.2764,
      "step": 13320
    },
    {
      "epoch": 2.138971355211426,
      "grad_norm": 0.4624422490596771,
      "learning_rate": 4.015296408698556e-06,
      "loss": 1.2667,
      "step": 13330
    },
    {
      "epoch": 2.140576105271604,
      "grad_norm": 0.45576006174087524,
      "learning_rate": 4.001424574199966e-06,
      "loss": 1.1627,
      "step": 13340
    },
    {
      "epoch": 2.142180855331782,
      "grad_norm": 0.43576085567474365,
      "learning_rate": 3.987570747322303e-06,
      "loss": 1.2478,
      "step": 13350
    },
    {
      "epoch": 2.1437856053919604,
      "grad_norm": 0.4686228632926941,
      "learning_rate": 3.973734969654528e-06,
      "loss": 1.2425,
      "step": 13360
    },
    {
      "epoch": 2.1453903554521383,
      "grad_norm": 0.5764669179916382,
      "learning_rate": 3.9599172827314e-06,
      "loss": 1.3849,
      "step": 13370
    },
    {
      "epoch": 2.1469951055123166,
      "grad_norm": 0.43015149235725403,
      "learning_rate": 3.946117728033388e-06,
      "loss": 1.2621,
      "step": 13380
    },
    {
      "epoch": 2.1485998555724946,
      "grad_norm": 0.45538100600242615,
      "learning_rate": 3.9323363469865096e-06,
      "loss": 1.1934,
      "step": 13390
    },
    {
      "epoch": 2.150204605632673,
      "grad_norm": 0.40922218561172485,
      "learning_rate": 3.918573180962243e-06,
      "loss": 1.3173,
      "step": 13400
    },
    {
      "epoch": 2.151809355692851,
      "grad_norm": 0.48956626653671265,
      "learning_rate": 3.904828271277372e-06,
      "loss": 1.337,
      "step": 13410
    },
    {
      "epoch": 2.153414105753029,
      "grad_norm": 0.3525984287261963,
      "learning_rate": 3.891101659193887e-06,
      "loss": 1.2667,
      "step": 13420
    },
    {
      "epoch": 2.155018855813207,
      "grad_norm": 0.575096845626831,
      "learning_rate": 3.877393385918834e-06,
      "loss": 1.206,
      "step": 13430
    },
    {
      "epoch": 2.1566236058733854,
      "grad_norm": 0.4286726713180542,
      "learning_rate": 3.863703492604225e-06,
      "loss": 1.2125,
      "step": 13440
    },
    {
      "epoch": 2.1582283559335633,
      "grad_norm": 0.5775670409202576,
      "learning_rate": 3.850032020346878e-06,
      "loss": 1.2932,
      "step": 13450
    },
    {
      "epoch": 2.1598331059937417,
      "grad_norm": 0.564394474029541,
      "learning_rate": 3.836379010188324e-06,
      "loss": 1.4875,
      "step": 13460
    },
    {
      "epoch": 2.1614378560539196,
      "grad_norm": 0.6403664350509644,
      "learning_rate": 3.822744503114663e-06,
      "loss": 1.3109,
      "step": 13470
    },
    {
      "epoch": 2.163042606114098,
      "grad_norm": 0.6253321170806885,
      "learning_rate": 3.8091285400564525e-06,
      "loss": 1.2761,
      "step": 13480
    },
    {
      "epoch": 2.164647356174276,
      "grad_norm": 0.45796576142311096,
      "learning_rate": 3.795531161888586e-06,
      "loss": 1.2786,
      "step": 13490
    },
    {
      "epoch": 2.166252106234454,
      "grad_norm": 0.6011670827865601,
      "learning_rate": 3.7819524094301517e-06,
      "loss": 1.4867,
      "step": 13500
    },
    {
      "epoch": 2.167856856294632,
      "grad_norm": 0.3494843542575836,
      "learning_rate": 3.768392323444342e-06,
      "loss": 1.2508,
      "step": 13510
    },
    {
      "epoch": 2.1694616063548104,
      "grad_norm": 0.6350054740905762,
      "learning_rate": 3.7548509446382954e-06,
      "loss": 1.4643,
      "step": 13520
    },
    {
      "epoch": 2.1710663564149884,
      "grad_norm": 0.4539957642555237,
      "learning_rate": 3.7413283136630063e-06,
      "loss": 1.156,
      "step": 13530
    },
    {
      "epoch": 2.1726711064751667,
      "grad_norm": 0.5929547548294067,
      "learning_rate": 3.727824471113174e-06,
      "loss": 1.1886,
      "step": 13540
    },
    {
      "epoch": 2.1742758565353446,
      "grad_norm": 0.6731887459754944,
      "learning_rate": 3.714339457527112e-06,
      "loss": 1.2553,
      "step": 13550
    },
    {
      "epoch": 2.1758806065955225,
      "grad_norm": 0.5324034094810486,
      "learning_rate": 3.7008733133865927e-06,
      "loss": 1.3484,
      "step": 13560
    },
    {
      "epoch": 2.177485356655701,
      "grad_norm": 0.7369810342788696,
      "learning_rate": 3.687426079116757e-06,
      "loss": 1.2671,
      "step": 13570
    },
    {
      "epoch": 2.179090106715879,
      "grad_norm": 0.4912567138671875,
      "learning_rate": 3.6739977950859663e-06,
      "loss": 1.0874,
      "step": 13580
    },
    {
      "epoch": 2.180694856776057,
      "grad_norm": 0.4154602885246277,
      "learning_rate": 3.6605885016057063e-06,
      "loss": 1.2266,
      "step": 13590
    },
    {
      "epoch": 2.182299606836235,
      "grad_norm": 0.7043424844741821,
      "learning_rate": 3.6471982389304417e-06,
      "loss": 1.3726,
      "step": 13600
    },
    {
      "epoch": 2.1839043568964134,
      "grad_norm": 0.4162144064903259,
      "learning_rate": 3.6338270472575164e-06,
      "loss": 1.3874,
      "step": 13610
    },
    {
      "epoch": 2.1855091069565917,
      "grad_norm": 0.36807772517204285,
      "learning_rate": 3.6204749667270147e-06,
      "loss": 1.2986,
      "step": 13620
    },
    {
      "epoch": 2.1871138570167696,
      "grad_norm": 0.4176618754863739,
      "learning_rate": 3.6071420374216594e-06,
      "loss": 1.2094,
      "step": 13630
    },
    {
      "epoch": 2.1887186070769475,
      "grad_norm": 0.6263065934181213,
      "learning_rate": 3.5938282993666705e-06,
      "loss": 1.2863,
      "step": 13640
    },
    {
      "epoch": 2.190323357137126,
      "grad_norm": 0.7772393822669983,
      "learning_rate": 3.5805337925296692e-06,
      "loss": 1.2822,
      "step": 13650
    },
    {
      "epoch": 2.1919281071973042,
      "grad_norm": 0.4929432272911072,
      "learning_rate": 3.56725855682053e-06,
      "loss": 1.2987,
      "step": 13660
    },
    {
      "epoch": 2.193532857257482,
      "grad_norm": 0.5012620091438293,
      "learning_rate": 3.5540026320912924e-06,
      "loss": 1.3245,
      "step": 13670
    },
    {
      "epoch": 2.19513760731766,
      "grad_norm": 0.44786372780799866,
      "learning_rate": 3.540766058136007e-06,
      "loss": 1.3247,
      "step": 13680
    },
    {
      "epoch": 2.1967423573778384,
      "grad_norm": 0.5712989568710327,
      "learning_rate": 3.5275488746906505e-06,
      "loss": 1.1117,
      "step": 13690
    },
    {
      "epoch": 2.1983471074380168,
      "grad_norm": 0.3385510742664337,
      "learning_rate": 3.514351121432974e-06,
      "loss": 1.0831,
      "step": 13700
    },
    {
      "epoch": 2.1999518574981947,
      "grad_norm": 0.7917481064796448,
      "learning_rate": 3.501172837982416e-06,
      "loss": 1.3523,
      "step": 13710
    },
    {
      "epoch": 2.2015566075583726,
      "grad_norm": 0.49354422092437744,
      "learning_rate": 3.4880140638999503e-06,
      "loss": 1.3103,
      "step": 13720
    },
    {
      "epoch": 2.203161357618551,
      "grad_norm": 0.42761820554733276,
      "learning_rate": 3.4748748386879982e-06,
      "loss": 1.2725,
      "step": 13730
    },
    {
      "epoch": 2.2047661076787293,
      "grad_norm": 0.5911994576454163,
      "learning_rate": 3.4617552017902835e-06,
      "loss": 1.2858,
      "step": 13740
    },
    {
      "epoch": 2.206370857738907,
      "grad_norm": 0.47753551602363586,
      "learning_rate": 3.448655192591738e-06,
      "loss": 1.2643,
      "step": 13750
    },
    {
      "epoch": 2.207975607799085,
      "grad_norm": 0.3645694851875305,
      "learning_rate": 3.4355748504183583e-06,
      "loss": 1.1931,
      "step": 13760
    },
    {
      "epoch": 2.2095803578592634,
      "grad_norm": 0.5870298147201538,
      "learning_rate": 3.422514214537116e-06,
      "loss": 1.3131,
      "step": 13770
    },
    {
      "epoch": 2.2111851079194413,
      "grad_norm": 0.5699328184127808,
      "learning_rate": 3.40947332415581e-06,
      "loss": 1.3154,
      "step": 13780
    },
    {
      "epoch": 2.2127898579796197,
      "grad_norm": 0.8536867499351501,
      "learning_rate": 3.3964522184229766e-06,
      "loss": 1.438,
      "step": 13790
    },
    {
      "epoch": 2.2143946080397976,
      "grad_norm": 0.8035925030708313,
      "learning_rate": 3.3834509364277455e-06,
      "loss": 1.3193,
      "step": 13800
    },
    {
      "epoch": 2.215999358099976,
      "grad_norm": 0.6655374765396118,
      "learning_rate": 3.3704695171997516e-06,
      "loss": 1.4129,
      "step": 13810
    },
    {
      "epoch": 2.217604108160154,
      "grad_norm": 0.45594489574432373,
      "learning_rate": 3.3575079997089843e-06,
      "loss": 1.1962,
      "step": 13820
    },
    {
      "epoch": 2.219208858220332,
      "grad_norm": 0.5604379177093506,
      "learning_rate": 3.344566422865707e-06,
      "loss": 1.2303,
      "step": 13830
    },
    {
      "epoch": 2.22081360828051,
      "grad_norm": 0.6318086981773376,
      "learning_rate": 3.331644825520304e-06,
      "loss": 1.1752,
      "step": 13840
    },
    {
      "epoch": 2.2224183583406885,
      "grad_norm": 0.35039618611335754,
      "learning_rate": 3.318743246463196e-06,
      "loss": 1.1881,
      "step": 13850
    },
    {
      "epoch": 2.2240231084008664,
      "grad_norm": 0.5512954592704773,
      "learning_rate": 3.305861724424698e-06,
      "loss": 1.3647,
      "step": 13860
    },
    {
      "epoch": 2.2256278584610447,
      "grad_norm": 0.47758758068084717,
      "learning_rate": 3.2930002980749244e-06,
      "loss": 1.3019,
      "step": 13870
    },
    {
      "epoch": 2.2272326085212226,
      "grad_norm": 0.5650530457496643,
      "learning_rate": 3.2801590060236544e-06,
      "loss": 1.3734,
      "step": 13880
    },
    {
      "epoch": 2.228837358581401,
      "grad_norm": 0.5604401230812073,
      "learning_rate": 3.267337886820223e-06,
      "loss": 1.2632,
      "step": 13890
    },
    {
      "epoch": 2.230442108641579,
      "grad_norm": 0.698774516582489,
      "learning_rate": 3.25453697895342e-06,
      "loss": 1.2819,
      "step": 13900
    },
    {
      "epoch": 2.2320468587017572,
      "grad_norm": 0.5378434658050537,
      "learning_rate": 3.2417563208513426e-06,
      "loss": 1.353,
      "step": 13910
    },
    {
      "epoch": 2.233651608761935,
      "grad_norm": 0.380614697933197,
      "learning_rate": 3.2289959508813173e-06,
      "loss": 1.259,
      "step": 13920
    },
    {
      "epoch": 2.2352563588221135,
      "grad_norm": 0.5600513815879822,
      "learning_rate": 3.216255907349748e-06,
      "loss": 1.3513,
      "step": 13930
    },
    {
      "epoch": 2.2368611088822914,
      "grad_norm": 0.4704553186893463,
      "learning_rate": 3.2035362285020356e-06,
      "loss": 1.3132,
      "step": 13940
    },
    {
      "epoch": 2.2384658589424697,
      "grad_norm": 0.6261813640594482,
      "learning_rate": 3.1908369525224315e-06,
      "loss": 1.4413,
      "step": 13950
    },
    {
      "epoch": 2.2400706090026477,
      "grad_norm": 0.411559134721756,
      "learning_rate": 3.1781581175339517e-06,
      "loss": 1.3464,
      "step": 13960
    },
    {
      "epoch": 2.241675359062826,
      "grad_norm": 0.6032297015190125,
      "learning_rate": 3.165499761598235e-06,
      "loss": 1.3552,
      "step": 13970
    },
    {
      "epoch": 2.243280109123004,
      "grad_norm": 0.5868331789970398,
      "learning_rate": 3.1528619227154587e-06,
      "loss": 1.2444,
      "step": 13980
    },
    {
      "epoch": 2.2448848591831823,
      "grad_norm": 0.3576239049434662,
      "learning_rate": 3.14024463882419e-06,
      "loss": 1.3237,
      "step": 13990
    },
    {
      "epoch": 2.24648960924336,
      "grad_norm": 0.5165978074073792,
      "learning_rate": 3.1276479478013066e-06,
      "loss": 1.2167,
      "step": 14000
    },
    {
      "epoch": 2.2480943593035385,
      "grad_norm": 0.49373432993888855,
      "learning_rate": 3.1150718874618547e-06,
      "loss": 1.3541,
      "step": 14010
    },
    {
      "epoch": 2.2496991093637164,
      "grad_norm": 0.4283064007759094,
      "learning_rate": 3.102516495558957e-06,
      "loss": 1.1973,
      "step": 14020
    },
    {
      "epoch": 2.2513038594238948,
      "grad_norm": 0.5774633884429932,
      "learning_rate": 3.089981809783681e-06,
      "loss": 1.2885,
      "step": 14030
    },
    {
      "epoch": 2.2529086094840727,
      "grad_norm": 0.544506311416626,
      "learning_rate": 3.0774678677649438e-06,
      "loss": 1.2837,
      "step": 14040
    },
    {
      "epoch": 2.254513359544251,
      "grad_norm": 0.6037463545799255,
      "learning_rate": 3.06497470706938e-06,
      "loss": 1.3072,
      "step": 14050
    },
    {
      "epoch": 2.256118109604429,
      "grad_norm": 0.42027297616004944,
      "learning_rate": 3.052502365201251e-06,
      "loss": 1.3677,
      "step": 14060
    },
    {
      "epoch": 2.2577228596646073,
      "grad_norm": 0.6839047074317932,
      "learning_rate": 3.0400508796023075e-06,
      "loss": 1.3773,
      "step": 14070
    },
    {
      "epoch": 2.259327609724785,
      "grad_norm": 0.3882099390029907,
      "learning_rate": 3.0276202876517025e-06,
      "loss": 1.2038,
      "step": 14080
    },
    {
      "epoch": 2.2609323597849635,
      "grad_norm": 0.81441730260849,
      "learning_rate": 3.0152106266658553e-06,
      "loss": 1.4317,
      "step": 14090
    },
    {
      "epoch": 2.2625371098451414,
      "grad_norm": 0.5561894774436951,
      "learning_rate": 3.002821933898361e-06,
      "loss": 1.4148,
      "step": 14100
    },
    {
      "epoch": 2.26414185990532,
      "grad_norm": 0.4996592700481415,
      "learning_rate": 2.990454246539859e-06,
      "loss": 1.2916,
      "step": 14110
    },
    {
      "epoch": 2.2657466099654977,
      "grad_norm": 0.544721245765686,
      "learning_rate": 2.97810760171794e-06,
      "loss": 1.2098,
      "step": 14120
    },
    {
      "epoch": 2.267351360025676,
      "grad_norm": 0.3909493684768677,
      "learning_rate": 2.9657820364970157e-06,
      "loss": 1.0895,
      "step": 14130
    },
    {
      "epoch": 2.268956110085854,
      "grad_norm": 0.5246338248252869,
      "learning_rate": 2.9534775878782274e-06,
      "loss": 1.4194,
      "step": 14140
    },
    {
      "epoch": 2.2705608601460323,
      "grad_norm": 0.5815643668174744,
      "learning_rate": 2.9411942927993153e-06,
      "loss": 1.3923,
      "step": 14150
    },
    {
      "epoch": 2.27216561020621,
      "grad_norm": 0.6334454417228699,
      "learning_rate": 2.9289321881345257e-06,
      "loss": 1.3095,
      "step": 14160
    },
    {
      "epoch": 2.2737703602663886,
      "grad_norm": 0.47091707587242126,
      "learning_rate": 2.916691310694485e-06,
      "loss": 1.2087,
      "step": 14170
    },
    {
      "epoch": 2.2753751103265665,
      "grad_norm": 0.49110472202301025,
      "learning_rate": 2.9044716972260943e-06,
      "loss": 1.2549,
      "step": 14180
    },
    {
      "epoch": 2.276979860386745,
      "grad_norm": 0.48349830508232117,
      "learning_rate": 2.8922733844124316e-06,
      "loss": 1.3233,
      "step": 14190
    },
    {
      "epoch": 2.2785846104469227,
      "grad_norm": 0.37177568674087524,
      "learning_rate": 2.880096408872618e-06,
      "loss": 1.3262,
      "step": 14200
    },
    {
      "epoch": 2.280189360507101,
      "grad_norm": 0.333139568567276,
      "learning_rate": 2.86794080716173e-06,
      "loss": 1.4449,
      "step": 14210
    },
    {
      "epoch": 2.281794110567279,
      "grad_norm": 0.7307138442993164,
      "learning_rate": 2.855806615770671e-06,
      "loss": 1.3409,
      "step": 14220
    },
    {
      "epoch": 2.2833988606274573,
      "grad_norm": 0.6091773509979248,
      "learning_rate": 2.8436938711260833e-06,
      "loss": 1.2336,
      "step": 14230
    },
    {
      "epoch": 2.2850036106876352,
      "grad_norm": 0.634221613407135,
      "learning_rate": 2.8316026095902127e-06,
      "loss": 1.1818,
      "step": 14240
    },
    {
      "epoch": 2.2866083607478136,
      "grad_norm": 0.5354887843132019,
      "learning_rate": 2.8195328674608246e-06,
      "loss": 1.267,
      "step": 14250
    },
    {
      "epoch": 2.2882131108079915,
      "grad_norm": 0.38295719027519226,
      "learning_rate": 2.8074846809710743e-06,
      "loss": 1.5646,
      "step": 14260
    },
    {
      "epoch": 2.28981786086817,
      "grad_norm": 0.675325334072113,
      "learning_rate": 2.7954580862894176e-06,
      "loss": 1.3377,
      "step": 14270
    },
    {
      "epoch": 2.2914226109283478,
      "grad_norm": 0.41120290756225586,
      "learning_rate": 2.7834531195194794e-06,
      "loss": 1.3036,
      "step": 14280
    },
    {
      "epoch": 2.293027360988526,
      "grad_norm": 0.6537375450134277,
      "learning_rate": 2.771469816699971e-06,
      "loss": 1.3383,
      "step": 14290
    },
    {
      "epoch": 2.294632111048704,
      "grad_norm": 0.5140973925590515,
      "learning_rate": 2.759508213804556e-06,
      "loss": 1.2447,
      "step": 14300
    },
    {
      "epoch": 2.2962368611088824,
      "grad_norm": 0.42593106627464294,
      "learning_rate": 2.7475683467417704e-06,
      "loss": 1.2753,
      "step": 14310
    },
    {
      "epoch": 2.2978416111690603,
      "grad_norm": 0.5057690143585205,
      "learning_rate": 2.7356502513548822e-06,
      "loss": 1.1442,
      "step": 14320
    },
    {
      "epoch": 2.2994463612292386,
      "grad_norm": 0.5707030296325684,
      "learning_rate": 2.723753963421818e-06,
      "loss": 1.3557,
      "step": 14330
    },
    {
      "epoch": 2.3010511112894165,
      "grad_norm": 0.38375335931777954,
      "learning_rate": 2.711879518655024e-06,
      "loss": 1.2436,
      "step": 14340
    },
    {
      "epoch": 2.302655861349595,
      "grad_norm": 0.3879392147064209,
      "learning_rate": 2.700026952701387e-06,
      "loss": 1.3398,
      "step": 14350
    },
    {
      "epoch": 2.304260611409773,
      "grad_norm": 0.660634458065033,
      "learning_rate": 2.6881963011421008e-06,
      "loss": 1.247,
      "step": 14360
    },
    {
      "epoch": 2.305865361469951,
      "grad_norm": 0.4879104495048523,
      "learning_rate": 2.676387599492587e-06,
      "loss": 1.361,
      "step": 14370
    },
    {
      "epoch": 2.307470111530129,
      "grad_norm": 0.5108522176742554,
      "learning_rate": 2.6646008832023572e-06,
      "loss": 1.2146,
      "step": 14380
    },
    {
      "epoch": 2.3090748615903074,
      "grad_norm": 0.8241807818412781,
      "learning_rate": 2.6528361876549415e-06,
      "loss": 1.3504,
      "step": 14390
    },
    {
      "epoch": 2.3106796116504853,
      "grad_norm": 0.4782828390598297,
      "learning_rate": 2.641093548167746e-06,
      "loss": 1.2695,
      "step": 14400
    },
    {
      "epoch": 2.3122843617106636,
      "grad_norm": 0.4414893686771393,
      "learning_rate": 2.6293729999919816e-06,
      "loss": 1.3205,
      "step": 14410
    },
    {
      "epoch": 2.3138891117708416,
      "grad_norm": 0.6318203806877136,
      "learning_rate": 2.617674578312528e-06,
      "loss": 1.3818,
      "step": 14420
    },
    {
      "epoch": 2.31549386183102,
      "grad_norm": 0.4954238831996918,
      "learning_rate": 2.605998318247851e-06,
      "loss": 1.3198,
      "step": 14430
    },
    {
      "epoch": 2.317098611891198,
      "grad_norm": 0.635686993598938,
      "learning_rate": 2.5943442548498808e-06,
      "loss": 1.176,
      "step": 14440
    },
    {
      "epoch": 2.318703361951376,
      "grad_norm": 0.4490485191345215,
      "learning_rate": 2.58271242310392e-06,
      "loss": 1.3372,
      "step": 14450
    },
    {
      "epoch": 2.320308112011554,
      "grad_norm": 0.903411328792572,
      "learning_rate": 2.5711028579285268e-06,
      "loss": 1.1194,
      "step": 14460
    },
    {
      "epoch": 2.3219128620717324,
      "grad_norm": 0.4838894307613373,
      "learning_rate": 2.5595155941754144e-06,
      "loss": 1.2128,
      "step": 14470
    },
    {
      "epoch": 2.3235176121319103,
      "grad_norm": 0.5925813317298889,
      "learning_rate": 2.5479506666293573e-06,
      "loss": 1.2161,
      "step": 14480
    },
    {
      "epoch": 2.3251223621920887,
      "grad_norm": 0.4229896366596222,
      "learning_rate": 2.536408110008064e-06,
      "loss": 1.2084,
      "step": 14490
    },
    {
      "epoch": 2.3267271122522666,
      "grad_norm": 0.6386305093765259,
      "learning_rate": 2.5248879589620976e-06,
      "loss": 1.1564,
      "step": 14500
    },
    {
      "epoch": 2.328331862312445,
      "grad_norm": 0.5466127395629883,
      "learning_rate": 2.5133902480747487e-06,
      "loss": 1.1942,
      "step": 14510
    },
    {
      "epoch": 2.329936612372623,
      "grad_norm": 0.6447052955627441,
      "learning_rate": 2.501915011861955e-06,
      "loss": 1.212,
      "step": 14520
    },
    {
      "epoch": 2.331541362432801,
      "grad_norm": 0.3669250011444092,
      "learning_rate": 2.490462284772175e-06,
      "loss": 1.1854,
      "step": 14530
    },
    {
      "epoch": 2.333146112492979,
      "grad_norm": 0.4530593454837799,
      "learning_rate": 2.479032101186304e-06,
      "loss": 1.3826,
      "step": 14540
    },
    {
      "epoch": 2.3347508625531574,
      "grad_norm": 0.5633776783943176,
      "learning_rate": 2.467624495417552e-06,
      "loss": 1.2491,
      "step": 14550
    },
    {
      "epoch": 2.3363556126133354,
      "grad_norm": 0.40445491671562195,
      "learning_rate": 2.456239501711364e-06,
      "loss": 1.3859,
      "step": 14560
    },
    {
      "epoch": 2.3379603626735137,
      "grad_norm": 0.3835849463939667,
      "learning_rate": 2.444877154245289e-06,
      "loss": 1.2172,
      "step": 14570
    },
    {
      "epoch": 2.3395651127336916,
      "grad_norm": 0.8109063506126404,
      "learning_rate": 2.433537487128905e-06,
      "loss": 1.1906,
      "step": 14580
    },
    {
      "epoch": 2.34116986279387,
      "grad_norm": 0.586273729801178,
      "learning_rate": 2.4222205344036964e-06,
      "loss": 1.1926,
      "step": 14590
    },
    {
      "epoch": 2.342774612854048,
      "grad_norm": 0.5107893943786621,
      "learning_rate": 2.4109263300429654e-06,
      "loss": 1.2771,
      "step": 14600
    },
    {
      "epoch": 2.344379362914226,
      "grad_norm": 0.5094608068466187,
      "learning_rate": 2.399654907951715e-06,
      "loss": 1.4098,
      "step": 14610
    },
    {
      "epoch": 2.345984112974404,
      "grad_norm": 0.42225906252861023,
      "learning_rate": 2.388406301966568e-06,
      "loss": 1.303,
      "step": 14620
    },
    {
      "epoch": 2.3475888630345825,
      "grad_norm": 0.6391902565956116,
      "learning_rate": 2.377180545855641e-06,
      "loss": 1.4154,
      "step": 14630
    },
    {
      "epoch": 2.3491936130947604,
      "grad_norm": 0.4718957245349884,
      "learning_rate": 2.365977673318468e-06,
      "loss": 1.2423,
      "step": 14640
    },
    {
      "epoch": 2.3507983631549387,
      "grad_norm": 0.38522425293922424,
      "learning_rate": 2.354797717985875e-06,
      "loss": 1.3349,
      "step": 14650
    },
    {
      "epoch": 2.3524031132151166,
      "grad_norm": 0.3431982398033142,
      "learning_rate": 2.3436407134199035e-06,
      "loss": 1.1402,
      "step": 14660
    },
    {
      "epoch": 2.354007863275295,
      "grad_norm": 0.38244545459747314,
      "learning_rate": 2.3325066931136843e-06,
      "loss": 1.2954,
      "step": 14670
    },
    {
      "epoch": 2.355612613335473,
      "grad_norm": 0.6788738369941711,
      "learning_rate": 2.3213956904913606e-06,
      "loss": 1.3754,
      "step": 14680
    },
    {
      "epoch": 2.3572173633956512,
      "grad_norm": 0.4722816050052643,
      "learning_rate": 2.3103077389079685e-06,
      "loss": 1.2875,
      "step": 14690
    },
    {
      "epoch": 2.358822113455829,
      "grad_norm": 0.383760929107666,
      "learning_rate": 2.2992428716493542e-06,
      "loss": 1.223,
      "step": 14700
    },
    {
      "epoch": 2.3604268635160075,
      "grad_norm": 0.5624072551727295,
      "learning_rate": 2.2882011219320544e-06,
      "loss": 1.2917,
      "step": 14710
    },
    {
      "epoch": 2.3620316135761854,
      "grad_norm": 0.5791492462158203,
      "learning_rate": 2.2771825229032195e-06,
      "loss": 1.3602,
      "step": 14720
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.5393161177635193,
      "learning_rate": 2.2661871076404885e-06,
      "loss": 1.2983,
      "step": 14730
    },
    {
      "epoch": 2.3652411136965417,
      "grad_norm": 0.38336437940597534,
      "learning_rate": 2.255214909151916e-06,
      "loss": 1.3439,
      "step": 14740
    },
    {
      "epoch": 2.36684586375672,
      "grad_norm": 0.5661198496818542,
      "learning_rate": 2.2442659603758497e-06,
      "loss": 1.2367,
      "step": 14750
    },
    {
      "epoch": 2.368450613816898,
      "grad_norm": 0.5630307793617249,
      "learning_rate": 2.2333402941808522e-06,
      "loss": 1.1924,
      "step": 14760
    },
    {
      "epoch": 2.3700553638770763,
      "grad_norm": 0.6797255873680115,
      "learning_rate": 2.222437943365581e-06,
      "loss": 1.3039,
      "step": 14770
    },
    {
      "epoch": 2.371660113937254,
      "grad_norm": 0.42470839619636536,
      "learning_rate": 2.2115589406587124e-06,
      "loss": 1.2279,
      "step": 14780
    },
    {
      "epoch": 2.3732648639974325,
      "grad_norm": 0.4334326982498169,
      "learning_rate": 2.200703318718821e-06,
      "loss": 1.306,
      "step": 14790
    },
    {
      "epoch": 2.3748696140576104,
      "grad_norm": 0.7738263607025146,
      "learning_rate": 2.189871110134303e-06,
      "loss": 1.2633,
      "step": 14800
    },
    {
      "epoch": 2.376474364117789,
      "grad_norm": 0.5106188654899597,
      "learning_rate": 2.1790623474232597e-06,
      "loss": 1.4085,
      "step": 14810
    },
    {
      "epoch": 2.3780791141779667,
      "grad_norm": 0.5717160701751709,
      "learning_rate": 2.168277063033417e-06,
      "loss": 1.2729,
      "step": 14820
    },
    {
      "epoch": 2.379683864238145,
      "grad_norm": 0.33011385798454285,
      "learning_rate": 2.157515289342008e-06,
      "loss": 1.1913,
      "step": 14830
    },
    {
      "epoch": 2.381288614298323,
      "grad_norm": 0.6405021548271179,
      "learning_rate": 2.1467770586557e-06,
      "loss": 1.3006,
      "step": 14840
    },
    {
      "epoch": 2.3828933643585013,
      "grad_norm": 0.535675585269928,
      "learning_rate": 2.1360624032104714e-06,
      "loss": 1.4296,
      "step": 14850
    },
    {
      "epoch": 2.384498114418679,
      "grad_norm": 0.35112643241882324,
      "learning_rate": 2.1253713551715394e-06,
      "loss": 1.281,
      "step": 14860
    },
    {
      "epoch": 2.3861028644788576,
      "grad_norm": 0.35354098677635193,
      "learning_rate": 2.1147039466332486e-06,
      "loss": 1.2535,
      "step": 14870
    },
    {
      "epoch": 2.3877076145390355,
      "grad_norm": 0.4229545295238495,
      "learning_rate": 2.104060209618972e-06,
      "loss": 1.202,
      "step": 14880
    },
    {
      "epoch": 2.389312364599214,
      "grad_norm": 0.4164540767669678,
      "learning_rate": 2.093440176081032e-06,
      "loss": 1.2938,
      "step": 14890
    },
    {
      "epoch": 2.3909171146593917,
      "grad_norm": 0.47207748889923096,
      "learning_rate": 2.0828438779005824e-06,
      "loss": 1.2999,
      "step": 14900
    },
    {
      "epoch": 2.39252186471957,
      "grad_norm": 0.5616251230239868,
      "learning_rate": 2.0722713468875343e-06,
      "loss": 1.2135,
      "step": 14910
    },
    {
      "epoch": 2.394126614779748,
      "grad_norm": 0.3102204501628876,
      "learning_rate": 2.0617226147804402e-06,
      "loss": 1.1715,
      "step": 14920
    },
    {
      "epoch": 2.3957313648399263,
      "grad_norm": 0.4858132302761078,
      "learning_rate": 2.051197713246419e-06,
      "loss": 1.2604,
      "step": 14930
    },
    {
      "epoch": 2.3973361149001042,
      "grad_norm": 0.4937502145767212,
      "learning_rate": 2.0406966738810395e-06,
      "loss": 1.1974,
      "step": 14940
    },
    {
      "epoch": 2.3989408649602826,
      "grad_norm": 0.5450496077537537,
      "learning_rate": 2.030219528208249e-06,
      "loss": 1.1817,
      "step": 14950
    },
    {
      "epoch": 2.4005456150204605,
      "grad_norm": 0.419904500246048,
      "learning_rate": 2.019766307680252e-06,
      "loss": 1.2655,
      "step": 14960
    },
    {
      "epoch": 2.402150365080639,
      "grad_norm": 0.4135594367980957,
      "learning_rate": 2.009337043677446e-06,
      "loss": 1.2941,
      "step": 14970
    },
    {
      "epoch": 2.4037551151408167,
      "grad_norm": 0.4631187617778778,
      "learning_rate": 1.9989317675082966e-06,
      "loss": 1.3059,
      "step": 14980
    },
    {
      "epoch": 2.405359865200995,
      "grad_norm": 0.5548889636993408,
      "learning_rate": 1.9885505104092695e-06,
      "loss": 1.3198,
      "step": 14990
    },
    {
      "epoch": 2.406964615261173,
      "grad_norm": 0.46484529972076416,
      "learning_rate": 1.9781933035447175e-06,
      "loss": 1.226,
      "step": 15000
    },
    {
      "epoch": 2.4085693653213514,
      "grad_norm": 0.4929300844669342,
      "learning_rate": 1.9678601780068042e-06,
      "loss": 1.4965,
      "step": 15010
    },
    {
      "epoch": 2.4101741153815293,
      "grad_norm": 0.5419467091560364,
      "learning_rate": 1.957551164815391e-06,
      "loss": 1.3277,
      "step": 15020
    },
    {
      "epoch": 2.4117788654417076,
      "grad_norm": 0.6285378932952881,
      "learning_rate": 1.9472662949179587e-06,
      "loss": 1.3971,
      "step": 15030
    },
    {
      "epoch": 2.4133836155018855,
      "grad_norm": 0.6118213534355164,
      "learning_rate": 1.9370055991895155e-06,
      "loss": 1.4201,
      "step": 15040
    },
    {
      "epoch": 2.414988365562064,
      "grad_norm": 0.471525102853775,
      "learning_rate": 1.926769108432489e-06,
      "loss": 1.2285,
      "step": 15050
    },
    {
      "epoch": 2.4165931156222418,
      "grad_norm": 0.6793577075004578,
      "learning_rate": 1.9165568533766566e-06,
      "loss": 1.2376,
      "step": 15060
    },
    {
      "epoch": 2.41819786568242,
      "grad_norm": 0.5598427653312683,
      "learning_rate": 1.9063688646790257e-06,
      "loss": 1.1831,
      "step": 15070
    },
    {
      "epoch": 2.419802615742598,
      "grad_norm": 0.5809389352798462,
      "learning_rate": 1.8962051729237719e-06,
      "loss": 1.328,
      "step": 15080
    },
    {
      "epoch": 2.4214073658027764,
      "grad_norm": 0.34231680631637573,
      "learning_rate": 1.8860658086221161e-06,
      "loss": 1.3291,
      "step": 15090
    },
    {
      "epoch": 2.4230121158629543,
      "grad_norm": 0.42632970213890076,
      "learning_rate": 1.8759508022122651e-06,
      "loss": 1.2755,
      "step": 15100
    },
    {
      "epoch": 2.4246168659231326,
      "grad_norm": 0.530934751033783,
      "learning_rate": 1.8658601840592883e-06,
      "loss": 1.2864,
      "step": 15110
    },
    {
      "epoch": 2.4262216159833105,
      "grad_norm": 0.6386594176292419,
      "learning_rate": 1.8557939844550543e-06,
      "loss": 1.4341,
      "step": 15120
    },
    {
      "epoch": 2.427826366043489,
      "grad_norm": 0.4663744568824768,
      "learning_rate": 1.8457522336181177e-06,
      "loss": 1.4272,
      "step": 15130
    },
    {
      "epoch": 2.429431116103667,
      "grad_norm": 0.6162775754928589,
      "learning_rate": 1.8357349616936481e-06,
      "loss": 1.2434,
      "step": 15140
    },
    {
      "epoch": 2.431035866163845,
      "grad_norm": 0.5784979462623596,
      "learning_rate": 1.8257421987533186e-06,
      "loss": 1.2386,
      "step": 15150
    },
    {
      "epoch": 2.432640616224023,
      "grad_norm": 0.7734998464584351,
      "learning_rate": 1.8157739747952384e-06,
      "loss": 1.3192,
      "step": 15160
    },
    {
      "epoch": 2.4342453662842014,
      "grad_norm": 0.4828326404094696,
      "learning_rate": 1.8058303197438399e-06,
      "loss": 1.353,
      "step": 15170
    },
    {
      "epoch": 2.4358501163443793,
      "grad_norm": 0.6376044750213623,
      "learning_rate": 1.795911263449812e-06,
      "loss": 1.3348,
      "step": 15180
    },
    {
      "epoch": 2.4374548664045577,
      "grad_norm": 0.4823874235153198,
      "learning_rate": 1.7860168356899866e-06,
      "loss": 1.401,
      "step": 15190
    },
    {
      "epoch": 2.4390596164647356,
      "grad_norm": 0.8710609078407288,
      "learning_rate": 1.776147066167272e-06,
      "loss": 1.352,
      "step": 15200
    },
    {
      "epoch": 2.440664366524914,
      "grad_norm": 0.5925241708755493,
      "learning_rate": 1.7663019845105412e-06,
      "loss": 1.2362,
      "step": 15210
    },
    {
      "epoch": 2.442269116585092,
      "grad_norm": 0.6092743277549744,
      "learning_rate": 1.7564816202745683e-06,
      "loss": 1.3967,
      "step": 15220
    },
    {
      "epoch": 2.44387386664527,
      "grad_norm": 0.6067444682121277,
      "learning_rate": 1.7466860029399124e-06,
      "loss": 1.1843,
      "step": 15230
    },
    {
      "epoch": 2.445478616705448,
      "grad_norm": 0.4880290627479553,
      "learning_rate": 1.7369151619128543e-06,
      "loss": 1.1634,
      "step": 15240
    },
    {
      "epoch": 2.4470833667656264,
      "grad_norm": 0.4888927936553955,
      "learning_rate": 1.7271691265252877e-06,
      "loss": 1.2483,
      "step": 15250
    },
    {
      "epoch": 2.4486881168258043,
      "grad_norm": 0.44271528720855713,
      "learning_rate": 1.7174479260346488e-06,
      "loss": 1.3217,
      "step": 15260
    },
    {
      "epoch": 2.4502928668859827,
      "grad_norm": 0.45482829213142395,
      "learning_rate": 1.7077515896238106e-06,
      "loss": 1.3689,
      "step": 15270
    },
    {
      "epoch": 2.4518976169461606,
      "grad_norm": 0.8505709171295166,
      "learning_rate": 1.6980801464010143e-06,
      "loss": 1.3277,
      "step": 15280
    },
    {
      "epoch": 2.453502367006339,
      "grad_norm": 0.40530335903167725,
      "learning_rate": 1.6884336253997658e-06,
      "loss": 1.33,
      "step": 15290
    },
    {
      "epoch": 2.455107117066517,
      "grad_norm": 0.4914875030517578,
      "learning_rate": 1.6788120555787513e-06,
      "loss": 1.3759,
      "step": 15300
    },
    {
      "epoch": 2.456711867126695,
      "grad_norm": 0.8713119029998779,
      "learning_rate": 1.6692154658217652e-06,
      "loss": 1.3325,
      "step": 15310
    },
    {
      "epoch": 2.458316617186873,
      "grad_norm": 0.37501129508018494,
      "learning_rate": 1.6596438849376019e-06,
      "loss": 1.2761,
      "step": 15320
    },
    {
      "epoch": 2.4599213672470515,
      "grad_norm": 0.4853300154209137,
      "learning_rate": 1.6500973416599874e-06,
      "loss": 1.313,
      "step": 15330
    },
    {
      "epoch": 2.4615261173072294,
      "grad_norm": 0.40892571210861206,
      "learning_rate": 1.6405758646474768e-06,
      "loss": 1.2118,
      "step": 15340
    },
    {
      "epoch": 2.4631308673674077,
      "grad_norm": 0.44508445262908936,
      "learning_rate": 1.6310794824833876e-06,
      "loss": 1.163,
      "step": 15350
    },
    {
      "epoch": 2.4647356174275856,
      "grad_norm": 0.5100158452987671,
      "learning_rate": 1.6216082236756891e-06,
      "loss": 1.2478,
      "step": 15360
    },
    {
      "epoch": 2.466340367487764,
      "grad_norm": 0.49621161818504333,
      "learning_rate": 1.6121621166569467e-06,
      "loss": 1.1812,
      "step": 15370
    },
    {
      "epoch": 2.467945117547942,
      "grad_norm": 0.5045111179351807,
      "learning_rate": 1.6027411897842048e-06,
      "loss": 1.4559,
      "step": 15380
    },
    {
      "epoch": 2.4695498676081202,
      "grad_norm": 0.7001656293869019,
      "learning_rate": 1.5933454713389306e-06,
      "loss": 1.2342,
      "step": 15390
    },
    {
      "epoch": 2.471154617668298,
      "grad_norm": 0.6658633351325989,
      "learning_rate": 1.5839749895269064e-06,
      "loss": 1.2276,
      "step": 15400
    },
    {
      "epoch": 2.472759367728476,
      "grad_norm": 0.3834189474582672,
      "learning_rate": 1.5746297724781623e-06,
      "loss": 1.261,
      "step": 15410
    },
    {
      "epoch": 2.4743641177886544,
      "grad_norm": 0.6650444865226746,
      "learning_rate": 1.5653098482468754e-06,
      "loss": 1.3715,
      "step": 15420
    },
    {
      "epoch": 2.4759688678488327,
      "grad_norm": 0.30963820219039917,
      "learning_rate": 1.556015244811303e-06,
      "loss": 1.212,
      "step": 15430
    },
    {
      "epoch": 2.4775736179090107,
      "grad_norm": 0.5046997666358948,
      "learning_rate": 1.5467459900736825e-06,
      "loss": 1.3783,
      "step": 15440
    },
    {
      "epoch": 2.4791783679691886,
      "grad_norm": 0.768170952796936,
      "learning_rate": 1.537502111860163e-06,
      "loss": 1.4686,
      "step": 15450
    },
    {
      "epoch": 2.480783118029367,
      "grad_norm": 0.3907994329929352,
      "learning_rate": 1.5282836379207033e-06,
      "loss": 1.4254,
      "step": 15460
    },
    {
      "epoch": 2.4823878680895453,
      "grad_norm": 0.603118896484375,
      "learning_rate": 1.5190905959290104e-06,
      "loss": 1.3255,
      "step": 15470
    },
    {
      "epoch": 2.483992618149723,
      "grad_norm": 0.4081866145133972,
      "learning_rate": 1.5099230134824328e-06,
      "loss": 1.1648,
      "step": 15480
    },
    {
      "epoch": 2.485597368209901,
      "grad_norm": 0.5594964623451233,
      "learning_rate": 1.5007809181019028e-06,
      "loss": 1.3729,
      "step": 15490
    },
    {
      "epoch": 2.4872021182700794,
      "grad_norm": 0.47680455446243286,
      "learning_rate": 1.4916643372318295e-06,
      "loss": 1.3953,
      "step": 15500
    },
    {
      "epoch": 2.4888068683302578,
      "grad_norm": 0.6132343411445618,
      "learning_rate": 1.4825732982400376e-06,
      "loss": 1.3168,
      "step": 15510
    },
    {
      "epoch": 2.4904116183904357,
      "grad_norm": 0.4447488784790039,
      "learning_rate": 1.4735078284176651e-06,
      "loss": 1.3231,
      "step": 15520
    },
    {
      "epoch": 2.4920163684506136,
      "grad_norm": 0.49294358491897583,
      "learning_rate": 1.464467954979103e-06,
      "loss": 1.2993,
      "step": 15530
    },
    {
      "epoch": 2.493621118510792,
      "grad_norm": 0.7353647351264954,
      "learning_rate": 1.4554537050618922e-06,
      "loss": 1.3853,
      "step": 15540
    },
    {
      "epoch": 2.4952258685709703,
      "grad_norm": 0.5442992448806763,
      "learning_rate": 1.446465105726661e-06,
      "loss": 1.2835,
      "step": 15550
    },
    {
      "epoch": 2.496830618631148,
      "grad_norm": 0.6823076009750366,
      "learning_rate": 1.437502183957028e-06,
      "loss": 1.2189,
      "step": 15560
    },
    {
      "epoch": 2.498435368691326,
      "grad_norm": 0.5535646080970764,
      "learning_rate": 1.4285649666595337e-06,
      "loss": 1.2009,
      "step": 15570
    },
    {
      "epoch": 2.5000401187515044,
      "grad_norm": 0.6033357977867126,
      "learning_rate": 1.419653480663551e-06,
      "loss": 1.3661,
      "step": 15580
    },
    {
      "epoch": 2.501644868811683,
      "grad_norm": 0.5595495700836182,
      "learning_rate": 1.410767752721205e-06,
      "loss": 1.3066,
      "step": 15590
    },
    {
      "epoch": 2.5032496188718607,
      "grad_norm": 0.4358268976211548,
      "learning_rate": 1.4019078095073057e-06,
      "loss": 1.3131,
      "step": 15600
    },
    {
      "epoch": 2.5048543689320386,
      "grad_norm": 0.5641258955001831,
      "learning_rate": 1.3930736776192444e-06,
      "loss": 1.2874,
      "step": 15610
    },
    {
      "epoch": 2.506459118992217,
      "grad_norm": 0.5525023341178894,
      "learning_rate": 1.3842653835769403e-06,
      "loss": 1.2805,
      "step": 15620
    },
    {
      "epoch": 2.5080638690523953,
      "grad_norm": 0.43664589524269104,
      "learning_rate": 1.3754829538227365e-06,
      "loss": 1.2375,
      "step": 15630
    },
    {
      "epoch": 2.509668619112573,
      "grad_norm": 0.6556921005249023,
      "learning_rate": 1.366726414721341e-06,
      "loss": 1.347,
      "step": 15640
    },
    {
      "epoch": 2.511273369172751,
      "grad_norm": 0.345285564661026,
      "learning_rate": 1.357995792559731e-06,
      "loss": 1.2483,
      "step": 15650
    },
    {
      "epoch": 2.5128781192329295,
      "grad_norm": 0.46077054738998413,
      "learning_rate": 1.3492911135470888e-06,
      "loss": 1.2194,
      "step": 15660
    },
    {
      "epoch": 2.514482869293108,
      "grad_norm": 0.4759752154350281,
      "learning_rate": 1.3406124038147084e-06,
      "loss": 1.4542,
      "step": 15670
    },
    {
      "epoch": 2.5160876193532857,
      "grad_norm": 0.3286915719509125,
      "learning_rate": 1.3319596894159314e-06,
      "loss": 1.3143,
      "step": 15680
    },
    {
      "epoch": 2.5176923694134636,
      "grad_norm": 0.3898930251598358,
      "learning_rate": 1.3233329963260555e-06,
      "loss": 1.2435,
      "step": 15690
    },
    {
      "epoch": 2.519297119473642,
      "grad_norm": 0.7080293297767639,
      "learning_rate": 1.3147323504422693e-06,
      "loss": 1.3481,
      "step": 15700
    },
    {
      "epoch": 2.5209018695338203,
      "grad_norm": 0.4322793781757355,
      "learning_rate": 1.306157777583561e-06,
      "loss": 1.2179,
      "step": 15710
    },
    {
      "epoch": 2.5225066195939982,
      "grad_norm": 0.6701524257659912,
      "learning_rate": 1.2976093034906555e-06,
      "loss": 1.3607,
      "step": 15720
    },
    {
      "epoch": 2.524111369654176,
      "grad_norm": 0.4855916202068329,
      "learning_rate": 1.2890869538259233e-06,
      "loss": 1.1727,
      "step": 15730
    },
    {
      "epoch": 2.5257161197143545,
      "grad_norm": 0.5183941125869751,
      "learning_rate": 1.2805907541733142e-06,
      "loss": 1.2372,
      "step": 15740
    },
    {
      "epoch": 2.527320869774533,
      "grad_norm": 0.7098335027694702,
      "learning_rate": 1.272120730038272e-06,
      "loss": 1.3071,
      "step": 15750
    },
    {
      "epoch": 2.5289256198347108,
      "grad_norm": 0.6844784021377563,
      "learning_rate": 1.2636769068476684e-06,
      "loss": 1.3823,
      "step": 15760
    },
    {
      "epoch": 2.5305303698948887,
      "grad_norm": 0.6642282605171204,
      "learning_rate": 1.2552593099497124e-06,
      "loss": 1.329,
      "step": 15770
    },
    {
      "epoch": 2.532135119955067,
      "grad_norm": 0.42015981674194336,
      "learning_rate": 1.246867964613888e-06,
      "loss": 1.3983,
      "step": 15780
    },
    {
      "epoch": 2.5337398700152454,
      "grad_norm": 0.6219363212585449,
      "learning_rate": 1.23850289603087e-06,
      "loss": 1.3358,
      "step": 15790
    },
    {
      "epoch": 2.5353446200754233,
      "grad_norm": 0.5744714736938477,
      "learning_rate": 1.2301641293124521e-06,
      "loss": 1.3026,
      "step": 15800
    },
    {
      "epoch": 2.536949370135601,
      "grad_norm": 0.32432734966278076,
      "learning_rate": 1.2218516894914679e-06,
      "loss": 1.2851,
      "step": 15810
    },
    {
      "epoch": 2.5385541201957795,
      "grad_norm": 0.5385662317276001,
      "learning_rate": 1.2135656015217223e-06,
      "loss": 1.1019,
      "step": 15820
    },
    {
      "epoch": 2.540158870255958,
      "grad_norm": 0.5463088750839233,
      "learning_rate": 1.2053058902779057e-06,
      "loss": 1.1906,
      "step": 15830
    },
    {
      "epoch": 2.541763620316136,
      "grad_norm": 0.5402642488479614,
      "learning_rate": 1.197072580555535e-06,
      "loss": 1.191,
      "step": 15840
    },
    {
      "epoch": 2.5433683703763137,
      "grad_norm": 0.5967260003089905,
      "learning_rate": 1.1888656970708633e-06,
      "loss": 1.359,
      "step": 15850
    },
    {
      "epoch": 2.544973120436492,
      "grad_norm": 0.6701377630233765,
      "learning_rate": 1.1806852644608114e-06,
      "loss": 1.363,
      "step": 15860
    },
    {
      "epoch": 2.5465778704966704,
      "grad_norm": 0.7168575525283813,
      "learning_rate": 1.1725313072829037e-06,
      "loss": 1.4961,
      "step": 15870
    },
    {
      "epoch": 2.5481826205568483,
      "grad_norm": 0.6761705875396729,
      "learning_rate": 1.1644038500151755e-06,
      "loss": 1.3903,
      "step": 15880
    },
    {
      "epoch": 2.549787370617026,
      "grad_norm": 0.577199399471283,
      "learning_rate": 1.156302917056119e-06,
      "loss": 1.3884,
      "step": 15890
    },
    {
      "epoch": 2.5513921206772046,
      "grad_norm": 0.6135506629943848,
      "learning_rate": 1.1482285327245922e-06,
      "loss": 1.3666,
      "step": 15900
    },
    {
      "epoch": 2.552996870737383,
      "grad_norm": 0.47552675008773804,
      "learning_rate": 1.1401807212597638e-06,
      "loss": 1.3267,
      "step": 15910
    },
    {
      "epoch": 2.554601620797561,
      "grad_norm": 0.7390226125717163,
      "learning_rate": 1.1321595068210212e-06,
      "loss": 1.3089,
      "step": 15920
    },
    {
      "epoch": 2.5562063708577387,
      "grad_norm": 0.4755348265171051,
      "learning_rate": 1.1241649134879162e-06,
      "loss": 1.4036,
      "step": 15930
    },
    {
      "epoch": 2.557811120917917,
      "grad_norm": 0.5030370950698853,
      "learning_rate": 1.11619696526008e-06,
      "loss": 1.2558,
      "step": 15940
    },
    {
      "epoch": 2.5594158709780954,
      "grad_norm": 0.45964768528938293,
      "learning_rate": 1.108255686057159e-06,
      "loss": 1.2743,
      "step": 15950
    },
    {
      "epoch": 2.5610206210382733,
      "grad_norm": 0.5173287987709045,
      "learning_rate": 1.1003410997187348e-06,
      "loss": 1.2959,
      "step": 15960
    },
    {
      "epoch": 2.5626253710984512,
      "grad_norm": 0.5349627137184143,
      "learning_rate": 1.0924532300042634e-06,
      "loss": 1.1594,
      "step": 15970
    },
    {
      "epoch": 2.5642301211586296,
      "grad_norm": 0.4488919973373413,
      "learning_rate": 1.0845921005929916e-06,
      "loss": 1.2048,
      "step": 15980
    },
    {
      "epoch": 2.565834871218808,
      "grad_norm": 0.4221695363521576,
      "learning_rate": 1.0767577350838977e-06,
      "loss": 1.2741,
      "step": 15990
    },
    {
      "epoch": 2.567439621278986,
      "grad_norm": 0.44394010305404663,
      "learning_rate": 1.068950156995614e-06,
      "loss": 1.2599,
      "step": 16000
    },
    {
      "epoch": 2.5690443713391637,
      "grad_norm": 0.3305031955242157,
      "learning_rate": 1.0611693897663534e-06,
      "loss": 1.3201,
      "step": 16010
    },
    {
      "epoch": 2.570649121399342,
      "grad_norm": 0.7008131742477417,
      "learning_rate": 1.0534154567538501e-06,
      "loss": 1.2017,
      "step": 16020
    },
    {
      "epoch": 2.5722538714595204,
      "grad_norm": 0.951565682888031,
      "learning_rate": 1.0456883812352747e-06,
      "loss": 1.3054,
      "step": 16030
    },
    {
      "epoch": 2.5738586215196984,
      "grad_norm": 0.514337956905365,
      "learning_rate": 1.0379881864071795e-06,
      "loss": 1.3821,
      "step": 16040
    },
    {
      "epoch": 2.5754633715798763,
      "grad_norm": 0.41192761063575745,
      "learning_rate": 1.0303148953854136e-06,
      "loss": 1.1914,
      "step": 16050
    },
    {
      "epoch": 2.5770681216400546,
      "grad_norm": 0.42838865518569946,
      "learning_rate": 1.02266853120507e-06,
      "loss": 1.2647,
      "step": 16060
    },
    {
      "epoch": 2.578672871700233,
      "grad_norm": 0.8695184588432312,
      "learning_rate": 1.0150491168204002e-06,
      "loss": 1.2566,
      "step": 16070
    },
    {
      "epoch": 2.580277621760411,
      "grad_norm": 0.8265466094017029,
      "learning_rate": 1.0074566751047576e-06,
      "loss": 1.1567,
      "step": 16080
    },
    {
      "epoch": 2.5818823718205888,
      "grad_norm": 0.6146081686019897,
      "learning_rate": 9.998912288505203e-07,
      "loss": 1.3028,
      "step": 16090
    },
    {
      "epoch": 2.583487121880767,
      "grad_norm": 0.5339607000350952,
      "learning_rate": 9.92352800769033e-07,
      "loss": 1.4615,
      "step": 16100
    },
    {
      "epoch": 2.5850918719409455,
      "grad_norm": 0.5154210329055786,
      "learning_rate": 9.848414134905226e-07,
      "loss": 1.313,
      "step": 16110
    },
    {
      "epoch": 2.5866966220011234,
      "grad_norm": 0.7315402626991272,
      "learning_rate": 9.773570895640494e-07,
      "loss": 1.4497,
      "step": 16120
    },
    {
      "epoch": 2.5883013720613013,
      "grad_norm": 0.31346651911735535,
      "learning_rate": 9.698998514574232e-07,
      "loss": 1.2816,
      "step": 16130
    },
    {
      "epoch": 2.5899061221214796,
      "grad_norm": 0.5805836319923401,
      "learning_rate": 9.62469721557149e-07,
      "loss": 1.2327,
      "step": 16140
    },
    {
      "epoch": 2.5915108721816575,
      "grad_norm": 0.890377938747406,
      "learning_rate": 9.550667221683452e-07,
      "loss": 1.1265,
      "step": 16150
    },
    {
      "epoch": 2.593115622241836,
      "grad_norm": 0.4623885154724121,
      "learning_rate": 9.47690875514694e-07,
      "loss": 1.302,
      "step": 16160
    },
    {
      "epoch": 2.594720372302014,
      "grad_norm": 0.5256097316741943,
      "learning_rate": 9.403422037383559e-07,
      "loss": 1.2479,
      "step": 16170
    },
    {
      "epoch": 2.596325122362192,
      "grad_norm": 0.5161231160163879,
      "learning_rate": 9.330207288999238e-07,
      "loss": 1.3013,
      "step": 16180
    },
    {
      "epoch": 2.59792987242237,
      "grad_norm": 0.41611093282699585,
      "learning_rate": 9.257264729783355e-07,
      "loss": 1.4612,
      "step": 16190
    },
    {
      "epoch": 2.5995346224825484,
      "grad_norm": 0.39373093843460083,
      "learning_rate": 9.184594578708262e-07,
      "loss": 1.2315,
      "step": 16200
    },
    {
      "epoch": 2.6011393725427263,
      "grad_norm": 0.5992099642753601,
      "learning_rate": 9.112197053928462e-07,
      "loss": 1.2164,
      "step": 16210
    },
    {
      "epoch": 2.6027441226029047,
      "grad_norm": 0.5586974024772644,
      "learning_rate": 9.040072372780151e-07,
      "loss": 1.3985,
      "step": 16220
    },
    {
      "epoch": 2.6043488726630826,
      "grad_norm": 0.5361524224281311,
      "learning_rate": 8.968220751780321e-07,
      "loss": 1.188,
      "step": 16230
    },
    {
      "epoch": 2.605953622723261,
      "grad_norm": 0.34888002276420593,
      "learning_rate": 8.896642406626377e-07,
      "loss": 1.3192,
      "step": 16240
    },
    {
      "epoch": 2.607558372783439,
      "grad_norm": 0.36977702379226685,
      "learning_rate": 8.825337552195235e-07,
      "loss": 1.0994,
      "step": 16250
    },
    {
      "epoch": 2.609163122843617,
      "grad_norm": 0.4604651629924774,
      "learning_rate": 8.754306402542889e-07,
      "loss": 1.3286,
      "step": 16260
    },
    {
      "epoch": 2.610767872903795,
      "grad_norm": 0.5458600521087646,
      "learning_rate": 8.683549170903605e-07,
      "loss": 1.3046,
      "step": 16270
    },
    {
      "epoch": 2.6123726229639734,
      "grad_norm": 0.6995880007743835,
      "learning_rate": 8.613066069689402e-07,
      "loss": 1.3108,
      "step": 16280
    },
    {
      "epoch": 2.6139773730241513,
      "grad_norm": 0.37636637687683105,
      "learning_rate": 8.542857310489339e-07,
      "loss": 1.2308,
      "step": 16290
    },
    {
      "epoch": 2.6155821230843297,
      "grad_norm": 0.5002078413963318,
      "learning_rate": 8.472923104068919e-07,
      "loss": 1.2966,
      "step": 16300
    },
    {
      "epoch": 2.6171868731445076,
      "grad_norm": 0.8029959201812744,
      "learning_rate": 8.40326366036941e-07,
      "loss": 1.567,
      "step": 16310
    },
    {
      "epoch": 2.618791623204686,
      "grad_norm": 0.5041281580924988,
      "learning_rate": 8.333879188507321e-07,
      "loss": 1.3635,
      "step": 16320
    },
    {
      "epoch": 2.620396373264864,
      "grad_norm": 0.5105291604995728,
      "learning_rate": 8.264769896773594e-07,
      "loss": 1.2764,
      "step": 16330
    },
    {
      "epoch": 2.622001123325042,
      "grad_norm": 0.4841490685939789,
      "learning_rate": 8.195935992633186e-07,
      "loss": 1.3603,
      "step": 16340
    },
    {
      "epoch": 2.62360587338522,
      "grad_norm": 0.5056232213973999,
      "learning_rate": 8.127377682724269e-07,
      "loss": 1.3672,
      "step": 16350
    },
    {
      "epoch": 2.6252106234453985,
      "grad_norm": 0.620578944683075,
      "learning_rate": 8.059095172857745e-07,
      "loss": 1.2852,
      "step": 16360
    },
    {
      "epoch": 2.6268153735055764,
      "grad_norm": 0.5794731378555298,
      "learning_rate": 7.991088668016511e-07,
      "loss": 1.3593,
      "step": 16370
    },
    {
      "epoch": 2.6284201235657547,
      "grad_norm": 0.46111541986465454,
      "learning_rate": 7.923358372354972e-07,
      "loss": 1.2386,
      "step": 16380
    },
    {
      "epoch": 2.6300248736259326,
      "grad_norm": 0.398937463760376,
      "learning_rate": 7.855904489198285e-07,
      "loss": 1.4356,
      "step": 16390
    },
    {
      "epoch": 2.631629623686111,
      "grad_norm": 0.5041158199310303,
      "learning_rate": 7.78872722104188e-07,
      "loss": 1.2753,
      "step": 16400
    },
    {
      "epoch": 2.633234373746289,
      "grad_norm": 0.35957297682762146,
      "learning_rate": 7.721826769550733e-07,
      "loss": 1.2433,
      "step": 16410
    },
    {
      "epoch": 2.6348391238064672,
      "grad_norm": 0.549613356590271,
      "learning_rate": 7.655203335558902e-07,
      "loss": 1.3032,
      "step": 16420
    },
    {
      "epoch": 2.636443873866645,
      "grad_norm": 0.3925011456012726,
      "learning_rate": 7.588857119068771e-07,
      "loss": 1.3257,
      "step": 16430
    },
    {
      "epoch": 2.6380486239268235,
      "grad_norm": 0.6857022047042847,
      "learning_rate": 7.522788319250529e-07,
      "loss": 1.3992,
      "step": 16440
    },
    {
      "epoch": 2.6396533739870014,
      "grad_norm": 0.6449393033981323,
      "learning_rate": 7.456997134441624e-07,
      "loss": 1.3104,
      "step": 16450
    },
    {
      "epoch": 2.6412581240471797,
      "grad_norm": 0.5980276465415955,
      "learning_rate": 7.391483762146046e-07,
      "loss": 1.2658,
      "step": 16460
    },
    {
      "epoch": 2.6428628741073577,
      "grad_norm": 0.6295572519302368,
      "learning_rate": 7.326248399033831e-07,
      "loss": 1.2244,
      "step": 16470
    },
    {
      "epoch": 2.644467624167536,
      "grad_norm": 0.6289500594139099,
      "learning_rate": 7.261291240940394e-07,
      "loss": 1.2744,
      "step": 16480
    },
    {
      "epoch": 2.646072374227714,
      "grad_norm": 0.389056533575058,
      "learning_rate": 7.196612482866061e-07,
      "loss": 1.2245,
      "step": 16490
    },
    {
      "epoch": 2.6476771242878923,
      "grad_norm": 0.5292389392852783,
      "learning_rate": 7.132212318975318e-07,
      "loss": 1.2585,
      "step": 16500
    },
    {
      "epoch": 2.64928187434807,
      "grad_norm": 0.5558943152427673,
      "learning_rate": 7.068090942596384e-07,
      "loss": 1.1902,
      "step": 16510
    },
    {
      "epoch": 2.6508866244082485,
      "grad_norm": 0.42351219058036804,
      "learning_rate": 7.004248546220515e-07,
      "loss": 1.1533,
      "step": 16520
    },
    {
      "epoch": 2.6524913744684264,
      "grad_norm": 0.4073263108730316,
      "learning_rate": 6.940685321501517e-07,
      "loss": 1.2203,
      "step": 16530
    },
    {
      "epoch": 2.6540961245286048,
      "grad_norm": 0.6106703877449036,
      "learning_rate": 6.877401459255073e-07,
      "loss": 1.4604,
      "step": 16540
    },
    {
      "epoch": 2.6557008745887827,
      "grad_norm": 0.711777925491333,
      "learning_rate": 6.814397149458296e-07,
      "loss": 1.2515,
      "step": 16550
    },
    {
      "epoch": 2.657305624648961,
      "grad_norm": 0.41234248876571655,
      "learning_rate": 6.751672581249002e-07,
      "loss": 1.3142,
      "step": 16560
    },
    {
      "epoch": 2.658910374709139,
      "grad_norm": 0.40448543429374695,
      "learning_rate": 6.689227942925325e-07,
      "loss": 1.2611,
      "step": 16570
    },
    {
      "epoch": 2.6605151247693173,
      "grad_norm": 0.5724660754203796,
      "learning_rate": 6.627063421944946e-07,
      "loss": 1.2874,
      "step": 16580
    },
    {
      "epoch": 2.662119874829495,
      "grad_norm": 0.41053956747055054,
      "learning_rate": 6.565179204924744e-07,
      "loss": 1.1557,
      "step": 16590
    },
    {
      "epoch": 2.6637246248896735,
      "grad_norm": 0.5192573070526123,
      "learning_rate": 6.503575477640034e-07,
      "loss": 1.2283,
      "step": 16600
    },
    {
      "epoch": 2.6653293749498514,
      "grad_norm": 0.3956882953643799,
      "learning_rate": 6.442252425024209e-07,
      "loss": 1.14,
      "step": 16610
    },
    {
      "epoch": 2.66693412501003,
      "grad_norm": 0.8217536807060242,
      "learning_rate": 6.381210231167967e-07,
      "loss": 1.4407,
      "step": 16620
    },
    {
      "epoch": 2.6685388750702077,
      "grad_norm": 0.44519880414009094,
      "learning_rate": 6.32044907931898e-07,
      "loss": 1.2265,
      "step": 16630
    },
    {
      "epoch": 2.670143625130386,
      "grad_norm": 0.4348823130130768,
      "learning_rate": 6.259969151881152e-07,
      "loss": 1.2241,
      "step": 16640
    },
    {
      "epoch": 2.671748375190564,
      "grad_norm": 0.4364720582962036,
      "learning_rate": 6.199770630414215e-07,
      "loss": 1.3003,
      "step": 16650
    },
    {
      "epoch": 2.6733531252507423,
      "grad_norm": 0.35295620560646057,
      "learning_rate": 6.13985369563308e-07,
      "loss": 1.3658,
      "step": 16660
    },
    {
      "epoch": 2.67495787531092,
      "grad_norm": 0.6078487038612366,
      "learning_rate": 6.080218527407389e-07,
      "loss": 1.3114,
      "step": 16670
    },
    {
      "epoch": 2.6765626253710986,
      "grad_norm": 0.4528530538082123,
      "learning_rate": 6.02086530476087e-07,
      "loss": 1.1972,
      "step": 16680
    },
    {
      "epoch": 2.6781673754312765,
      "grad_norm": 0.564997136592865,
      "learning_rate": 5.961794205870908e-07,
      "loss": 1.4351,
      "step": 16690
    },
    {
      "epoch": 2.679772125491455,
      "grad_norm": 0.5090463757514954,
      "learning_rate": 5.903005408067919e-07,
      "loss": 1.3718,
      "step": 16700
    },
    {
      "epoch": 2.6813768755516327,
      "grad_norm": 0.42987388372421265,
      "learning_rate": 5.844499087834865e-07,
      "loss": 1.4218,
      "step": 16710
    },
    {
      "epoch": 2.682981625611811,
      "grad_norm": 0.3823462724685669,
      "learning_rate": 5.78627542080673e-07,
      "loss": 1.2248,
      "step": 16720
    },
    {
      "epoch": 2.684586375671989,
      "grad_norm": 0.39322522282600403,
      "learning_rate": 5.728334581769957e-07,
      "loss": 1.2445,
      "step": 16730
    },
    {
      "epoch": 2.6861911257321673,
      "grad_norm": 0.4649645984172821,
      "learning_rate": 5.670676744661974e-07,
      "loss": 1.2113,
      "step": 16740
    },
    {
      "epoch": 2.6877958757923452,
      "grad_norm": 0.7145809531211853,
      "learning_rate": 5.613302082570604e-07,
      "loss": 1.356,
      "step": 16750
    },
    {
      "epoch": 2.6894006258525236,
      "grad_norm": 0.4320562779903412,
      "learning_rate": 5.556210767733605e-07,
      "loss": 1.2479,
      "step": 16760
    },
    {
      "epoch": 2.6910053759127015,
      "grad_norm": 0.5474115014076233,
      "learning_rate": 5.499402971538126e-07,
      "loss": 1.2418,
      "step": 16770
    },
    {
      "epoch": 2.69261012597288,
      "grad_norm": 0.3371680676937103,
      "learning_rate": 5.442878864520229e-07,
      "loss": 1.346,
      "step": 16780
    },
    {
      "epoch": 2.6942148760330578,
      "grad_norm": 0.3912970721721649,
      "learning_rate": 5.38663861636427e-07,
      "loss": 1.1878,
      "step": 16790
    },
    {
      "epoch": 2.695819626093236,
      "grad_norm": 0.4090435802936554,
      "learning_rate": 5.330682395902553e-07,
      "loss": 1.3191,
      "step": 16800
    },
    {
      "epoch": 2.697424376153414,
      "grad_norm": 0.4155922532081604,
      "learning_rate": 5.275010371114675e-07,
      "loss": 1.1091,
      "step": 16810
    },
    {
      "epoch": 2.6990291262135924,
      "grad_norm": 0.5114448070526123,
      "learning_rate": 5.219622709127114e-07,
      "loss": 1.2995,
      "step": 16820
    },
    {
      "epoch": 2.7006338762737703,
      "grad_norm": 0.4896058738231659,
      "learning_rate": 5.164519576212667e-07,
      "loss": 1.3558,
      "step": 16830
    },
    {
      "epoch": 2.7022386263339486,
      "grad_norm": 0.505361795425415,
      "learning_rate": 5.109701137790024e-07,
      "loss": 1.3431,
      "step": 16840
    },
    {
      "epoch": 2.7038433763941265,
      "grad_norm": 0.8152896165847778,
      "learning_rate": 5.05516755842318e-07,
      "loss": 1.331,
      "step": 16850
    },
    {
      "epoch": 2.705448126454305,
      "grad_norm": 0.8078464269638062,
      "learning_rate": 5.000919001821014e-07,
      "loss": 1.248,
      "step": 16860
    },
    {
      "epoch": 2.707052876514483,
      "grad_norm": 0.5083989500999451,
      "learning_rate": 4.946955630836769e-07,
      "loss": 1.2024,
      "step": 16870
    },
    {
      "epoch": 2.708657626574661,
      "grad_norm": 0.8950571417808533,
      "learning_rate": 4.893277607467561e-07,
      "loss": 1.2276,
      "step": 16880
    },
    {
      "epoch": 2.710262376634839,
      "grad_norm": 0.5400675535202026,
      "learning_rate": 4.839885092853891e-07,
      "loss": 1.1805,
      "step": 16890
    },
    {
      "epoch": 2.7118671266950174,
      "grad_norm": 0.5090773701667786,
      "learning_rate": 4.7867782472792e-07,
      "loss": 1.2382,
      "step": 16900
    },
    {
      "epoch": 2.7134718767551953,
      "grad_norm": 0.9231069087982178,
      "learning_rate": 4.733957230169295e-07,
      "loss": 1.2114,
      "step": 16910
    },
    {
      "epoch": 2.7150766268153737,
      "grad_norm": 0.5339071154594421,
      "learning_rate": 4.681422200092012e-07,
      "loss": 1.3084,
      "step": 16920
    },
    {
      "epoch": 2.7166813768755516,
      "grad_norm": 0.40187475085258484,
      "learning_rate": 4.629173314756563e-07,
      "loss": 1.315,
      "step": 16930
    },
    {
      "epoch": 2.71828612693573,
      "grad_norm": 0.5075650811195374,
      "learning_rate": 4.5772107310132486e-07,
      "loss": 1.1564,
      "step": 16940
    },
    {
      "epoch": 2.719890876995908,
      "grad_norm": 0.4164139926433563,
      "learning_rate": 4.5255346048528213e-07,
      "loss": 1.2998,
      "step": 16950
    },
    {
      "epoch": 2.721495627056086,
      "grad_norm": 0.5580094456672668,
      "learning_rate": 4.474145091406179e-07,
      "loss": 1.4135,
      "step": 16960
    },
    {
      "epoch": 2.723100377116264,
      "grad_norm": 0.33146074414253235,
      "learning_rate": 4.4230423449437066e-07,
      "loss": 1.3347,
      "step": 16970
    },
    {
      "epoch": 2.7247051271764424,
      "grad_norm": 0.6485412120819092,
      "learning_rate": 4.3722265188750225e-07,
      "loss": 1.332,
      "step": 16980
    },
    {
      "epoch": 2.7263098772366203,
      "grad_norm": 0.39256399869918823,
      "learning_rate": 4.321697765748356e-07,
      "loss": 1.4146,
      "step": 16990
    },
    {
      "epoch": 2.7279146272967987,
      "grad_norm": 0.3812972903251648,
      "learning_rate": 4.271456237250127e-07,
      "loss": 1.3392,
      "step": 17000
    },
    {
      "epoch": 2.7295193773569766,
      "grad_norm": 0.5107985138893127,
      "learning_rate": 4.221502084204587e-07,
      "loss": 1.1553,
      "step": 17010
    },
    {
      "epoch": 2.731124127417155,
      "grad_norm": 0.6257354021072388,
      "learning_rate": 4.1718354565732253e-07,
      "loss": 1.1718,
      "step": 17020
    },
    {
      "epoch": 2.732728877477333,
      "grad_norm": 0.49820399284362793,
      "learning_rate": 4.122456503454442e-07,
      "loss": 1.0694,
      "step": 17030
    },
    {
      "epoch": 2.734333627537511,
      "grad_norm": 0.6634145975112915,
      "learning_rate": 4.0733653730829604e-07,
      "loss": 1.1928,
      "step": 17040
    },
    {
      "epoch": 2.735938377597689,
      "grad_norm": 0.6311155557632446,
      "learning_rate": 4.0245622128295635e-07,
      "loss": 1.293,
      "step": 17050
    },
    {
      "epoch": 2.7375431276578674,
      "grad_norm": 0.4032334089279175,
      "learning_rate": 3.97604716920047e-07,
      "loss": 1.2602,
      "step": 17060
    },
    {
      "epoch": 2.7391478777180454,
      "grad_norm": 0.3935014307498932,
      "learning_rate": 3.9278203878370335e-07,
      "loss": 1.2158,
      "step": 17070
    },
    {
      "epoch": 2.7407526277782237,
      "grad_norm": 0.6484265327453613,
      "learning_rate": 3.8798820135152016e-07,
      "loss": 1.4435,
      "step": 17080
    },
    {
      "epoch": 2.7423573778384016,
      "grad_norm": 0.5803165435791016,
      "learning_rate": 3.832232190145191e-07,
      "loss": 1.3431,
      "step": 17090
    },
    {
      "epoch": 2.74396212789858,
      "grad_norm": 0.5970931649208069,
      "learning_rate": 3.7848710607709337e-07,
      "loss": 1.2059,
      "step": 17100
    },
    {
      "epoch": 2.745566877958758,
      "grad_norm": 0.4602697789669037,
      "learning_rate": 3.737798767569745e-07,
      "loss": 1.2792,
      "step": 17110
    },
    {
      "epoch": 2.747171628018936,
      "grad_norm": 0.46645140647888184,
      "learning_rate": 3.6910154518518203e-07,
      "loss": 1.2022,
      "step": 17120
    },
    {
      "epoch": 2.748776378079114,
      "grad_norm": 0.4009815454483032,
      "learning_rate": 3.644521254059896e-07,
      "loss": 1.2708,
      "step": 17130
    },
    {
      "epoch": 2.750381128139292,
      "grad_norm": 0.5828425884246826,
      "learning_rate": 3.598316313768746e-07,
      "loss": 1.3301,
      "step": 17140
    },
    {
      "epoch": 2.7519858781994704,
      "grad_norm": 0.32523399591445923,
      "learning_rate": 3.552400769684827e-07,
      "loss": 1.1886,
      "step": 17150
    },
    {
      "epoch": 2.7535906282596487,
      "grad_norm": 0.8620737195014954,
      "learning_rate": 3.5067747596458033e-07,
      "loss": 1.2778,
      "step": 17160
    },
    {
      "epoch": 2.7551953783198266,
      "grad_norm": 0.5909776091575623,
      "learning_rate": 3.4614384206202003e-07,
      "loss": 1.3012,
      "step": 17170
    },
    {
      "epoch": 2.7568001283800045,
      "grad_norm": 0.435677707195282,
      "learning_rate": 3.4163918887068936e-07,
      "loss": 1.2611,
      "step": 17180
    },
    {
      "epoch": 2.758404878440183,
      "grad_norm": 0.6675191521644592,
      "learning_rate": 3.3716352991348456e-07,
      "loss": 1.4467,
      "step": 17190
    },
    {
      "epoch": 2.7600096285003612,
      "grad_norm": 0.6174426674842834,
      "learning_rate": 3.327168786262536e-07,
      "loss": 1.3421,
      "step": 17200
    },
    {
      "epoch": 2.761614378560539,
      "grad_norm": 0.3934857249259949,
      "learning_rate": 3.282992483577696e-07,
      "loss": 1.3756,
      "step": 17210
    },
    {
      "epoch": 2.763219128620717,
      "grad_norm": 0.4335234463214874,
      "learning_rate": 3.239106523696833e-07,
      "loss": 1.1183,
      "step": 17220
    },
    {
      "epoch": 2.7648238786808954,
      "grad_norm": 0.8507736325263977,
      "learning_rate": 3.195511038364851e-07,
      "loss": 1.1627,
      "step": 17230
    },
    {
      "epoch": 2.7664286287410738,
      "grad_norm": 0.7402805685997009,
      "learning_rate": 3.152206158454629e-07,
      "loss": 1.3731,
      "step": 17240
    },
    {
      "epoch": 2.7680333788012517,
      "grad_norm": 0.43293800950050354,
      "learning_rate": 3.109192013966711e-07,
      "loss": 1.2879,
      "step": 17250
    },
    {
      "epoch": 2.7696381288614296,
      "grad_norm": 0.6128173470497131,
      "learning_rate": 3.0664687340287936e-07,
      "loss": 1.3483,
      "step": 17260
    },
    {
      "epoch": 2.771242878921608,
      "grad_norm": 0.6377682089805603,
      "learning_rate": 3.024036446895451e-07,
      "loss": 1.3625,
      "step": 17270
    },
    {
      "epoch": 2.7728476289817863,
      "grad_norm": 0.35266613960266113,
      "learning_rate": 2.9818952799476664e-07,
      "loss": 1.3059,
      "step": 17280
    },
    {
      "epoch": 2.774452379041964,
      "grad_norm": 0.581010639667511,
      "learning_rate": 2.940045359692534e-07,
      "loss": 1.3699,
      "step": 17290
    },
    {
      "epoch": 2.776057129102142,
      "grad_norm": 0.6591640710830688,
      "learning_rate": 2.8984868117627687e-07,
      "loss": 1.342,
      "step": 17300
    },
    {
      "epoch": 2.7776618791623204,
      "grad_norm": 0.9393012523651123,
      "learning_rate": 2.857219760916441e-07,
      "loss": 1.3274,
      "step": 17310
    },
    {
      "epoch": 2.779266629222499,
      "grad_norm": 0.5356320142745972,
      "learning_rate": 2.8162443310365104e-07,
      "loss": 1.2947,
      "step": 17320
    },
    {
      "epoch": 2.7808713792826767,
      "grad_norm": 0.35614028573036194,
      "learning_rate": 2.775560645130548e-07,
      "loss": 1.3105,
      "step": 17330
    },
    {
      "epoch": 2.7824761293428546,
      "grad_norm": 0.6526917219161987,
      "learning_rate": 2.7351688253302475e-07,
      "loss": 1.3834,
      "step": 17340
    },
    {
      "epoch": 2.784080879403033,
      "grad_norm": 0.4340428411960602,
      "learning_rate": 2.695068992891203e-07,
      "loss": 1.3019,
      "step": 17350
    },
    {
      "epoch": 2.7856856294632113,
      "grad_norm": 0.4412083625793457,
      "learning_rate": 2.6552612681923883e-07,
      "loss": 1.3103,
      "step": 17360
    },
    {
      "epoch": 2.787290379523389,
      "grad_norm": 0.4547877013683319,
      "learning_rate": 2.6157457707359336e-07,
      "loss": 1.1206,
      "step": 17370
    },
    {
      "epoch": 2.788895129583567,
      "grad_norm": 0.43229037523269653,
      "learning_rate": 2.576522619146682e-07,
      "loss": 1.2206,
      "step": 17380
    },
    {
      "epoch": 2.7904998796437455,
      "grad_norm": 0.5240358710289001,
      "learning_rate": 2.537591931171868e-07,
      "loss": 1.2696,
      "step": 17390
    },
    {
      "epoch": 2.792104629703924,
      "grad_norm": 0.5578109622001648,
      "learning_rate": 2.49895382368075e-07,
      "loss": 1.2175,
      "step": 17400
    },
    {
      "epoch": 2.7937093797641017,
      "grad_norm": 0.4707072675228119,
      "learning_rate": 2.460608412664256e-07,
      "loss": 1.3273,
      "step": 17410
    },
    {
      "epoch": 2.7953141298242796,
      "grad_norm": 0.6317185759544373,
      "learning_rate": 2.422555813234684e-07,
      "loss": 1.464,
      "step": 17420
    },
    {
      "epoch": 2.796918879884458,
      "grad_norm": 0.3212819993495941,
      "learning_rate": 2.384796139625245e-07,
      "loss": 1.2107,
      "step": 17430
    },
    {
      "epoch": 2.7985236299446363,
      "grad_norm": 0.5237717628479004,
      "learning_rate": 2.3473295051898658e-07,
      "loss": 1.4584,
      "step": 17440
    },
    {
      "epoch": 2.8001283800048142,
      "grad_norm": 0.540053129196167,
      "learning_rate": 2.310156022402732e-07,
      "loss": 1.2839,
      "step": 17450
    },
    {
      "epoch": 2.801733130064992,
      "grad_norm": 0.6216922998428345,
      "learning_rate": 2.2732758028580103e-07,
      "loss": 1.3064,
      "step": 17460
    },
    {
      "epoch": 2.8033378801251705,
      "grad_norm": 0.37358158826828003,
      "learning_rate": 2.2366889572694727e-07,
      "loss": 1.2881,
      "step": 17470
    },
    {
      "epoch": 2.804942630185349,
      "grad_norm": 0.5069852471351624,
      "learning_rate": 2.2003955954702173e-07,
      "loss": 1.3653,
      "step": 17480
    },
    {
      "epoch": 2.8065473802455267,
      "grad_norm": 0.42019158601760864,
      "learning_rate": 2.1643958264123022e-07,
      "loss": 1.2525,
      "step": 17490
    },
    {
      "epoch": 2.8081521303057047,
      "grad_norm": 0.5148032307624817,
      "learning_rate": 2.1286897581664134e-07,
      "loss": 1.3515,
      "step": 17500
    },
    {
      "epoch": 2.809756880365883,
      "grad_norm": 0.6531961560249329,
      "learning_rate": 2.093277497921542e-07,
      "loss": 1.3545,
      "step": 17510
    },
    {
      "epoch": 2.8113616304260614,
      "grad_norm": 0.7502983808517456,
      "learning_rate": 2.0581591519847176e-07,
      "loss": 1.3782,
      "step": 17520
    },
    {
      "epoch": 2.8129663804862393,
      "grad_norm": 0.5701701641082764,
      "learning_rate": 2.0233348257805874e-07,
      "loss": 1.2941,
      "step": 17530
    },
    {
      "epoch": 2.814571130546417,
      "grad_norm": 0.6881681680679321,
      "learning_rate": 1.9888046238512036e-07,
      "loss": 1.3563,
      "step": 17540
    },
    {
      "epoch": 2.8161758806065955,
      "grad_norm": 0.5706901550292969,
      "learning_rate": 1.954568649855626e-07,
      "loss": 1.331,
      "step": 17550
    },
    {
      "epoch": 2.817780630666774,
      "grad_norm": 0.7031036019325256,
      "learning_rate": 1.9206270065696753e-07,
      "loss": 1.3138,
      "step": 17560
    },
    {
      "epoch": 2.8193853807269518,
      "grad_norm": 0.37055039405822754,
      "learning_rate": 1.8869797958855795e-07,
      "loss": 1.2477,
      "step": 17570
    },
    {
      "epoch": 2.8209901307871297,
      "grad_norm": 0.6351083517074585,
      "learning_rate": 1.853627118811685e-07,
      "loss": 1.3043,
      "step": 17580
    },
    {
      "epoch": 2.822594880847308,
      "grad_norm": 0.5286720991134644,
      "learning_rate": 1.8205690754721783e-07,
      "loss": 1.2079,
      "step": 17590
    },
    {
      "epoch": 2.8241996309074864,
      "grad_norm": 0.4238099753856659,
      "learning_rate": 1.7878057651067205e-07,
      "loss": 1.122,
      "step": 17600
    },
    {
      "epoch": 2.8258043809676643,
      "grad_norm": 0.644159734249115,
      "learning_rate": 1.7553372860702244e-07,
      "loss": 1.2811,
      "step": 17610
    },
    {
      "epoch": 2.827409131027842,
      "grad_norm": 0.5158137679100037,
      "learning_rate": 1.7231637358324894e-07,
      "loss": 1.2323,
      "step": 17620
    },
    {
      "epoch": 2.8290138810880205,
      "grad_norm": 0.40520423650741577,
      "learning_rate": 1.6912852109779775e-07,
      "loss": 1.4894,
      "step": 17630
    },
    {
      "epoch": 2.830618631148199,
      "grad_norm": 0.45929592847824097,
      "learning_rate": 1.6597018072054715e-07,
      "loss": 1.3087,
      "step": 17640
    },
    {
      "epoch": 2.832223381208377,
      "grad_norm": 0.556201696395874,
      "learning_rate": 1.6284136193278178e-07,
      "loss": 1.3947,
      "step": 17650
    },
    {
      "epoch": 2.8338281312685547,
      "grad_norm": 0.7063337564468384,
      "learning_rate": 1.597420741271616e-07,
      "loss": 1.2817,
      "step": 17660
    },
    {
      "epoch": 2.835432881328733,
      "grad_norm": 0.5663663744926453,
      "learning_rate": 1.5667232660769527e-07,
      "loss": 1.3339,
      "step": 17670
    },
    {
      "epoch": 2.8370376313889114,
      "grad_norm": 0.6226805448532104,
      "learning_rate": 1.5363212858971243e-07,
      "loss": 1.2263,
      "step": 17680
    },
    {
      "epoch": 2.8386423814490893,
      "grad_norm": 0.4531145989894867,
      "learning_rate": 1.50621489199837e-07,
      "loss": 1.3156,
      "step": 17690
    },
    {
      "epoch": 2.840247131509267,
      "grad_norm": 0.6100195646286011,
      "learning_rate": 1.4764041747595493e-07,
      "loss": 1.5388,
      "step": 17700
    },
    {
      "epoch": 2.8418518815694456,
      "grad_norm": 0.6196862459182739,
      "learning_rate": 1.4468892236719433e-07,
      "loss": 1.3379,
      "step": 17710
    },
    {
      "epoch": 2.843456631629624,
      "grad_norm": 0.3806380331516266,
      "learning_rate": 1.4176701273389104e-07,
      "loss": 1.3605,
      "step": 17720
    },
    {
      "epoch": 2.845061381689802,
      "grad_norm": 0.7030655741691589,
      "learning_rate": 1.3887469734757075e-07,
      "loss": 1.3816,
      "step": 17730
    },
    {
      "epoch": 2.8466661317499797,
      "grad_norm": 0.4334409236907959,
      "learning_rate": 1.3601198489091138e-07,
      "loss": 1.3601,
      "step": 17740
    },
    {
      "epoch": 2.848270881810158,
      "grad_norm": 0.794817328453064,
      "learning_rate": 1.331788839577297e-07,
      "loss": 1.4124,
      "step": 17750
    },
    {
      "epoch": 2.8498756318703364,
      "grad_norm": 0.6611198782920837,
      "learning_rate": 1.303754030529458e-07,
      "loss": 1.1981,
      "step": 17760
    },
    {
      "epoch": 2.8514803819305143,
      "grad_norm": 0.7293237447738647,
      "learning_rate": 1.2760155059255986e-07,
      "loss": 1.2272,
      "step": 17770
    },
    {
      "epoch": 2.8530851319906922,
      "grad_norm": 0.3229457139968872,
      "learning_rate": 1.2485733490363083e-07,
      "loss": 1.2285,
      "step": 17780
    },
    {
      "epoch": 2.8546898820508706,
      "grad_norm": 0.3746863007545471,
      "learning_rate": 1.2214276422424787e-07,
      "loss": 1.5336,
      "step": 17790
    },
    {
      "epoch": 2.856294632111049,
      "grad_norm": 0.887474775314331,
      "learning_rate": 1.1945784670350346e-07,
      "loss": 1.4013,
      "step": 17800
    },
    {
      "epoch": 2.857899382171227,
      "grad_norm": 0.39736393094062805,
      "learning_rate": 1.168025904014769e-07,
      "loss": 1.1808,
      "step": 17810
    },
    {
      "epoch": 2.8595041322314048,
      "grad_norm": 0.857252836227417,
      "learning_rate": 1.141770032892009e-07,
      "loss": 1.4106,
      "step": 17820
    },
    {
      "epoch": 2.861108882291583,
      "grad_norm": 0.4010452926158905,
      "learning_rate": 1.1158109324864274e-07,
      "loss": 1.2114,
      "step": 17830
    },
    {
      "epoch": 2.8627136323517615,
      "grad_norm": 0.7611783742904663,
      "learning_rate": 1.0901486807268102e-07,
      "loss": 1.365,
      "step": 17840
    },
    {
      "epoch": 2.8643183824119394,
      "grad_norm": 0.7367109060287476,
      "learning_rate": 1.064783354650789e-07,
      "loss": 1.27,
      "step": 17850
    },
    {
      "epoch": 2.8659231324721173,
      "grad_norm": 0.7632745504379272,
      "learning_rate": 1.0397150304046422e-07,
      "loss": 1.3569,
      "step": 17860
    },
    {
      "epoch": 2.8675278825322956,
      "grad_norm": 0.6267468333244324,
      "learning_rate": 1.0149437832430498e-07,
      "loss": 1.3268,
      "step": 17870
    },
    {
      "epoch": 2.869132632592474,
      "grad_norm": 0.44412609934806824,
      "learning_rate": 9.904696875288832e-08,
      "loss": 1.3457,
      "step": 17880
    },
    {
      "epoch": 2.870737382652652,
      "grad_norm": 0.6800945401191711,
      "learning_rate": 9.662928167329389e-08,
      "loss": 1.3497,
      "step": 17890
    },
    {
      "epoch": 2.87234213271283,
      "grad_norm": 0.49543821811676025,
      "learning_rate": 9.42413243433793e-08,
      "loss": 1.2641,
      "step": 17900
    },
    {
      "epoch": 2.873946882773008,
      "grad_norm": 0.37065589427948,
      "learning_rate": 9.188310393175026e-08,
      "loss": 1.2339,
      "step": 17910
    },
    {
      "epoch": 2.8755516328331865,
      "grad_norm": 0.5313907265663147,
      "learning_rate": 8.955462751774502e-08,
      "loss": 1.4156,
      "step": 17920
    },
    {
      "epoch": 2.8771563828933644,
      "grad_norm": 0.4053478240966797,
      "learning_rate": 8.725590209140878e-08,
      "loss": 1.3196,
      "step": 17930
    },
    {
      "epoch": 2.8787611329535423,
      "grad_norm": 0.5007176399230957,
      "learning_rate": 8.498693455347595e-08,
      "loss": 1.2193,
      "step": 17940
    },
    {
      "epoch": 2.8803658830137207,
      "grad_norm": 0.5304114818572998,
      "learning_rate": 8.274773171535023e-08,
      "loss": 1.2921,
      "step": 17950
    },
    {
      "epoch": 2.881970633073899,
      "grad_norm": 0.5167708992958069,
      "learning_rate": 8.053830029907894e-08,
      "loss": 1.2869,
      "step": 17960
    },
    {
      "epoch": 2.883575383134077,
      "grad_norm": 0.36873170733451843,
      "learning_rate": 7.835864693733653e-08,
      "loss": 1.2096,
      "step": 17970
    },
    {
      "epoch": 2.885180133194255,
      "grad_norm": 0.5436723828315735,
      "learning_rate": 7.620877817340666e-08,
      "loss": 1.3257,
      "step": 17980
    },
    {
      "epoch": 2.886784883254433,
      "grad_norm": 0.5815848708152771,
      "learning_rate": 7.408870046115791e-08,
      "loss": 1.2714,
      "step": 17990
    },
    {
      "epoch": 2.8883896333146115,
      "grad_norm": 0.37579643726348877,
      "learning_rate": 7.199842016502811e-08,
      "loss": 1.1112,
      "step": 18000
    },
    {
      "epoch": 2.8899943833747894,
      "grad_norm": 0.42850324511528015,
      "learning_rate": 6.993794356000227e-08,
      "loss": 1.321,
      "step": 18010
    },
    {
      "epoch": 2.8915991334349673,
      "grad_norm": 0.5995984673500061,
      "learning_rate": 6.790727683159582e-08,
      "loss": 1.2919,
      "step": 18020
    },
    {
      "epoch": 2.8932038834951457,
      "grad_norm": 0.5743855834007263,
      "learning_rate": 6.590642607583686e-08,
      "loss": 1.2716,
      "step": 18030
    },
    {
      "epoch": 2.894808633555324,
      "grad_norm": 0.5702255368232727,
      "learning_rate": 6.393539729924292e-08,
      "loss": 1.2427,
      "step": 18040
    },
    {
      "epoch": 2.896413383615502,
      "grad_norm": 0.5377230048179626,
      "learning_rate": 6.199419641881088e-08,
      "loss": 1.2677,
      "step": 18050
    },
    {
      "epoch": 2.89801813367568,
      "grad_norm": 0.5678423047065735,
      "learning_rate": 6.00828292619915e-08,
      "loss": 1.2361,
      "step": 18060
    },
    {
      "epoch": 2.899622883735858,
      "grad_norm": 0.6985659003257751,
      "learning_rate": 5.8201301566676026e-08,
      "loss": 1.3262,
      "step": 18070
    },
    {
      "epoch": 2.9012276337960365,
      "grad_norm": 0.515032172203064,
      "learning_rate": 5.6349618981179635e-08,
      "loss": 1.2666,
      "step": 18080
    },
    {
      "epoch": 2.9028323838562144,
      "grad_norm": 0.5985629558563232,
      "learning_rate": 5.4527787064220237e-08,
      "loss": 1.2709,
      "step": 18090
    },
    {
      "epoch": 2.9044371339163924,
      "grad_norm": 0.38464897871017456,
      "learning_rate": 5.2735811284908564e-08,
      "loss": 1.3977,
      "step": 18100
    },
    {
      "epoch": 2.9060418839765707,
      "grad_norm": 0.8225547075271606,
      "learning_rate": 5.097369702272259e-08,
      "loss": 1.3259,
      "step": 18110
    },
    {
      "epoch": 2.9076466340367486,
      "grad_norm": 0.428671270608902,
      "learning_rate": 4.9241449567500876e-08,
      "loss": 1.2862,
      "step": 18120
    },
    {
      "epoch": 2.909251384096927,
      "grad_norm": 0.8611279726028442,
      "learning_rate": 4.753907411942038e-08,
      "loss": 1.3833,
      "step": 18130
    },
    {
      "epoch": 2.910856134157105,
      "grad_norm": 0.6973825693130493,
      "learning_rate": 4.586657578898202e-08,
      "loss": 1.3084,
      "step": 18140
    },
    {
      "epoch": 2.912460884217283,
      "grad_norm": 0.5993285775184631,
      "learning_rate": 4.422395959699843e-08,
      "loss": 1.2382,
      "step": 18150
    },
    {
      "epoch": 2.914065634277461,
      "grad_norm": 0.42166343331336975,
      "learning_rate": 4.261123047457516e-08,
      "loss": 1.191,
      "step": 18160
    },
    {
      "epoch": 2.9156703843376395,
      "grad_norm": 0.5681373476982117,
      "learning_rate": 4.1028393263097266e-08,
      "loss": 1.1135,
      "step": 18170
    },
    {
      "epoch": 2.9172751343978174,
      "grad_norm": 0.5722049474716187,
      "learning_rate": 3.947545271421493e-08,
      "loss": 1.3075,
      "step": 18180
    },
    {
      "epoch": 2.9188798844579957,
      "grad_norm": 0.5430333614349365,
      "learning_rate": 3.7952413489830144e-08,
      "loss": 1.3261,
      "step": 18190
    },
    {
      "epoch": 2.9204846345181736,
      "grad_norm": 0.4490331709384918,
      "learning_rate": 3.645928016207889e-08,
      "loss": 1.2225,
      "step": 18200
    },
    {
      "epoch": 2.922089384578352,
      "grad_norm": 0.40999293327331543,
      "learning_rate": 3.499605721332344e-08,
      "loss": 1.2379,
      "step": 18210
    },
    {
      "epoch": 2.92369413463853,
      "grad_norm": 0.5616937279701233,
      "learning_rate": 3.3562749036132325e-08,
      "loss": 1.3178,
      "step": 18220
    },
    {
      "epoch": 2.9252988846987082,
      "grad_norm": 0.5677050352096558,
      "learning_rate": 3.2159359933274794e-08,
      "loss": 1.2795,
      "step": 18230
    },
    {
      "epoch": 2.926903634758886,
      "grad_norm": 0.610206663608551,
      "learning_rate": 3.078589411769861e-08,
      "loss": 1.2783,
      "step": 18240
    },
    {
      "epoch": 2.9285083848190645,
      "grad_norm": 0.4313344657421112,
      "learning_rate": 2.9442355712525628e-08,
      "loss": 1.2561,
      "step": 18250
    },
    {
      "epoch": 2.9301131348792424,
      "grad_norm": 0.5415025949478149,
      "learning_rate": 2.8128748751035106e-08,
      "loss": 1.2875,
      "step": 18260
    },
    {
      "epoch": 2.9317178849394208,
      "grad_norm": 0.5131865739822388,
      "learning_rate": 2.6845077176652633e-08,
      "loss": 1.1718,
      "step": 18270
    },
    {
      "epoch": 2.9333226349995987,
      "grad_norm": 0.5064764618873596,
      "learning_rate": 2.5591344842939013e-08,
      "loss": 1.2423,
      "step": 18280
    },
    {
      "epoch": 2.934927385059777,
      "grad_norm": 0.44686636328697205,
      "learning_rate": 2.4367555513575836e-08,
      "loss": 1.2156,
      "step": 18290
    },
    {
      "epoch": 2.936532135119955,
      "grad_norm": 0.44459667801856995,
      "learning_rate": 2.317371286235992e-08,
      "loss": 1.1964,
      "step": 18300
    },
    {
      "epoch": 2.9381368851801333,
      "grad_norm": 0.4648866355419159,
      "learning_rate": 2.200982047318445e-08,
      "loss": 1.3717,
      "step": 18310
    },
    {
      "epoch": 2.939741635240311,
      "grad_norm": 0.35513100028038025,
      "learning_rate": 2.087588184003453e-08,
      "loss": 1.4003,
      "step": 18320
    },
    {
      "epoch": 2.9413463853004895,
      "grad_norm": 0.5315214395523071,
      "learning_rate": 1.9771900366976073e-08,
      "loss": 1.3254,
      "step": 18330
    },
    {
      "epoch": 2.9429511353606674,
      "grad_norm": 0.5654463171958923,
      "learning_rate": 1.869787936814138e-08,
      "loss": 1.3969,
      "step": 18340
    },
    {
      "epoch": 2.944555885420846,
      "grad_norm": 0.6629233360290527,
      "learning_rate": 1.7653822067723593e-08,
      "loss": 1.3304,
      "step": 18350
    },
    {
      "epoch": 2.9461606354810237,
      "grad_norm": 0.5487380027770996,
      "learning_rate": 1.663973159996557e-08,
      "loss": 1.4091,
      "step": 18360
    },
    {
      "epoch": 2.947765385541202,
      "grad_norm": 0.45088958740234375,
      "learning_rate": 1.5655611009147698e-08,
      "loss": 1.1919,
      "step": 18370
    },
    {
      "epoch": 2.94937013560138,
      "grad_norm": 0.4265685975551605,
      "learning_rate": 1.4701463249585657e-08,
      "loss": 1.2403,
      "step": 18380
    },
    {
      "epoch": 2.9509748856615583,
      "grad_norm": 0.579399585723877,
      "learning_rate": 1.3777291185612662e-08,
      "loss": 1.3619,
      "step": 18390
    },
    {
      "epoch": 2.952579635721736,
      "grad_norm": 0.47600388526916504,
      "learning_rate": 1.2883097591578353e-08,
      "loss": 1.2636,
      "step": 18400
    },
    {
      "epoch": 2.9541843857819146,
      "grad_norm": 0.4826064705848694,
      "learning_rate": 1.2018885151835468e-08,
      "loss": 1.3568,
      "step": 18410
    },
    {
      "epoch": 2.9557891358420925,
      "grad_norm": 0.5558674335479736,
      "learning_rate": 1.1184656460737631e-08,
      "loss": 1.438,
      "step": 18420
    },
    {
      "epoch": 2.957393885902271,
      "grad_norm": 0.5459105372428894,
      "learning_rate": 1.0380414022622687e-08,
      "loss": 1.2086,
      "step": 18430
    },
    {
      "epoch": 2.9589986359624487,
      "grad_norm": 0.41211825609207153,
      "learning_rate": 9.606160251812712e-09,
      "loss": 1.3941,
      "step": 18440
    },
    {
      "epoch": 2.960603386022627,
      "grad_norm": 0.4913010895252228,
      "learning_rate": 8.86189747260624e-09,
      "loss": 1.4081,
      "step": 18450
    },
    {
      "epoch": 2.962208136082805,
      "grad_norm": 0.37799158692359924,
      "learning_rate": 8.147627919266044e-09,
      "loss": 1.1824,
      "step": 18460
    },
    {
      "epoch": 2.9638128861429833,
      "grad_norm": 0.4274607002735138,
      "learning_rate": 7.46335373601692e-09,
      "loss": 1.1823,
      "step": 18470
    },
    {
      "epoch": 2.9654176362031612,
      "grad_norm": 0.6096999645233154,
      "learning_rate": 6.809076977037921e-09,
      "loss": 1.2972,
      "step": 18480
    },
    {
      "epoch": 2.9670223862633396,
      "grad_norm": 0.8661333322525024,
      "learning_rate": 6.184799606457903e-09,
      "loss": 1.3185,
      "step": 18490
    },
    {
      "epoch": 2.9686271363235175,
      "grad_norm": 0.6364095211029053,
      "learning_rate": 5.590523498346656e-09,
      "loss": 1.357,
      "step": 18500
    },
    {
      "epoch": 2.970231886383696,
      "grad_norm": 0.451429158449173,
      "learning_rate": 5.026250436711566e-09,
      "loss": 1.2391,
      "step": 18510
    },
    {
      "epoch": 2.9718366364438737,
      "grad_norm": 0.5649818778038025,
      "learning_rate": 4.491982115488735e-09,
      "loss": 1.4751,
      "step": 18520
    },
    {
      "epoch": 2.973441386504052,
      "grad_norm": 0.6985239386558533,
      "learning_rate": 3.987720138542983e-09,
      "loss": 1.3343,
      "step": 18530
    },
    {
      "epoch": 2.97504613656423,
      "grad_norm": 0.3865548074245453,
      "learning_rate": 3.5134660196622927e-09,
      "loss": 1.2769,
      "step": 18540
    },
    {
      "epoch": 2.9766508866244084,
      "grad_norm": 0.5381142497062683,
      "learning_rate": 3.069221182546711e-09,
      "loss": 1.1617,
      "step": 18550
    },
    {
      "epoch": 2.9782556366845863,
      "grad_norm": 0.5672022700309753,
      "learning_rate": 2.6549869608127888e-09,
      "loss": 1.2994,
      "step": 18560
    },
    {
      "epoch": 2.9798603867447646,
      "grad_norm": 0.7378777265548706,
      "learning_rate": 2.2707645979858085e-09,
      "loss": 1.1699,
      "step": 18570
    },
    {
      "epoch": 2.9814651368049425,
      "grad_norm": 0.5031852126121521,
      "learning_rate": 1.9165552474920136e-09,
      "loss": 1.3865,
      "step": 18580
    },
    {
      "epoch": 2.983069886865121,
      "grad_norm": 0.44068285822868347,
      "learning_rate": 1.5923599726641593e-09,
      "loss": 1.2757,
      "step": 18590
    },
    {
      "epoch": 2.9846746369252988,
      "grad_norm": 0.6102472543716431,
      "learning_rate": 1.2981797467304103e-09,
      "loss": 1.262,
      "step": 18600
    },
    {
      "epoch": 2.986279386985477,
      "grad_norm": 0.4267982542514801,
      "learning_rate": 1.034015452814341e-09,
      "loss": 1.3118,
      "step": 18610
    },
    {
      "epoch": 2.987884137045655,
      "grad_norm": 0.400490939617157,
      "learning_rate": 7.998678839327145e-10,
      "loss": 1.1675,
      "step": 18620
    },
    {
      "epoch": 2.9894888871058334,
      "grad_norm": 0.6708554625511169,
      "learning_rate": 5.957377429921529e-10,
      "loss": 1.2569,
      "step": 18630
    },
    {
      "epoch": 2.9910936371660113,
      "grad_norm": 0.44135475158691406,
      "learning_rate": 4.2162564278913633e-10,
      "loss": 1.2948,
      "step": 18640
    },
    {
      "epoch": 2.9926983872261896,
      "grad_norm": 0.47133520245552063,
      "learning_rate": 2.77532106003342e-10,
      "loss": 1.387,
      "step": 18650
    },
    {
      "epoch": 2.9943031372863675,
      "grad_norm": 0.6364811658859253,
      "learning_rate": 1.6345756520319555e-10,
      "loss": 1.3622,
      "step": 18660
    },
    {
      "epoch": 2.995907887346546,
      "grad_norm": 0.7490768432617188,
      "learning_rate": 7.940236283698888e-11,
      "loss": 1.4606,
      "step": 18670
    },
    {
      "epoch": 2.997512637406724,
      "grad_norm": 0.6661316156387329,
      "learning_rate": 2.536675123732124e-11,
      "loss": 1.3072,
      "step": 18680
    },
    {
      "epoch": 2.999117387466902,
      "grad_norm": 0.5604644417762756,
      "learning_rate": 1.350892618878774e-12,
      "loss": 1.304,
      "step": 18690
    }
  ],
  "logging_steps": 10,
  "max_steps": 18693,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.535078046165632e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
