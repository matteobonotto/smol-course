{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `HuggingFaceTB/smoltalk` dataset</p>\n",
    "    <p>üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n",
    "    <p>ü¶Å Select a dataset that relates to a real world use case your interested in</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0890330160d4a23ab52d3d277d32227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format, DataCollatorForCompletionOnlyLM\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# )\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device,\n",
    ")#.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-haiku-fullfinetune\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 2260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['full_topic', 'messages'],\n",
      "        num_rows: 119\n",
      "    })\n",
      "})\n",
      "[{'content': 'Hi there', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\", 'role': 'user'}, {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\", 'role': 'assistant'}, {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?', 'role': 'user'}, {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.', 'role': 'assistant'}, {'content': \"Okay, I'll look into those. Thanks for the recommendations!\", 'role': 'user'}, {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "# ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "# ds = load_dataset(\"bigcode/the-stack-smol\", split=\"train\")\n",
    "\n",
    "# ds = load_dataset(\"davanstrien/haiku_dpo\", split=\"train\")\n",
    "\n",
    "# def format_ds_text_label(sample):\n",
    "#     return {\"text\":sample[\"question\"], \"label\":sample[\"chosen\"]}\n",
    "# ds = ds.map(format_ds_text_label)\n",
    "print(ds)\n",
    "print(ds[\"train\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'generation_model', 'generation_prompt', 'generations', 'scores', 'chosen', 'chosen_score', 'rejected', 'rejected_score', 'tie', 'difference_in_score', 'system', 'messages'],\n",
      "    num_rows: 3916\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are helpful assistant', 'role': 'system'},\n",
       " {'content': 'Can you compose a haiku about the serenity of mountain peaks?',\n",
       "  'role': 'user'},\n",
       " {'content': \"Majestic peaks so tall,\\nSilent whispers of time's grace,\\nNature's cathedral.\\n\\nMountaintop so high,\\nSilence wraps around in peace,\\nNature's breath in sighs.\\n\\nMajestic peaks soar,\\nSilent wisdom in their heights,\\nNature's calming song.\\n\\nMajestic mountain,  \\nSerenity in every peak,  \\nNature's quiet song.\\n\\nMajestic peaks soar,\\nSilent whispers of stone and snow,\\nNature's cathedral.\\n\\nMajestic peak so high,\\nIn silence, nature's beauty,\\nEternal peace reigns.\\n\\nMajestic peaks soar,\\nSilence speaks, soul at ease,\\nNature's grand cathedral.\\n\\nMajestic peaks soar,\\nSilent whispers of stone and sky,\\nNature's grand cathedral.\\n\\nMajestic peaks so high,\\nSilent sentinels of time,\\nNature's cathedral.\\n\\nMajestic peaks ascend,\\nSilent, stoic, standing tall,\\nNature's grand cathedral.\\n\\nMajestic peaks so high,\\nSilent whispers of the sky,\\nNature's crown of pride.\\n\\nMajestic peaks rise,\\nSilent strength in snow and stone,\\nNature's cathedral.\\n\\nMajestic peaks rise,\\nSilent sentinels of time,\\nNature's grand cathedral.\\n\\nMajestic peaks rise,\\nSilent, still, in purest white,\\nSky's embrace, a gift.\\n\\nMajestic peaks soar,\\nSilent whispers of stone and ice,\\nNature's throne of peace.\\n\\nMajestic peaks so high,\\nSnow-capped summits touch the sky,\\nTranquil world up above.\\n\\nMajestic mountain peaks,\\nSnow-capped, standing tall and proud,\\nNature's silent throne.\\n\\nMajestic peaks soar,\\nSilent whispers of stone and ice,\\nNature's cathedral.\\n\\nMajestic peaks soar,\\nSilence whispers, secrets kept,\\nHeaven's touch, pure grace.\\n\\nMajestic peaks soar,\\nSilence echoes in vast space,\\nNature's masterpiece.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "# dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\", split=\"train\")\n",
    "# dataset = load_dataset(path=\"prithivMLmods/Math-Solve\", split=\"train[:25000]\")\n",
    "\n",
    "# def format_dataset(sample):\n",
    "#     content = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are helpful\"},\n",
    "#         {\"role\": \"user\", \"content\": sample[\"problem\"]}, \n",
    "#         {\"role\": \"assistant\", \"content\": sample[\"solution\"]}\n",
    "#     ]\n",
    "#     return {\"messages\":content}\n",
    "\n",
    "# dataset = dataset.map(format_dataset)\n",
    "\n",
    "\n",
    "# dataset = load_dataset(path=\"Qurtana/medical-o1-reasoning-SFT-orpo\",  split=\"train\")\n",
    "# def format(sample):\n",
    "#     content = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": sample[\"prompt\"]}, \n",
    "#         {\"role\": \"assistant\", \"content\": sample[\"accepted\"]}\n",
    "#     ]\n",
    "#     return {\"messages\":content}\n",
    "\n",
    "# dataset=dataset.map(format)\n",
    "\n",
    "# dataset_train = dataset.select(range(20000))\n",
    "# dataset_test = dataset.select(range(dataset.shape[0]-2000, dataset.shape[0]))\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "# dataset = load_dataset(path=\"openai/gsm8k\", name=\"main\")\n",
    "\n",
    "# def format(sample):\n",
    "#     pass\n",
    "#     content = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": sample[\"question\"]}, \n",
    "#         {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "#     ]\n",
    "#     return {\"messages\":content}\n",
    "\n",
    "# dataset=dataset.map(format)\n",
    "\n",
    "# dataset_train = dataset[\"train\"]\n",
    "# dataset_test = dataset[\"test\"]\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "dataset = load_dataset(path=\"davanstrien/haiku_dpo\", split=\"train\")\n",
    "\n",
    "# data = []\n",
    "# for row in tqdm(dataset.iter(batch_size=1), total=len(dataset)):\n",
    "#     question = row[\"question\"][0]\n",
    "#     generations = row[\"generations\"][0]\n",
    "#     if len(question) > 2:\n",
    "#         data.extend((question, x) for x in generations)\n",
    "\n",
    "# random.shuffle(data)\n",
    "\n",
    "# ds = Dataset.from_dict(\n",
    "#     {\n",
    "#         \"question\": [d[0] for d in data],\n",
    "#         \"generations\": [d[1] for d in data]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# data = [(x,y) for data]\n",
    "\n",
    "def format(sample):\n",
    "    pass\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": sample[\"question\"]}, \n",
    "        {\"role\": \"assistant\", \"content\": \"\\n\\n\".join(sample[\"generations\"])} \n",
    "    ]\n",
    "    return {\"messages\":messages}\n",
    "\n",
    "\n",
    "dataset=dataset.map(format)\n",
    "\n",
    "train_rartio = .95 \n",
    "idx_train = list(range(int(dataset.shape[0]*train_rartio)))\n",
    "idx_test = range(idx_train[-1]+1, dataset.shape[0])\n",
    "dataset_train = dataset.select(idx_train)\n",
    "dataset_test = dataset.select(idx_test)\n",
    "\n",
    "# dataset_train = dataset.select(range(15000))\n",
    "# dataset_test = dataset.select(range(dataset.shape[0]-1000, dataset.shape[0]))\n",
    "\n",
    "print(dataset_train)\n",
    "\n",
    "dataset_train[0][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majestic peaks so tall,\n",
      "Silent whispers of time's grace,\n",
      "Nature's cathedral.\n",
      "\n",
      "Mountaintop so high,\n",
      "Silence wraps around in peace,\n",
      "Nature's breath in sighs.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silent wisdom in their heights,\n",
      "Nature's calming song.\n",
      "\n",
      "Majestic mountain,  \n",
      "Serenity in every peak,  \n",
      "Nature's quiet song.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silent whispers of stone and snow,\n",
      "Nature's cathedral.\n",
      "\n",
      "Majestic peak so high,\n",
      "In silence, nature's beauty,\n",
      "Eternal peace reigns.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silence speaks, soul at ease,\n",
      "Nature's grand cathedral.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silent whispers of stone and sky,\n",
      "Nature's grand cathedral.\n",
      "\n",
      "Majestic peaks so high,\n",
      "Silent sentinels of time,\n",
      "Nature's cathedral.\n",
      "\n",
      "Majestic peaks ascend,\n",
      "Silent, stoic, standing tall,\n",
      "Nature's grand cathedral.\n",
      "\n",
      "Majestic peaks so high,\n",
      "Silent whispers of the sky,\n",
      "Nature's crown of pride.\n",
      "\n",
      "Majestic peaks rise,\n",
      "Silent strength in snow and stone,\n",
      "Nature's cathedral.\n",
      "\n",
      "Majestic peaks rise,\n",
      "Silent sentinels of time,\n",
      "Nature's grand cathedral.\n",
      "\n",
      "Majestic peaks rise,\n",
      "Silent, still, in purest white,\n",
      "Sky's embrace, a gift.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silent whispers of stone and ice,\n",
      "Nature's throne of peace.\n",
      "\n",
      "Majestic peaks so high,\n",
      "Snow-capped summits touch the sky,\n",
      "Tranquil world up above.\n",
      "\n",
      "Majestic mountain peaks,\n",
      "Snow-capped, standing tall and proud,\n",
      "Nature's silent throne.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silent whispers of stone and ice,\n",
      "Nature's cathedral.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silence whispers, secrets kept,\n",
      "Heaven's touch, pure grace.\n",
      "\n",
      "Majestic peaks soar,\n",
      "Silence echoes in vast space,\n",
      "Nature's masterpiece.\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[0][\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the SFTTrainer\n",
    "\n",
    "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matte/smol-course/.venv/lib/python3.10/site-packages/transformers/training_args.py:1576: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    # Small learning rate to prevent catastrophic forgetting\n",
    "    learning_rate=1e-5,\n",
    "    # Linear learning rate decay over training\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # Maximum combined length of prompt + completion\n",
    "    max_seq_length=1512,\n",
    "    # # Maximum length for input prompts\n",
    "    # max_prompt_length=512,\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # Helps with training stability by accumulating gradients before updating\n",
    "    gradient_accumulation_steps=4,\n",
    "    # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
    "    # optim=\"paged_adamw_8bit\" if device == \"cuda\" else \"adamw_torch\",\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=4,\n",
    "    # When to run evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # Evaluate every 20% of training\n",
    "    eval_steps=0.2,\n",
    "    # Log metrics every step\n",
    "    logging_steps=1,\n",
    "    # Gradual learning rate warmup\n",
    "    warmup_steps=100,\n",
    "    # Disable external logging\n",
    "    report_to=\"wandb\",\n",
    "    # Where to save model/checkpoints\n",
    "    output_dir=finetune_name,\n",
    "    # Enable MPS (Metal Performance Shaders) if available\n",
    "    # use_mps_device=device == \"mps\",\n",
    "    hub_model_id=finetune_name,\n",
    "    # Use bfloat16 precision for faster training\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# def format_cols(sample):\n",
    "#     return {\"text\":sample[\"question\"], \"label\":sample[\"answer\"]}\n",
    "\n",
    "# ds = ds.map(format_cols)\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: ü¶Å üêï align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(trainer.train_dataset[0][\"input_ids\"]))\n",
    "\n",
    "# print(tokenizer.decode(trainer.train_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteob-90-hotmail-it\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matte/smol-course/1_instruction_tuning/notebooks/wandb/run-20250124_094620-prkydoiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteob-90-hotmail-it/huggingface/runs/prkydoiq' target=\"_blank\">SmolLM2-FT-haiku-fullfinetune</a></strong> to <a href='https://wandb.ai/matteob-90-hotmail-it/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteob-90-hotmail-it/huggingface' target=\"_blank\">https://wandb.ai/matteob-90-hotmail-it/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteob-90-hotmail-it/huggingface/runs/prkydoiq' target=\"_blank\">https://wandb.ai/matteob-90-hotmail-it/huggingface/runs/prkydoiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='976' max='976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [976/976 1:07:50, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.681000</td>\n",
       "      <td>1.572136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>1.532200</td>\n",
       "      <td>1.468184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>1.409500</td>\n",
       "      <td>1.431804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>1.508700</td>\n",
       "      <td>1.419608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abba8daf69f4ac0a59088ec1ad6245e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbf1bae1a6e44f2bfa74c97f41fb043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb685148c5e442c89c6ec5c04172f698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/matteobonotto/SmolLM2-FT-haiku-fullfinetune/commit/4106f047e5716be03aa22391be06f53f14c67c11', commit_message='End of training', commit_description='', oid='4106f047e5716be03aa22391be06f53f14c67c11', pr_url=None, repo_url=RepoUrl('https://huggingface.co/matteobonotto/SmolLM2-FT-haiku-fullfinetune', endpoint='https://huggingface.co', repo_type='model', repo_id='matteobonotto/SmolLM2-FT-haiku-fullfinetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Write a haiku about Italy\n",
      "\n",
      "Italy's beauty,\n",
      "A gentle breeze whispers,\n",
      "Nature's symphony.\n",
      "\n",
      "Italy's beauty,\n",
      "A gentle breeze whispers,\n",
      "Nature's symphony.\n",
      "\n",
      "Italy's beauty,\n",
      "A gentle breeze whispers,\n",
      "Nature's symphony.\n",
      "\n",
      "Italy's beauty,\n",
      "A gentle breeze whispers,\n",
      "Nature's symphony.\n",
      "\n",
      "Italy's beauty,\n",
      "A gentle breeze whispers,\n",
      "Nature's symphony.\n",
      "\n",
      "Italy's beauty,\n",
      "A gentle breeze whispers,\n",
      "Nature's symphony\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about Italy\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# TODO: use the fine-tuned to model generate a response, just like with the base example.\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(\n",
    "    tokenizer.decode(out[0], skip_special_tokens=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Write a haiku about python\n",
      "\n",
      "Python's beauty,\n",
      "\n",
      "A gentle, yet powerful force,\n",
      "\n",
      "Nature's art, in code.\n",
      "\n",
      "Python's beauty,\n",
      "\n",
      "A gentle, yet powerful force,\n",
      "\n",
      "Nature's art, in code.\n",
      "\n",
      "Python's beauty,\n",
      "\n",
      "A gentle, yet powerful force,\n",
      "\n",
      "Nature's art, in code.\n",
      "\n",
      "Python's beauty,\n",
      "\n",
      "A gentle, yet powerful force,\n",
      "\n",
      "Nature's art, in code.\n",
      "\n",
      "Python's beauty\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about python\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# TODO: use the fine-tuned to model generate a response, just like with the base example.\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(\n",
    "    tokenizer.decode(out[0], skip_special_tokens=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
