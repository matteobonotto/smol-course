{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0408580183861083,
  "eval_steps": 196,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0040858018386108275,
      "grad_norm": 3.3829736709594727,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 2.5089,
      "step": 1
    },
    {
      "epoch": 0.008171603677221655,
      "grad_norm": 3.2491164207458496,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 2.4799,
      "step": 2
    },
    {
      "epoch": 0.012257405515832482,
      "grad_norm": 3.290837287902832,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 2.3185,
      "step": 3
    },
    {
      "epoch": 0.01634320735444331,
      "grad_norm": 3.1849443912506104,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 2.3057,
      "step": 4
    },
    {
      "epoch": 0.020429009193054137,
      "grad_norm": 3.328892946243286,
      "learning_rate": 5.000000000000001e-07,
      "loss": 2.3949,
      "step": 5
    },
    {
      "epoch": 0.024514811031664963,
      "grad_norm": 3.3742828369140625,
      "learning_rate": 6.000000000000001e-07,
      "loss": 2.4727,
      "step": 6
    },
    {
      "epoch": 0.028600612870275793,
      "grad_norm": 3.344877004623413,
      "learning_rate": 7.000000000000001e-07,
      "loss": 2.4281,
      "step": 7
    },
    {
      "epoch": 0.03268641470888662,
      "grad_norm": 3.237417697906494,
      "learning_rate": 8.000000000000001e-07,
      "loss": 2.5296,
      "step": 8
    },
    {
      "epoch": 0.03677221654749745,
      "grad_norm": 3.208129644393921,
      "learning_rate": 9.000000000000001e-07,
      "loss": 2.4847,
      "step": 9
    },
    {
      "epoch": 0.04085801838610827,
      "grad_norm": 3.245870351791382,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.394,
      "step": 10
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 3.204270362854004,
      "learning_rate": 1.1e-06,
      "loss": 2.4251,
      "step": 11
    },
    {
      "epoch": 0.049029622063329927,
      "grad_norm": 3.3433761596679688,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 2.431,
      "step": 12
    },
    {
      "epoch": 0.05311542390194075,
      "grad_norm": 3.3835549354553223,
      "learning_rate": 1.3e-06,
      "loss": 2.4456,
      "step": 13
    },
    {
      "epoch": 0.05720122574055159,
      "grad_norm": 3.36755633354187,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 2.5022,
      "step": 14
    },
    {
      "epoch": 0.06128702757916241,
      "grad_norm": 3.331289529800415,
      "learning_rate": 1.5e-06,
      "loss": 2.4943,
      "step": 15
    },
    {
      "epoch": 0.06537282941777324,
      "grad_norm": 3.1281816959381104,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 2.3366,
      "step": 16
    },
    {
      "epoch": 0.06945863125638406,
      "grad_norm": 3.1873154640197754,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 2.5708,
      "step": 17
    },
    {
      "epoch": 0.0735444330949949,
      "grad_norm": 3.2119953632354736,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 2.4829,
      "step": 18
    },
    {
      "epoch": 0.07763023493360573,
      "grad_norm": 3.1607604026794434,
      "learning_rate": 1.9000000000000002e-06,
      "loss": 2.4966,
      "step": 19
    },
    {
      "epoch": 0.08171603677221655,
      "grad_norm": 3.2040622234344482,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.4853,
      "step": 20
    },
    {
      "epoch": 0.08580183861082738,
      "grad_norm": 3.156081199645996,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 2.4293,
      "step": 21
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 3.155705690383911,
      "learning_rate": 2.2e-06,
      "loss": 2.3213,
      "step": 22
    },
    {
      "epoch": 0.09397344228804903,
      "grad_norm": 3.203162908554077,
      "learning_rate": 2.3000000000000004e-06,
      "loss": 2.4453,
      "step": 23
    },
    {
      "epoch": 0.09805924412665985,
      "grad_norm": 3.2644028663635254,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.4128,
      "step": 24
    },
    {
      "epoch": 0.10214504596527069,
      "grad_norm": 3.2368576526641846,
      "learning_rate": 2.5e-06,
      "loss": 2.3705,
      "step": 25
    },
    {
      "epoch": 0.1062308478038815,
      "grad_norm": 3.14498233795166,
      "learning_rate": 2.6e-06,
      "loss": 2.4845,
      "step": 26
    },
    {
      "epoch": 0.11031664964249234,
      "grad_norm": 3.1693668365478516,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 2.4444,
      "step": 27
    },
    {
      "epoch": 0.11440245148110317,
      "grad_norm": 3.3279011249542236,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 2.3683,
      "step": 28
    },
    {
      "epoch": 0.118488253319714,
      "grad_norm": 3.2898716926574707,
      "learning_rate": 2.9e-06,
      "loss": 2.4696,
      "step": 29
    },
    {
      "epoch": 0.12257405515832483,
      "grad_norm": 3.293219804763794,
      "learning_rate": 3e-06,
      "loss": 2.3695,
      "step": 30
    },
    {
      "epoch": 0.12665985699693566,
      "grad_norm": 3.105851411819458,
      "learning_rate": 3.1000000000000004e-06,
      "loss": 2.3508,
      "step": 31
    },
    {
      "epoch": 0.13074565883554648,
      "grad_norm": 3.2666871547698975,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 2.4466,
      "step": 32
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 3.132329225540161,
      "learning_rate": 3.3000000000000006e-06,
      "loss": 2.489,
      "step": 33
    },
    {
      "epoch": 0.13891726251276812,
      "grad_norm": 3.0588462352752686,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 2.405,
      "step": 34
    },
    {
      "epoch": 0.14300306435137897,
      "grad_norm": 3.1943771839141846,
      "learning_rate": 3.5e-06,
      "loss": 2.3765,
      "step": 35
    },
    {
      "epoch": 0.1470888661899898,
      "grad_norm": 3.1176047325134277,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 2.3619,
      "step": 36
    },
    {
      "epoch": 0.1511746680286006,
      "grad_norm": 3.16823673248291,
      "learning_rate": 3.7e-06,
      "loss": 2.3449,
      "step": 37
    },
    {
      "epoch": 0.15526046986721145,
      "grad_norm": 2.9607386589050293,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 2.3667,
      "step": 38
    },
    {
      "epoch": 0.15934627170582227,
      "grad_norm": 3.111638307571411,
      "learning_rate": 3.900000000000001e-06,
      "loss": 2.4315,
      "step": 39
    },
    {
      "epoch": 0.1634320735444331,
      "grad_norm": 2.9505045413970947,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.3377,
      "step": 40
    },
    {
      "epoch": 0.1675178753830439,
      "grad_norm": 3.1034393310546875,
      "learning_rate": 4.1e-06,
      "loss": 2.3025,
      "step": 41
    },
    {
      "epoch": 0.17160367722165476,
      "grad_norm": 3.0407190322875977,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 2.3749,
      "step": 42
    },
    {
      "epoch": 0.17568947906026558,
      "grad_norm": 2.979146718978882,
      "learning_rate": 4.3e-06,
      "loss": 2.3454,
      "step": 43
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 2.901794910430908,
      "learning_rate": 4.4e-06,
      "loss": 2.329,
      "step": 44
    },
    {
      "epoch": 0.18386108273748722,
      "grad_norm": 2.814213752746582,
      "learning_rate": 4.5e-06,
      "loss": 2.3967,
      "step": 45
    },
    {
      "epoch": 0.18794688457609807,
      "grad_norm": 2.9190244674682617,
      "learning_rate": 4.600000000000001e-06,
      "loss": 2.3115,
      "step": 46
    },
    {
      "epoch": 0.1920326864147089,
      "grad_norm": 3.0396504402160645,
      "learning_rate": 4.7e-06,
      "loss": 2.3389,
      "step": 47
    },
    {
      "epoch": 0.1961184882533197,
      "grad_norm": 2.8625245094299316,
      "learning_rate": 4.800000000000001e-06,
      "loss": 2.3661,
      "step": 48
    },
    {
      "epoch": 0.20020429009193055,
      "grad_norm": 2.877645254135132,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 2.4112,
      "step": 49
    },
    {
      "epoch": 0.20429009193054137,
      "grad_norm": 2.943817138671875,
      "learning_rate": 5e-06,
      "loss": 2.3444,
      "step": 50
    },
    {
      "epoch": 0.2083758937691522,
      "grad_norm": 2.7169415950775146,
      "learning_rate": 5.1e-06,
      "loss": 2.3719,
      "step": 51
    },
    {
      "epoch": 0.212461695607763,
      "grad_norm": 2.7281627655029297,
      "learning_rate": 5.2e-06,
      "loss": 2.2388,
      "step": 52
    },
    {
      "epoch": 0.21654749744637386,
      "grad_norm": 2.61973237991333,
      "learning_rate": 5.300000000000001e-06,
      "loss": 2.224,
      "step": 53
    },
    {
      "epoch": 0.22063329928498468,
      "grad_norm": 2.6662778854370117,
      "learning_rate": 5.400000000000001e-06,
      "loss": 2.1945,
      "step": 54
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 2.509321451187134,
      "learning_rate": 5.500000000000001e-06,
      "loss": 2.2927,
      "step": 55
    },
    {
      "epoch": 0.22880490296220635,
      "grad_norm": 2.3763692378997803,
      "learning_rate": 5.600000000000001e-06,
      "loss": 2.3623,
      "step": 56
    },
    {
      "epoch": 0.23289070480081717,
      "grad_norm": 2.3128390312194824,
      "learning_rate": 5.7e-06,
      "loss": 2.3144,
      "step": 57
    },
    {
      "epoch": 0.236976506639428,
      "grad_norm": 2.4734749794006348,
      "learning_rate": 5.8e-06,
      "loss": 2.1994,
      "step": 58
    },
    {
      "epoch": 0.2410623084780388,
      "grad_norm": 2.4007058143615723,
      "learning_rate": 5.9e-06,
      "loss": 2.2547,
      "step": 59
    },
    {
      "epoch": 0.24514811031664965,
      "grad_norm": 2.3849408626556396,
      "learning_rate": 6e-06,
      "loss": 2.2413,
      "step": 60
    },
    {
      "epoch": 0.24923391215526047,
      "grad_norm": 2.3451406955718994,
      "learning_rate": 6.1e-06,
      "loss": 2.2176,
      "step": 61
    },
    {
      "epoch": 0.2533197139938713,
      "grad_norm": 2.3156731128692627,
      "learning_rate": 6.200000000000001e-06,
      "loss": 2.3031,
      "step": 62
    },
    {
      "epoch": 0.2574055158324821,
      "grad_norm": 2.373661756515503,
      "learning_rate": 6.300000000000001e-06,
      "loss": 2.0754,
      "step": 63
    },
    {
      "epoch": 0.26149131767109296,
      "grad_norm": 2.112269163131714,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 2.166,
      "step": 64
    },
    {
      "epoch": 0.26557711950970375,
      "grad_norm": 2.2189927101135254,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 2.1825,
      "step": 65
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 2.077983856201172,
      "learning_rate": 6.600000000000001e-06,
      "loss": 2.3392,
      "step": 66
    },
    {
      "epoch": 0.27374872318692545,
      "grad_norm": 2.210522174835205,
      "learning_rate": 6.700000000000001e-06,
      "loss": 2.1832,
      "step": 67
    },
    {
      "epoch": 0.27783452502553624,
      "grad_norm": 2.177490472793579,
      "learning_rate": 6.800000000000001e-06,
      "loss": 2.2173,
      "step": 68
    },
    {
      "epoch": 0.2819203268641471,
      "grad_norm": 2.1432788372039795,
      "learning_rate": 6.9e-06,
      "loss": 2.2471,
      "step": 69
    },
    {
      "epoch": 0.28600612870275793,
      "grad_norm": 1.9484342336654663,
      "learning_rate": 7e-06,
      "loss": 2.217,
      "step": 70
    },
    {
      "epoch": 0.2900919305413687,
      "grad_norm": 1.8661788702011108,
      "learning_rate": 7.100000000000001e-06,
      "loss": 1.965,
      "step": 71
    },
    {
      "epoch": 0.2941777323799796,
      "grad_norm": 1.8478305339813232,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 2.2066,
      "step": 72
    },
    {
      "epoch": 0.2982635342185904,
      "grad_norm": 1.7833147048950195,
      "learning_rate": 7.3e-06,
      "loss": 1.9782,
      "step": 73
    },
    {
      "epoch": 0.3023493360572012,
      "grad_norm": 1.6863828897476196,
      "learning_rate": 7.4e-06,
      "loss": 2.113,
      "step": 74
    },
    {
      "epoch": 0.30643513789581206,
      "grad_norm": 1.712652564048767,
      "learning_rate": 7.500000000000001e-06,
      "loss": 2.1471,
      "step": 75
    },
    {
      "epoch": 0.3105209397344229,
      "grad_norm": 1.562754511833191,
      "learning_rate": 7.600000000000001e-06,
      "loss": 2.0002,
      "step": 76
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 1.533638596534729,
      "learning_rate": 7.7e-06,
      "loss": 2.1104,
      "step": 77
    },
    {
      "epoch": 0.31869254341164455,
      "grad_norm": 1.6071219444274902,
      "learning_rate": 7.800000000000002e-06,
      "loss": 2.0483,
      "step": 78
    },
    {
      "epoch": 0.32277834525025534,
      "grad_norm": 1.4763665199279785,
      "learning_rate": 7.9e-06,
      "loss": 2.0629,
      "step": 79
    },
    {
      "epoch": 0.3268641470888662,
      "grad_norm": 1.3456838130950928,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9614,
      "step": 80
    },
    {
      "epoch": 0.33094994892747703,
      "grad_norm": 1.438092589378357,
      "learning_rate": 8.1e-06,
      "loss": 2.0619,
      "step": 81
    },
    {
      "epoch": 0.3350357507660878,
      "grad_norm": 1.2771646976470947,
      "learning_rate": 8.2e-06,
      "loss": 2.0531,
      "step": 82
    },
    {
      "epoch": 0.3391215526046987,
      "grad_norm": 1.2244858741760254,
      "learning_rate": 8.3e-06,
      "loss": 1.9334,
      "step": 83
    },
    {
      "epoch": 0.3432073544433095,
      "grad_norm": 1.2445896863937378,
      "learning_rate": 8.400000000000001e-06,
      "loss": 2.0841,
      "step": 84
    },
    {
      "epoch": 0.3472931562819203,
      "grad_norm": 1.2144769430160522,
      "learning_rate": 8.5e-06,
      "loss": 1.8723,
      "step": 85
    },
    {
      "epoch": 0.35137895812053116,
      "grad_norm": 1.094497799873352,
      "learning_rate": 8.6e-06,
      "loss": 1.9161,
      "step": 86
    },
    {
      "epoch": 0.355464759959142,
      "grad_norm": 1.1723986864089966,
      "learning_rate": 8.700000000000001e-06,
      "loss": 1.9345,
      "step": 87
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 1.0887377262115479,
      "learning_rate": 8.8e-06,
      "loss": 1.9869,
      "step": 88
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 1.044213056564331,
      "learning_rate": 8.900000000000001e-06,
      "loss": 1.9782,
      "step": 89
    },
    {
      "epoch": 0.36772216547497444,
      "grad_norm": 1.0860035419464111,
      "learning_rate": 9e-06,
      "loss": 1.9602,
      "step": 90
    },
    {
      "epoch": 0.3718079673135853,
      "grad_norm": 1.0279744863510132,
      "learning_rate": 9.100000000000001e-06,
      "loss": 1.8569,
      "step": 91
    },
    {
      "epoch": 0.37589376915219613,
      "grad_norm": 1.072224497795105,
      "learning_rate": 9.200000000000002e-06,
      "loss": 1.9846,
      "step": 92
    },
    {
      "epoch": 0.3799795709908069,
      "grad_norm": 1.00211501121521,
      "learning_rate": 9.3e-06,
      "loss": 1.8289,
      "step": 93
    },
    {
      "epoch": 0.3840653728294178,
      "grad_norm": 1.0193885564804077,
      "learning_rate": 9.4e-06,
      "loss": 1.803,
      "step": 94
    },
    {
      "epoch": 0.3881511746680286,
      "grad_norm": 1.068361520767212,
      "learning_rate": 9.5e-06,
      "loss": 1.8208,
      "step": 95
    },
    {
      "epoch": 0.3922369765066394,
      "grad_norm": 0.9977104663848877,
      "learning_rate": 9.600000000000001e-06,
      "loss": 1.9059,
      "step": 96
    },
    {
      "epoch": 0.39632277834525026,
      "grad_norm": 0.9563716650009155,
      "learning_rate": 9.7e-06,
      "loss": 1.899,
      "step": 97
    },
    {
      "epoch": 0.4004085801838611,
      "grad_norm": 1.058883547782898,
      "learning_rate": 9.800000000000001e-06,
      "loss": 1.8806,
      "step": 98
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 1.005527138710022,
      "learning_rate": 9.9e-06,
      "loss": 1.7639,
      "step": 99
    },
    {
      "epoch": 0.40858018386108275,
      "grad_norm": 0.9955249428749084,
      "learning_rate": 1e-05,
      "loss": 1.8659,
      "step": 100
    },
    {
      "epoch": 0.41266598569969354,
      "grad_norm": 1.010312557220459,
      "learning_rate": 9.999967846291054e-06,
      "loss": 1.8895,
      "step": 101
    },
    {
      "epoch": 0.4167517875383044,
      "grad_norm": 1.1165413856506348,
      "learning_rate": 9.999871385577756e-06,
      "loss": 1.8746,
      "step": 102
    },
    {
      "epoch": 0.42083758937691523,
      "grad_norm": 1.3805980682373047,
      "learning_rate": 9.999710619100732e-06,
      "loss": 1.8606,
      "step": 103
    },
    {
      "epoch": 0.424923391215526,
      "grad_norm": 1.0794728994369507,
      "learning_rate": 9.999485548927686e-06,
      "loss": 1.8,
      "step": 104
    },
    {
      "epoch": 0.4290091930541369,
      "grad_norm": 1.0054545402526855,
      "learning_rate": 9.999196177953345e-06,
      "loss": 1.8217,
      "step": 105
    },
    {
      "epoch": 0.4330949948927477,
      "grad_norm": 0.9761620759963989,
      "learning_rate": 9.998842509899456e-06,
      "loss": 1.7438,
      "step": 106
    },
    {
      "epoch": 0.4371807967313585,
      "grad_norm": 0.9001764059066772,
      "learning_rate": 9.99842454931471e-06,
      "loss": 1.813,
      "step": 107
    },
    {
      "epoch": 0.44126659856996936,
      "grad_norm": 0.948833703994751,
      "learning_rate": 9.997942301574701e-06,
      "loss": 1.7688,
      "step": 108
    },
    {
      "epoch": 0.4453524004085802,
      "grad_norm": 0.9818925857543945,
      "learning_rate": 9.997395772881853e-06,
      "loss": 1.7147,
      "step": 109
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.9761221408843994,
      "learning_rate": 9.996784970265335e-06,
      "loss": 1.7671,
      "step": 110
    },
    {
      "epoch": 0.45352400408580185,
      "grad_norm": 0.9516489505767822,
      "learning_rate": 9.99610990158097e-06,
      "loss": 1.7461,
      "step": 111
    },
    {
      "epoch": 0.4576098059244127,
      "grad_norm": 0.8722044229507446,
      "learning_rate": 9.995370575511151e-06,
      "loss": 1.7357,
      "step": 112
    },
    {
      "epoch": 0.4616956077630235,
      "grad_norm": 0.8773142099380493,
      "learning_rate": 9.994567001564703e-06,
      "loss": 1.7653,
      "step": 113
    },
    {
      "epoch": 0.46578140960163433,
      "grad_norm": 0.8347426056861877,
      "learning_rate": 9.993699190076778e-06,
      "loss": 1.6817,
      "step": 114
    },
    {
      "epoch": 0.4698672114402451,
      "grad_norm": 0.8453010320663452,
      "learning_rate": 9.992767152208724e-06,
      "loss": 1.7029,
      "step": 115
    },
    {
      "epoch": 0.473953013278856,
      "grad_norm": 0.8705855011940002,
      "learning_rate": 9.991770899947925e-06,
      "loss": 1.8604,
      "step": 116
    },
    {
      "epoch": 0.4780388151174668,
      "grad_norm": 0.8153737187385559,
      "learning_rate": 9.99071044610767e-06,
      "loss": 1.6799,
      "step": 117
    },
    {
      "epoch": 0.4821246169560776,
      "grad_norm": 0.973566472530365,
      "learning_rate": 9.989585804326963e-06,
      "loss": 1.748,
      "step": 118
    },
    {
      "epoch": 0.48621041879468846,
      "grad_norm": 0.8062142133712769,
      "learning_rate": 9.988396989070365e-06,
      "loss": 1.7347,
      "step": 119
    },
    {
      "epoch": 0.4902962206332993,
      "grad_norm": 0.8926886916160583,
      "learning_rate": 9.98714401562781e-06,
      "loss": 1.688,
      "step": 120
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 1.0157231092453003,
      "learning_rate": 9.985826900114391e-06,
      "loss": 1.7779,
      "step": 121
    },
    {
      "epoch": 0.49846782431052095,
      "grad_norm": 0.882121741771698,
      "learning_rate": 9.984445659470166e-06,
      "loss": 1.7893,
      "step": 122
    },
    {
      "epoch": 0.5025536261491318,
      "grad_norm": 0.8479747772216797,
      "learning_rate": 9.983000311459943e-06,
      "loss": 1.6397,
      "step": 123
    },
    {
      "epoch": 0.5066394279877426,
      "grad_norm": 0.9136170744895935,
      "learning_rate": 9.98149087467304e-06,
      "loss": 1.6974,
      "step": 124
    },
    {
      "epoch": 0.5107252298263534,
      "grad_norm": 0.8176090121269226,
      "learning_rate": 9.979917368523053e-06,
      "loss": 1.5626,
      "step": 125
    },
    {
      "epoch": 0.5148110316649642,
      "grad_norm": 0.7951239347457886,
      "learning_rate": 9.978279813247605e-06,
      "loss": 1.7914,
      "step": 126
    },
    {
      "epoch": 0.5188968335035751,
      "grad_norm": 0.8319465517997742,
      "learning_rate": 9.97657822990809e-06,
      "loss": 1.6348,
      "step": 127
    },
    {
      "epoch": 0.5229826353421859,
      "grad_norm": 0.866348385810852,
      "learning_rate": 9.97481264038939e-06,
      "loss": 1.5998,
      "step": 128
    },
    {
      "epoch": 0.5270684371807968,
      "grad_norm": 0.8951460123062134,
      "learning_rate": 9.972983067399604e-06,
      "loss": 1.7466,
      "step": 129
    },
    {
      "epoch": 0.5311542390194075,
      "grad_norm": 0.9599961042404175,
      "learning_rate": 9.97108953446976e-06,
      "loss": 1.623,
      "step": 130
    },
    {
      "epoch": 0.5352400408580184,
      "grad_norm": 0.8499050736427307,
      "learning_rate": 9.969132065953499e-06,
      "loss": 1.6571,
      "step": 131
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.8590211868286133,
      "learning_rate": 9.967110687026769e-06,
      "loss": 1.7522,
      "step": 132
    },
    {
      "epoch": 0.54341164453524,
      "grad_norm": 0.8864648342132568,
      "learning_rate": 9.965025423687505e-06,
      "loss": 1.7311,
      "step": 133
    },
    {
      "epoch": 0.5474974463738509,
      "grad_norm": 0.7737080454826355,
      "learning_rate": 9.962876302755283e-06,
      "loss": 1.7091,
      "step": 134
    },
    {
      "epoch": 0.5515832482124617,
      "grad_norm": 0.8004711270332336,
      "learning_rate": 9.960663351870988e-06,
      "loss": 1.6638,
      "step": 135
    },
    {
      "epoch": 0.5556690500510725,
      "grad_norm": 0.8826137185096741,
      "learning_rate": 9.95838659949645e-06,
      "loss": 1.7382,
      "step": 136
    },
    {
      "epoch": 0.5597548518896833,
      "grad_norm": 0.8368663191795349,
      "learning_rate": 9.956046074914088e-06,
      "loss": 1.789,
      "step": 137
    },
    {
      "epoch": 0.5638406537282942,
      "grad_norm": 0.8223842978477478,
      "learning_rate": 9.953641808226513e-06,
      "loss": 1.5768,
      "step": 138
    },
    {
      "epoch": 0.567926455566905,
      "grad_norm": 0.8901197910308838,
      "learning_rate": 9.951173830356168e-06,
      "loss": 1.7281,
      "step": 139
    },
    {
      "epoch": 0.5720122574055159,
      "grad_norm": 0.8528128266334534,
      "learning_rate": 9.948642173044906e-06,
      "loss": 1.6319,
      "step": 140
    },
    {
      "epoch": 0.5760980592441267,
      "grad_norm": 0.8387688994407654,
      "learning_rate": 9.946046868853595e-06,
      "loss": 1.5486,
      "step": 141
    },
    {
      "epoch": 0.5801838610827375,
      "grad_norm": 0.8162190318107605,
      "learning_rate": 9.943387951161702e-06,
      "loss": 1.7066,
      "step": 142
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.8108418583869934,
      "learning_rate": 9.940665454166851e-06,
      "loss": 1.5913,
      "step": 143
    },
    {
      "epoch": 0.5883554647599591,
      "grad_norm": 0.7919254899024963,
      "learning_rate": 9.93787941288439e-06,
      "loss": 1.6546,
      "step": 144
    },
    {
      "epoch": 0.59244126659857,
      "grad_norm": 1.0231281518936157,
      "learning_rate": 9.935029863146946e-06,
      "loss": 1.7856,
      "step": 145
    },
    {
      "epoch": 0.5965270684371808,
      "grad_norm": 0.9042031168937683,
      "learning_rate": 9.932116841603954e-06,
      "loss": 1.8318,
      "step": 146
    },
    {
      "epoch": 0.6006128702757916,
      "grad_norm": 1.0128453969955444,
      "learning_rate": 9.929140385721193e-06,
      "loss": 1.7692,
      "step": 147
    },
    {
      "epoch": 0.6046986721144024,
      "grad_norm": 0.7685208916664124,
      "learning_rate": 9.926100533780304e-06,
      "loss": 1.612,
      "step": 148
    },
    {
      "epoch": 0.6087844739530133,
      "grad_norm": 0.820930004119873,
      "learning_rate": 9.92299732487829e-06,
      "loss": 1.6642,
      "step": 149
    },
    {
      "epoch": 0.6128702757916241,
      "grad_norm": 0.9259809255599976,
      "learning_rate": 9.919830798927024e-06,
      "loss": 1.7359,
      "step": 150
    },
    {
      "epoch": 0.616956077630235,
      "grad_norm": 0.8309621214866638,
      "learning_rate": 9.916600996652726e-06,
      "loss": 1.639,
      "step": 151
    },
    {
      "epoch": 0.6210418794688458,
      "grad_norm": 0.8183449506759644,
      "learning_rate": 9.913307959595443e-06,
      "loss": 1.68,
      "step": 152
    },
    {
      "epoch": 0.6251276813074566,
      "grad_norm": 0.9040635228157043,
      "learning_rate": 9.90995173010852e-06,
      "loss": 1.6539,
      "step": 153
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.8058280348777771,
      "learning_rate": 9.906532351358047e-06,
      "loss": 1.6781,
      "step": 154
    },
    {
      "epoch": 0.6332992849846782,
      "grad_norm": 0.8160686492919922,
      "learning_rate": 9.903049867322308e-06,
      "loss": 1.7503,
      "step": 155
    },
    {
      "epoch": 0.6373850868232891,
      "grad_norm": 0.862239420413971,
      "learning_rate": 9.899504322791212e-06,
      "loss": 1.7535,
      "step": 156
    },
    {
      "epoch": 0.6414708886618999,
      "grad_norm": 0.7590650320053101,
      "learning_rate": 9.895895763365722e-06,
      "loss": 1.6628,
      "step": 157
    },
    {
      "epoch": 0.6455566905005107,
      "grad_norm": 0.9287768006324768,
      "learning_rate": 9.892224235457269e-06,
      "loss": 1.6552,
      "step": 158
    },
    {
      "epoch": 0.6496424923391215,
      "grad_norm": 0.7802994847297668,
      "learning_rate": 9.888489786287146e-06,
      "loss": 1.5915,
      "step": 159
    },
    {
      "epoch": 0.6537282941777324,
      "grad_norm": 0.7706590294837952,
      "learning_rate": 9.88469246388591e-06,
      "loss": 1.5479,
      "step": 160
    },
    {
      "epoch": 0.6578140960163432,
      "grad_norm": 0.7894954681396484,
      "learning_rate": 9.880832317092762e-06,
      "loss": 1.5405,
      "step": 161
    },
    {
      "epoch": 0.6618998978549541,
      "grad_norm": 0.9521697759628296,
      "learning_rate": 9.876909395554915e-06,
      "loss": 1.5876,
      "step": 162
    },
    {
      "epoch": 0.6659856996935649,
      "grad_norm": 0.7764186859130859,
      "learning_rate": 9.872923749726959e-06,
      "loss": 1.5824,
      "step": 163
    },
    {
      "epoch": 0.6700715015321757,
      "grad_norm": 0.8229662179946899,
      "learning_rate": 9.868875430870217e-06,
      "loss": 1.5182,
      "step": 164
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 0.8251367807388306,
      "learning_rate": 9.86476449105207e-06,
      "loss": 1.6334,
      "step": 165
    },
    {
      "epoch": 0.6782431052093973,
      "grad_norm": 0.9041468501091003,
      "learning_rate": 9.860590983145307e-06,
      "loss": 1.6433,
      "step": 166
    },
    {
      "epoch": 0.6823289070480082,
      "grad_norm": 0.8367764949798584,
      "learning_rate": 9.85635496082743e-06,
      "loss": 1.6951,
      "step": 167
    },
    {
      "epoch": 0.686414708886619,
      "grad_norm": 0.8435304760932922,
      "learning_rate": 9.85205647857997e-06,
      "loss": 1.5691,
      "step": 168
    },
    {
      "epoch": 0.6905005107252298,
      "grad_norm": 0.8856136202812195,
      "learning_rate": 9.847695591687788e-06,
      "loss": 1.7521,
      "step": 169
    },
    {
      "epoch": 0.6945863125638406,
      "grad_norm": 0.8881165385246277,
      "learning_rate": 9.843272356238355e-06,
      "loss": 1.6593,
      "step": 170
    },
    {
      "epoch": 0.6986721144024515,
      "grad_norm": 0.844526469707489,
      "learning_rate": 9.838786829121045e-06,
      "loss": 1.6263,
      "step": 171
    },
    {
      "epoch": 0.7027579162410623,
      "grad_norm": 0.848063051700592,
      "learning_rate": 9.834239068026388e-06,
      "loss": 1.6838,
      "step": 172
    },
    {
      "epoch": 0.7068437180796732,
      "grad_norm": 0.9740725159645081,
      "learning_rate": 9.829629131445342e-06,
      "loss": 1.671,
      "step": 173
    },
    {
      "epoch": 0.710929519918284,
      "grad_norm": 0.8551496863365173,
      "learning_rate": 9.82495707866853e-06,
      "loss": 1.7248,
      "step": 174
    },
    {
      "epoch": 0.7150153217568948,
      "grad_norm": 0.8832868337631226,
      "learning_rate": 9.82022296978548e-06,
      "loss": 1.5535,
      "step": 175
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.8564367294311523,
      "learning_rate": 9.815426865683858e-06,
      "loss": 1.6053,
      "step": 176
    },
    {
      "epoch": 0.7231869254341164,
      "grad_norm": 0.8426526188850403,
      "learning_rate": 9.810568828048674e-06,
      "loss": 1.5508,
      "step": 177
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.7850543260574341,
      "learning_rate": 9.805648919361505e-06,
      "loss": 1.451,
      "step": 178
    },
    {
      "epoch": 0.7313585291113381,
      "grad_norm": 0.8240954279899597,
      "learning_rate": 9.800667202899672e-06,
      "loss": 1.5227,
      "step": 179
    },
    {
      "epoch": 0.7354443309499489,
      "grad_norm": 0.8508866429328918,
      "learning_rate": 9.79562374273544e-06,
      "loss": 1.6148,
      "step": 180
    },
    {
      "epoch": 0.7395301327885597,
      "grad_norm": 0.863655149936676,
      "learning_rate": 9.790518603735191e-06,
      "loss": 1.6512,
      "step": 181
    },
    {
      "epoch": 0.7436159346271706,
      "grad_norm": 0.8587683439254761,
      "learning_rate": 9.785351851558585e-06,
      "loss": 1.6248,
      "step": 182
    },
    {
      "epoch": 0.7477017364657814,
      "grad_norm": 0.8545947074890137,
      "learning_rate": 9.780123552657722e-06,
      "loss": 1.5823,
      "step": 183
    },
    {
      "epoch": 0.7517875383043923,
      "grad_norm": 0.826912522315979,
      "learning_rate": 9.774833774276278e-06,
      "loss": 1.6564,
      "step": 184
    },
    {
      "epoch": 0.7558733401430031,
      "grad_norm": 0.8710339665412903,
      "learning_rate": 9.769482584448654e-06,
      "loss": 1.6967,
      "step": 185
    },
    {
      "epoch": 0.7599591419816139,
      "grad_norm": 0.7784331440925598,
      "learning_rate": 9.76407005199909e-06,
      "loss": 1.4609,
      "step": 186
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 0.8147224187850952,
      "learning_rate": 9.758596246540782e-06,
      "loss": 1.5571,
      "step": 187
    },
    {
      "epoch": 0.7681307456588355,
      "grad_norm": 0.83614182472229,
      "learning_rate": 9.75306123847499e-06,
      "loss": 1.4844,
      "step": 188
    },
    {
      "epoch": 0.7722165474974464,
      "grad_norm": 0.8114571571350098,
      "learning_rate": 9.74746509899013e-06,
      "loss": 1.6736,
      "step": 189
    },
    {
      "epoch": 0.7763023493360572,
      "grad_norm": 0.799305260181427,
      "learning_rate": 9.741807900060858e-06,
      "loss": 1.6541,
      "step": 190
    },
    {
      "epoch": 0.780388151174668,
      "grad_norm": 0.8416231274604797,
      "learning_rate": 9.736089714447144e-06,
      "loss": 1.5901,
      "step": 191
    },
    {
      "epoch": 0.7844739530132788,
      "grad_norm": 0.846994161605835,
      "learning_rate": 9.73031061569334e-06,
      "loss": 1.5777,
      "step": 192
    },
    {
      "epoch": 0.7885597548518897,
      "grad_norm": 0.8148579001426697,
      "learning_rate": 9.724470678127226e-06,
      "loss": 1.4717,
      "step": 193
    },
    {
      "epoch": 0.7926455566905005,
      "grad_norm": 0.8310193419456482,
      "learning_rate": 9.718569976859068e-06,
      "loss": 1.6922,
      "step": 194
    },
    {
      "epoch": 0.7967313585291114,
      "grad_norm": 0.8231230974197388,
      "learning_rate": 9.712608587780634e-06,
      "loss": 1.6503,
      "step": 195
    },
    {
      "epoch": 0.8008171603677222,
      "grad_norm": 0.8353662490844727,
      "learning_rate": 9.706586587564236e-06,
      "loss": 1.681,
      "step": 196
    },
    {
      "epoch": 0.8008171603677222,
      "eval_loss": 1.5721359252929688,
      "eval_runtime": 18.5783,
      "eval_samples_per_second": 11.142,
      "eval_steps_per_second": 2.799,
      "step": 196
    },
    {
      "epoch": 0.804902962206333,
      "grad_norm": 0.8078488707542419,
      "learning_rate": 9.700504053661728e-06,
      "loss": 1.5936,
      "step": 197
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.8761959075927734,
      "learning_rate": 9.69436106430352e-06,
      "loss": 1.6603,
      "step": 198
    },
    {
      "epoch": 0.8130745658835546,
      "grad_norm": 0.8630024194717407,
      "learning_rate": 9.68815769849757e-06,
      "loss": 1.6273,
      "step": 199
    },
    {
      "epoch": 0.8171603677221655,
      "grad_norm": 0.8546648621559143,
      "learning_rate": 9.681894036028365e-06,
      "loss": 1.4874,
      "step": 200
    },
    {
      "epoch": 0.8212461695607763,
      "grad_norm": 0.9027106165885925,
      "learning_rate": 9.675570157455899e-06,
      "loss": 1.6791,
      "step": 201
    },
    {
      "epoch": 0.8253319713993871,
      "grad_norm": 0.8465160131454468,
      "learning_rate": 9.669186144114627e-06,
      "loss": 1.5772,
      "step": 202
    },
    {
      "epoch": 0.8294177732379979,
      "grad_norm": 0.8317087292671204,
      "learning_rate": 9.662742078112436e-06,
      "loss": 1.5413,
      "step": 203
    },
    {
      "epoch": 0.8335035750766088,
      "grad_norm": 0.8394920825958252,
      "learning_rate": 9.656238042329575e-06,
      "loss": 1.6293,
      "step": 204
    },
    {
      "epoch": 0.8375893769152196,
      "grad_norm": 0.8029077053070068,
      "learning_rate": 9.649674120417591e-06,
      "loss": 1.5882,
      "step": 205
    },
    {
      "epoch": 0.8416751787538305,
      "grad_norm": 0.7874294519424438,
      "learning_rate": 9.643050396798262e-06,
      "loss": 1.6176,
      "step": 206
    },
    {
      "epoch": 0.8457609805924413,
      "grad_norm": 0.8644412159919739,
      "learning_rate": 9.636366956662496e-06,
      "loss": 1.6649,
      "step": 207
    },
    {
      "epoch": 0.849846782431052,
      "grad_norm": 0.8101349472999573,
      "learning_rate": 9.62962388596925e-06,
      "loss": 1.6552,
      "step": 208
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.7655364274978638,
      "learning_rate": 9.622821271444418e-06,
      "loss": 1.524,
      "step": 209
    },
    {
      "epoch": 0.8580183861082737,
      "grad_norm": 0.8285725712776184,
      "learning_rate": 9.615959200579716e-06,
      "loss": 1.6133,
      "step": 210
    },
    {
      "epoch": 0.8621041879468846,
      "grad_norm": 0.7825216054916382,
      "learning_rate": 9.609037761631552e-06,
      "loss": 1.5411,
      "step": 211
    },
    {
      "epoch": 0.8661899897854954,
      "grad_norm": 0.8331167101860046,
      "learning_rate": 9.602057043619903e-06,
      "loss": 1.6185,
      "step": 212
    },
    {
      "epoch": 0.8702757916241062,
      "grad_norm": 0.7688961625099182,
      "learning_rate": 9.595017136327158e-06,
      "loss": 1.6032,
      "step": 213
    },
    {
      "epoch": 0.874361593462717,
      "grad_norm": 0.9999822378158569,
      "learning_rate": 9.587918130296969e-06,
      "loss": 1.5303,
      "step": 214
    },
    {
      "epoch": 0.8784473953013279,
      "grad_norm": 0.8608517646789551,
      "learning_rate": 9.580760116833086e-06,
      "loss": 1.6971,
      "step": 215
    },
    {
      "epoch": 0.8825331971399387,
      "grad_norm": 0.8009931445121765,
      "learning_rate": 9.57354318799818e-06,
      "loss": 1.4975,
      "step": 216
    },
    {
      "epoch": 0.8866189989785496,
      "grad_norm": 0.8063307404518127,
      "learning_rate": 9.566267436612662e-06,
      "loss": 1.5682,
      "step": 217
    },
    {
      "epoch": 0.8907048008171604,
      "grad_norm": 0.8491661548614502,
      "learning_rate": 9.558932956253492e-06,
      "loss": 1.5964,
      "step": 218
    },
    {
      "epoch": 0.8947906026557712,
      "grad_norm": 0.7991448640823364,
      "learning_rate": 9.551539841252969e-06,
      "loss": 1.5575,
      "step": 219
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.8133922219276428,
      "learning_rate": 9.544088186697515e-06,
      "loss": 1.6025,
      "step": 220
    },
    {
      "epoch": 0.9029622063329928,
      "grad_norm": 0.9020516276359558,
      "learning_rate": 9.536578088426468e-06,
      "loss": 1.6239,
      "step": 221
    },
    {
      "epoch": 0.9070480081716037,
      "grad_norm": 0.7939598560333252,
      "learning_rate": 9.529009643030831e-06,
      "loss": 1.6162,
      "step": 222
    },
    {
      "epoch": 0.9111338100102145,
      "grad_norm": 0.8187311887741089,
      "learning_rate": 9.521382947852042e-06,
      "loss": 1.5538,
      "step": 223
    },
    {
      "epoch": 0.9152196118488254,
      "grad_norm": 0.8332200646400452,
      "learning_rate": 9.513698100980715e-06,
      "loss": 1.6345,
      "step": 224
    },
    {
      "epoch": 0.9193054136874361,
      "grad_norm": 0.885047435760498,
      "learning_rate": 9.505955201255381e-06,
      "loss": 1.591,
      "step": 225
    },
    {
      "epoch": 0.923391215526047,
      "grad_norm": 0.9464643001556396,
      "learning_rate": 9.498154348261217e-06,
      "loss": 1.6149,
      "step": 226
    },
    {
      "epoch": 0.9274770173646578,
      "grad_norm": 0.7604774832725525,
      "learning_rate": 9.490295642328768e-06,
      "loss": 1.5415,
      "step": 227
    },
    {
      "epoch": 0.9315628192032687,
      "grad_norm": 0.774958610534668,
      "learning_rate": 9.482379184532652e-06,
      "loss": 1.5357,
      "step": 228
    },
    {
      "epoch": 0.9356486210418795,
      "grad_norm": 0.8544102311134338,
      "learning_rate": 9.474405076690257e-06,
      "loss": 1.5947,
      "step": 229
    },
    {
      "epoch": 0.9397344228804902,
      "grad_norm": 0.7783493995666504,
      "learning_rate": 9.466373421360442e-06,
      "loss": 1.5274,
      "step": 230
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 0.7830492854118347,
      "learning_rate": 9.45828432184221e-06,
      "loss": 1.4588,
      "step": 231
    },
    {
      "epoch": 0.947906026557712,
      "grad_norm": 0.829025149345398,
      "learning_rate": 9.450137882173385e-06,
      "loss": 1.6432,
      "step": 232
    },
    {
      "epoch": 0.9519918283963228,
      "grad_norm": 0.909126341342926,
      "learning_rate": 9.441934207129263e-06,
      "loss": 1.6836,
      "step": 233
    },
    {
      "epoch": 0.9560776302349336,
      "grad_norm": 0.7756729125976562,
      "learning_rate": 9.433673402221275e-06,
      "loss": 1.5496,
      "step": 234
    },
    {
      "epoch": 0.9601634320735445,
      "grad_norm": 0.8532222509384155,
      "learning_rate": 9.425355573695628e-06,
      "loss": 1.5463,
      "step": 235
    },
    {
      "epoch": 0.9642492339121552,
      "grad_norm": 0.8349085450172424,
      "learning_rate": 9.416980828531944e-06,
      "loss": 1.4966,
      "step": 236
    },
    {
      "epoch": 0.9683350357507661,
      "grad_norm": 0.8697748780250549,
      "learning_rate": 9.40854927444186e-06,
      "loss": 1.7189,
      "step": 237
    },
    {
      "epoch": 0.9724208375893769,
      "grad_norm": 1.0067797899246216,
      "learning_rate": 9.40006101986768e-06,
      "loss": 1.5666,
      "step": 238
    },
    {
      "epoch": 0.9765066394279878,
      "grad_norm": 0.8448037505149841,
      "learning_rate": 9.391516173980942e-06,
      "loss": 1.6128,
      "step": 239
    },
    {
      "epoch": 0.9805924412665986,
      "grad_norm": 0.9429301023483276,
      "learning_rate": 9.382914846681049e-06,
      "loss": 1.5469,
      "step": 240
    },
    {
      "epoch": 0.9846782431052093,
      "grad_norm": 0.7961666584014893,
      "learning_rate": 9.374257148593824e-06,
      "loss": 1.5416,
      "step": 241
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 0.8371102213859558,
      "learning_rate": 9.365543191070114e-06,
      "loss": 1.6245,
      "step": 242
    },
    {
      "epoch": 0.992849846782431,
      "grad_norm": 0.8211541175842285,
      "learning_rate": 9.356773086184338e-06,
      "loss": 1.4769,
      "step": 243
    },
    {
      "epoch": 0.9969356486210419,
      "grad_norm": 0.8450199365615845,
      "learning_rate": 9.347946946733055e-06,
      "loss": 1.5063,
      "step": 244
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8986855745315552,
      "learning_rate": 9.339064886233515e-06,
      "loss": 1.558,
      "step": 245
    },
    {
      "epoch": 1.0040858018386107,
      "grad_norm": 0.8158212304115295,
      "learning_rate": 9.330127018922195e-06,
      "loss": 1.4924,
      "step": 246
    },
    {
      "epoch": 1.0081716036772217,
      "grad_norm": 0.7308236956596375,
      "learning_rate": 9.321133459753322e-06,
      "loss": 1.4617,
      "step": 247
    },
    {
      "epoch": 1.0122574055158324,
      "grad_norm": 0.8010638356208801,
      "learning_rate": 9.312084324397416e-06,
      "loss": 1.5416,
      "step": 248
    },
    {
      "epoch": 1.0163432073544434,
      "grad_norm": 0.7734800577163696,
      "learning_rate": 9.30297972923978e-06,
      "loss": 1.4439,
      "step": 249
    },
    {
      "epoch": 1.0204290091930541,
      "grad_norm": 0.8287535905838013,
      "learning_rate": 9.293819791379016e-06,
      "loss": 1.5042,
      "step": 250
    },
    {
      "epoch": 1.0245148110316649,
      "grad_norm": 0.9475878477096558,
      "learning_rate": 9.284604628625514e-06,
      "loss": 1.654,
      "step": 251
    },
    {
      "epoch": 1.0286006128702758,
      "grad_norm": 0.7581089735031128,
      "learning_rate": 9.275334359499936e-06,
      "loss": 1.5769,
      "step": 252
    },
    {
      "epoch": 1.0326864147088866,
      "grad_norm": 0.833729088306427,
      "learning_rate": 9.266009103231702e-06,
      "loss": 1.5287,
      "step": 253
    },
    {
      "epoch": 1.0367722165474975,
      "grad_norm": 0.8135689496994019,
      "learning_rate": 9.256628979757435e-06,
      "loss": 1.561,
      "step": 254
    },
    {
      "epoch": 1.0408580183861083,
      "grad_norm": 0.877798318862915,
      "learning_rate": 9.247194109719446e-06,
      "loss": 1.5281,
      "step": 255
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 0.80463707447052,
      "learning_rate": 9.237704614464157e-06,
      "loss": 1.5551,
      "step": 256
    },
    {
      "epoch": 1.04902962206333,
      "grad_norm": 0.8087096214294434,
      "learning_rate": 9.228160616040555e-06,
      "loss": 1.5665,
      "step": 257
    },
    {
      "epoch": 1.0531154239019407,
      "grad_norm": 0.7685936689376831,
      "learning_rate": 9.218562237198624e-06,
      "loss": 1.4297,
      "step": 258
    },
    {
      "epoch": 1.0572012257405516,
      "grad_norm": 0.7632666230201721,
      "learning_rate": 9.208909601387748e-06,
      "loss": 1.5218,
      "step": 259
    },
    {
      "epoch": 1.0612870275791624,
      "grad_norm": 0.802725613117218,
      "learning_rate": 9.19920283275515e-06,
      "loss": 1.5163,
      "step": 260
    },
    {
      "epoch": 1.0653728294177733,
      "grad_norm": 0.8542250990867615,
      "learning_rate": 9.189442056144271e-06,
      "loss": 1.5225,
      "step": 261
    },
    {
      "epoch": 1.069458631256384,
      "grad_norm": 0.7980052828788757,
      "learning_rate": 9.179627397093184e-06,
      "loss": 1.5216,
      "step": 262
    },
    {
      "epoch": 1.0735444330949948,
      "grad_norm": 0.7846449017524719,
      "learning_rate": 9.169758981832964e-06,
      "loss": 1.5577,
      "step": 263
    },
    {
      "epoch": 1.0776302349336058,
      "grad_norm": 0.8401265740394592,
      "learning_rate": 9.15983693728607e-06,
      "loss": 1.4495,
      "step": 264
    },
    {
      "epoch": 1.0817160367722165,
      "grad_norm": 0.7411726713180542,
      "learning_rate": 9.149861391064714e-06,
      "loss": 1.4884,
      "step": 265
    },
    {
      "epoch": 1.0858018386108275,
      "grad_norm": 0.8734695315361023,
      "learning_rate": 9.139832471469224e-06,
      "loss": 1.5144,
      "step": 266
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 0.9423707723617554,
      "learning_rate": 9.12975030748638e-06,
      "loss": 1.6712,
      "step": 267
    },
    {
      "epoch": 1.093973442288049,
      "grad_norm": 0.8205363750457764,
      "learning_rate": 9.119615028787771e-06,
      "loss": 1.5803,
      "step": 268
    },
    {
      "epoch": 1.09805924412666,
      "grad_norm": 0.7784472107887268,
      "learning_rate": 9.10942676572812e-06,
      "loss": 1.5074,
      "step": 269
    },
    {
      "epoch": 1.1021450459652706,
      "grad_norm": 0.8008567690849304,
      "learning_rate": 9.0991856493436e-06,
      "loss": 1.4687,
      "step": 270
    },
    {
      "epoch": 1.1062308478038816,
      "grad_norm": 0.8330549001693726,
      "learning_rate": 9.088891811350164e-06,
      "loss": 1.4861,
      "step": 271
    },
    {
      "epoch": 1.1103166496424923,
      "grad_norm": 0.7925741672515869,
      "learning_rate": 9.07854538414184e-06,
      "loss": 1.5611,
      "step": 272
    },
    {
      "epoch": 1.1144024514811033,
      "grad_norm": 0.8315894603729248,
      "learning_rate": 9.06814650078903e-06,
      "loss": 1.4283,
      "step": 273
    },
    {
      "epoch": 1.118488253319714,
      "grad_norm": 0.7448769807815552,
      "learning_rate": 9.057695295036806e-06,
      "loss": 1.3893,
      "step": 274
    },
    {
      "epoch": 1.1225740551583248,
      "grad_norm": 0.8177236318588257,
      "learning_rate": 9.047191901303176e-06,
      "loss": 1.512,
      "step": 275
    },
    {
      "epoch": 1.1266598569969357,
      "grad_norm": 0.7749916315078735,
      "learning_rate": 9.036636454677363e-06,
      "loss": 1.6248,
      "step": 276
    },
    {
      "epoch": 1.1307456588355465,
      "grad_norm": 0.7649291753768921,
      "learning_rate": 9.026029090918076e-06,
      "loss": 1.5051,
      "step": 277
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 0.7827522158622742,
      "learning_rate": 9.015369946451749e-06,
      "loss": 1.5133,
      "step": 278
    },
    {
      "epoch": 1.1389172625127681,
      "grad_norm": 0.7906701564788818,
      "learning_rate": 9.004659158370788e-06,
      "loss": 1.5255,
      "step": 279
    },
    {
      "epoch": 1.1430030643513789,
      "grad_norm": 0.8074409365653992,
      "learning_rate": 8.993896864431825e-06,
      "loss": 1.6319,
      "step": 280
    },
    {
      "epoch": 1.1470888661899898,
      "grad_norm": 0.8793272972106934,
      "learning_rate": 8.983083203053924e-06,
      "loss": 1.5358,
      "step": 281
    },
    {
      "epoch": 1.1511746680286006,
      "grad_norm": 0.8079075217247009,
      "learning_rate": 8.972218313316812e-06,
      "loss": 1.4518,
      "step": 282
    },
    {
      "epoch": 1.1552604698672115,
      "grad_norm": 0.7733301520347595,
      "learning_rate": 8.96130233495909e-06,
      "loss": 1.4323,
      "step": 283
    },
    {
      "epoch": 1.1593462717058223,
      "grad_norm": 0.790723443031311,
      "learning_rate": 8.950335408376438e-06,
      "loss": 1.5589,
      "step": 284
    },
    {
      "epoch": 1.163432073544433,
      "grad_norm": 0.7364273071289062,
      "learning_rate": 8.939317674619797e-06,
      "loss": 1.5209,
      "step": 285
    },
    {
      "epoch": 1.167517875383044,
      "grad_norm": 0.9020221829414368,
      "learning_rate": 8.928249275393572e-06,
      "loss": 1.5173,
      "step": 286
    },
    {
      "epoch": 1.1716036772216547,
      "grad_norm": 0.7394547462463379,
      "learning_rate": 8.917130353053798e-06,
      "loss": 1.5851,
      "step": 287
    },
    {
      "epoch": 1.1756894790602657,
      "grad_norm": 0.8539962768554688,
      "learning_rate": 8.905961050606311e-06,
      "loss": 1.5613,
      "step": 288
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 0.8378245830535889,
      "learning_rate": 8.894741511704911e-06,
      "loss": 1.5371,
      "step": 289
    },
    {
      "epoch": 1.1838610827374871,
      "grad_norm": 0.874537467956543,
      "learning_rate": 8.883471880649514e-06,
      "loss": 1.5442,
      "step": 290
    },
    {
      "epoch": 1.187946884576098,
      "grad_norm": 0.8247148990631104,
      "learning_rate": 8.872152302384294e-06,
      "loss": 1.5779,
      "step": 291
    },
    {
      "epoch": 1.1920326864147088,
      "grad_norm": 0.7660832405090332,
      "learning_rate": 8.860782922495821e-06,
      "loss": 1.4738,
      "step": 292
    },
    {
      "epoch": 1.1961184882533198,
      "grad_norm": 0.8574827909469604,
      "learning_rate": 8.84936388721119e-06,
      "loss": 1.4742,
      "step": 293
    },
    {
      "epoch": 1.2002042900919305,
      "grad_norm": 0.7999985218048096,
      "learning_rate": 8.837895343396132e-06,
      "loss": 1.6662,
      "step": 294
    },
    {
      "epoch": 1.2042900919305413,
      "grad_norm": 0.9391121864318848,
      "learning_rate": 8.826377438553138e-06,
      "loss": 1.5503,
      "step": 295
    },
    {
      "epoch": 1.2083758937691522,
      "grad_norm": 0.8536441326141357,
      "learning_rate": 8.814810320819551e-06,
      "loss": 1.5576,
      "step": 296
    },
    {
      "epoch": 1.212461695607763,
      "grad_norm": 0.9065053462982178,
      "learning_rate": 8.803194138965665e-06,
      "loss": 1.6257,
      "step": 297
    },
    {
      "epoch": 1.216547497446374,
      "grad_norm": 0.7807546257972717,
      "learning_rate": 8.791529042392813e-06,
      "loss": 1.4919,
      "step": 298
    },
    {
      "epoch": 1.2206332992849847,
      "grad_norm": 0.7594956159591675,
      "learning_rate": 8.779815181131444e-06,
      "loss": 1.4702,
      "step": 299
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 0.7585300803184509,
      "learning_rate": 8.76805270583919e-06,
      "loss": 1.5555,
      "step": 300
    },
    {
      "epoch": 1.2288049029622063,
      "grad_norm": 0.7367005348205566,
      "learning_rate": 8.756241767798934e-06,
      "loss": 1.3994,
      "step": 301
    },
    {
      "epoch": 1.232890704800817,
      "grad_norm": 0.8184664845466614,
      "learning_rate": 8.744382518916866e-06,
      "loss": 1.6789,
      "step": 302
    },
    {
      "epoch": 1.236976506639428,
      "grad_norm": 0.7969249486923218,
      "learning_rate": 8.732475111720513e-06,
      "loss": 1.5229,
      "step": 303
    },
    {
      "epoch": 1.2410623084780388,
      "grad_norm": 0.7626066207885742,
      "learning_rate": 8.720519699356804e-06,
      "loss": 1.5233,
      "step": 304
    },
    {
      "epoch": 1.2451481103166497,
      "grad_norm": 0.804427981376648,
      "learning_rate": 8.708516435590075e-06,
      "loss": 1.5625,
      "step": 305
    },
    {
      "epoch": 1.2492339121552605,
      "grad_norm": 0.8922007083892822,
      "learning_rate": 8.696465474800109e-06,
      "loss": 1.517,
      "step": 306
    },
    {
      "epoch": 1.2533197139938714,
      "grad_norm": 0.8083730340003967,
      "learning_rate": 8.684366971980139e-06,
      "loss": 1.5177,
      "step": 307
    },
    {
      "epoch": 1.2574055158324822,
      "grad_norm": 0.813470184803009,
      "learning_rate": 8.672221082734857e-06,
      "loss": 1.5744,
      "step": 308
    },
    {
      "epoch": 1.261491317671093,
      "grad_norm": 0.7859700918197632,
      "learning_rate": 8.660027963278421e-06,
      "loss": 1.5099,
      "step": 309
    },
    {
      "epoch": 1.2655771195097039,
      "grad_norm": 0.751887857913971,
      "learning_rate": 8.647787770432439e-06,
      "loss": 1.5092,
      "step": 310
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 0.8037948608398438,
      "learning_rate": 8.635500661623946e-06,
      "loss": 1.4617,
      "step": 311
    },
    {
      "epoch": 1.2737487231869253,
      "grad_norm": 0.842064380645752,
      "learning_rate": 8.623166794883393e-06,
      "loss": 1.4511,
      "step": 312
    },
    {
      "epoch": 1.2778345250255363,
      "grad_norm": 0.7675585746765137,
      "learning_rate": 8.610786328842602e-06,
      "loss": 1.4774,
      "step": 313
    },
    {
      "epoch": 1.281920326864147,
      "grad_norm": 0.7021605372428894,
      "learning_rate": 8.598359422732736e-06,
      "loss": 1.4887,
      "step": 314
    },
    {
      "epoch": 1.286006128702758,
      "grad_norm": 0.8544597625732422,
      "learning_rate": 8.585886236382244e-06,
      "loss": 1.4611,
      "step": 315
    },
    {
      "epoch": 1.2900919305413687,
      "grad_norm": 0.754283607006073,
      "learning_rate": 8.573366930214807e-06,
      "loss": 1.5539,
      "step": 316
    },
    {
      "epoch": 1.2941777323799797,
      "grad_norm": 0.8407593965530396,
      "learning_rate": 8.560801665247273e-06,
      "loss": 1.4956,
      "step": 317
    },
    {
      "epoch": 1.2982635342185904,
      "grad_norm": 0.7430266737937927,
      "learning_rate": 8.548190603087594e-06,
      "loss": 1.6089,
      "step": 318
    },
    {
      "epoch": 1.3023493360572012,
      "grad_norm": 0.808535635471344,
      "learning_rate": 8.535533905932739e-06,
      "loss": 1.5348,
      "step": 319
    },
    {
      "epoch": 1.3064351378958121,
      "grad_norm": 0.8748639225959778,
      "learning_rate": 8.522831736566607e-06,
      "loss": 1.5126,
      "step": 320
    },
    {
      "epoch": 1.3105209397344229,
      "grad_norm": 0.8359003663063049,
      "learning_rate": 8.510084258357947e-06,
      "loss": 1.6569,
      "step": 321
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 0.861433744430542,
      "learning_rate": 8.497291635258235e-06,
      "loss": 1.468,
      "step": 322
    },
    {
      "epoch": 1.3186925434116445,
      "grad_norm": 0.8519131541252136,
      "learning_rate": 8.484454031799588e-06,
      "loss": 1.487,
      "step": 323
    },
    {
      "epoch": 1.3227783452502553,
      "grad_norm": 0.8033623099327087,
      "learning_rate": 8.471571613092626e-06,
      "loss": 1.4889,
      "step": 324
    },
    {
      "epoch": 1.3268641470888662,
      "grad_norm": 0.7894397377967834,
      "learning_rate": 8.458644544824371e-06,
      "loss": 1.5257,
      "step": 325
    },
    {
      "epoch": 1.330949948927477,
      "grad_norm": 0.879136323928833,
      "learning_rate": 8.445672993256095e-06,
      "loss": 1.6276,
      "step": 326
    },
    {
      "epoch": 1.335035750766088,
      "grad_norm": 0.7890186309814453,
      "learning_rate": 8.4326571252212e-06,
      "loss": 1.6048,
      "step": 327
    },
    {
      "epoch": 1.3391215526046987,
      "grad_norm": 0.7880071997642517,
      "learning_rate": 8.419597108123054e-06,
      "loss": 1.4107,
      "step": 328
    },
    {
      "epoch": 1.3432073544433094,
      "grad_norm": 0.8717417120933533,
      "learning_rate": 8.406493109932856e-06,
      "loss": 1.454,
      "step": 329
    },
    {
      "epoch": 1.3472931562819204,
      "grad_norm": 0.7909759283065796,
      "learning_rate": 8.393345299187463e-06,
      "loss": 1.5573,
      "step": 330
    },
    {
      "epoch": 1.351378958120531,
      "grad_norm": 0.7730523347854614,
      "learning_rate": 8.380153844987225e-06,
      "loss": 1.4424,
      "step": 331
    },
    {
      "epoch": 1.355464759959142,
      "grad_norm": 0.7944605946540833,
      "learning_rate": 8.366918916993817e-06,
      "loss": 1.6493,
      "step": 332
    },
    {
      "epoch": 1.3595505617977528,
      "grad_norm": 0.8007334470748901,
      "learning_rate": 8.353640685428045e-06,
      "loss": 1.4988,
      "step": 333
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.8276387453079224,
      "learning_rate": 8.340319321067668e-06,
      "loss": 1.6772,
      "step": 334
    },
    {
      "epoch": 1.3677221654749745,
      "grad_norm": 0.8055758476257324,
      "learning_rate": 8.326954995245194e-06,
      "loss": 1.4727,
      "step": 335
    },
    {
      "epoch": 1.3718079673135852,
      "grad_norm": 0.8051984906196594,
      "learning_rate": 8.313547879845682e-06,
      "loss": 1.46,
      "step": 336
    },
    {
      "epoch": 1.3758937691521962,
      "grad_norm": 0.7948698997497559,
      "learning_rate": 8.300098147304523e-06,
      "loss": 1.4501,
      "step": 337
    },
    {
      "epoch": 1.379979570990807,
      "grad_norm": 0.7521846294403076,
      "learning_rate": 8.286605970605234e-06,
      "loss": 1.4299,
      "step": 338
    },
    {
      "epoch": 1.3840653728294177,
      "grad_norm": 0.7203874588012695,
      "learning_rate": 8.273071523277223e-06,
      "loss": 1.4115,
      "step": 339
    },
    {
      "epoch": 1.3881511746680286,
      "grad_norm": 0.8733167052268982,
      "learning_rate": 8.259494979393563e-06,
      "loss": 1.5369,
      "step": 340
    },
    {
      "epoch": 1.3922369765066394,
      "grad_norm": 0.8101521730422974,
      "learning_rate": 8.245876513568749e-06,
      "loss": 1.4088,
      "step": 341
    },
    {
      "epoch": 1.3963227783452503,
      "grad_norm": 0.7971808910369873,
      "learning_rate": 8.232216300956457e-06,
      "loss": 1.4014,
      "step": 342
    },
    {
      "epoch": 1.400408580183861,
      "grad_norm": 0.7763895988464355,
      "learning_rate": 8.218514517247287e-06,
      "loss": 1.5291,
      "step": 343
    },
    {
      "epoch": 1.404494382022472,
      "grad_norm": 0.8161207437515259,
      "learning_rate": 8.204771338666504e-06,
      "loss": 1.4547,
      "step": 344
    },
    {
      "epoch": 1.4085801838610827,
      "grad_norm": 0.8467191457748413,
      "learning_rate": 8.190986941971773e-06,
      "loss": 1.4073,
      "step": 345
    },
    {
      "epoch": 1.4126659856996935,
      "grad_norm": 0.812650203704834,
      "learning_rate": 8.177161504450887e-06,
      "loss": 1.5296,
      "step": 346
    },
    {
      "epoch": 1.4167517875383044,
      "grad_norm": 0.8035727739334106,
      "learning_rate": 8.163295203919486e-06,
      "loss": 1.476,
      "step": 347
    },
    {
      "epoch": 1.4208375893769152,
      "grad_norm": 0.8378661274909973,
      "learning_rate": 8.149388218718763e-06,
      "loss": 1.5057,
      "step": 348
    },
    {
      "epoch": 1.424923391215526,
      "grad_norm": 0.7915577292442322,
      "learning_rate": 8.135440727713179e-06,
      "loss": 1.4347,
      "step": 349
    },
    {
      "epoch": 1.4290091930541369,
      "grad_norm": 0.7738305926322937,
      "learning_rate": 8.121452910288164e-06,
      "loss": 1.433,
      "step": 350
    },
    {
      "epoch": 1.4330949948927478,
      "grad_norm": 0.8240776658058167,
      "learning_rate": 8.107424946347801e-06,
      "loss": 1.4574,
      "step": 351
    },
    {
      "epoch": 1.4371807967313586,
      "grad_norm": 0.7924173474311829,
      "learning_rate": 8.093357016312518e-06,
      "loss": 1.4359,
      "step": 352
    },
    {
      "epoch": 1.4412665985699693,
      "grad_norm": 0.9301562905311584,
      "learning_rate": 8.079249301116765e-06,
      "loss": 1.6351,
      "step": 353
    },
    {
      "epoch": 1.4453524004085803,
      "grad_norm": 0.8300731182098389,
      "learning_rate": 8.06510198220669e-06,
      "loss": 1.5462,
      "step": 354
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 0.7852367758750916,
      "learning_rate": 8.050915241537802e-06,
      "loss": 1.4817,
      "step": 355
    },
    {
      "epoch": 1.4535240040858017,
      "grad_norm": 0.8947411179542542,
      "learning_rate": 8.036689261572636e-06,
      "loss": 1.5216,
      "step": 356
    },
    {
      "epoch": 1.4576098059244127,
      "grad_norm": 0.8475333452224731,
      "learning_rate": 8.022424225278394e-06,
      "loss": 1.5259,
      "step": 357
    },
    {
      "epoch": 1.4616956077630234,
      "grad_norm": 0.8591018915176392,
      "learning_rate": 8.008120316124612e-06,
      "loss": 1.5419,
      "step": 358
    },
    {
      "epoch": 1.4657814096016344,
      "grad_norm": 0.7490316033363342,
      "learning_rate": 7.993777718080782e-06,
      "loss": 1.5127,
      "step": 359
    },
    {
      "epoch": 1.4698672114402451,
      "grad_norm": 0.8247952461242676,
      "learning_rate": 7.97939661561399e-06,
      "loss": 1.4781,
      "step": 360
    },
    {
      "epoch": 1.473953013278856,
      "grad_norm": 0.825117826461792,
      "learning_rate": 7.964977193686551e-06,
      "loss": 1.4638,
      "step": 361
    },
    {
      "epoch": 1.4780388151174668,
      "grad_norm": 0.8420026898384094,
      "learning_rate": 7.950519637753623e-06,
      "loss": 1.5675,
      "step": 362
    },
    {
      "epoch": 1.4821246169560776,
      "grad_norm": 0.8234542012214661,
      "learning_rate": 7.936024133760825e-06,
      "loss": 1.5183,
      "step": 363
    },
    {
      "epoch": 1.4862104187946885,
      "grad_norm": 0.8512513041496277,
      "learning_rate": 7.921490868141843e-06,
      "loss": 1.5536,
      "step": 364
    },
    {
      "epoch": 1.4902962206332993,
      "grad_norm": 0.8005079627037048,
      "learning_rate": 7.906920027816034e-06,
      "loss": 1.4661,
      "step": 365
    },
    {
      "epoch": 1.49438202247191,
      "grad_norm": 0.7688696384429932,
      "learning_rate": 7.89231180018602e-06,
      "loss": 1.5462,
      "step": 366
    },
    {
      "epoch": 1.498467824310521,
      "grad_norm": 0.8760764002799988,
      "learning_rate": 7.877666373135287e-06,
      "loss": 1.4788,
      "step": 367
    },
    {
      "epoch": 1.502553626149132,
      "grad_norm": 0.7730899453163147,
      "learning_rate": 7.862983935025745e-06,
      "loss": 1.5326,
      "step": 368
    },
    {
      "epoch": 1.5066394279877426,
      "grad_norm": 0.7986984252929688,
      "learning_rate": 7.848264674695338e-06,
      "loss": 1.4203,
      "step": 369
    },
    {
      "epoch": 1.5107252298263534,
      "grad_norm": 0.8527814745903015,
      "learning_rate": 7.83350878145559e-06,
      "loss": 1.4705,
      "step": 370
    },
    {
      "epoch": 1.5148110316649643,
      "grad_norm": 0.7956666350364685,
      "learning_rate": 7.818716445089177e-06,
      "loss": 1.4976,
      "step": 371
    },
    {
      "epoch": 1.518896833503575,
      "grad_norm": 0.7266256809234619,
      "learning_rate": 7.80388785584749e-06,
      "loss": 1.4946,
      "step": 372
    },
    {
      "epoch": 1.5229826353421858,
      "grad_norm": 0.9460698962211609,
      "learning_rate": 7.789023204448189e-06,
      "loss": 1.6005,
      "step": 373
    },
    {
      "epoch": 1.5270684371807968,
      "grad_norm": 0.758091390132904,
      "learning_rate": 7.774122682072743e-06,
      "loss": 1.5159,
      "step": 374
    },
    {
      "epoch": 1.5311542390194075,
      "grad_norm": 0.8327276706695557,
      "learning_rate": 7.759186480363974e-06,
      "loss": 1.5752,
      "step": 375
    },
    {
      "epoch": 1.5352400408580182,
      "grad_norm": 0.8294745087623596,
      "learning_rate": 7.744214791423597e-06,
      "loss": 1.4798,
      "step": 376
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 0.7205362319946289,
      "learning_rate": 7.729207807809743e-06,
      "loss": 1.4719,
      "step": 377
    },
    {
      "epoch": 1.5434116445352402,
      "grad_norm": 1.0108789205551147,
      "learning_rate": 7.714165722534485e-06,
      "loss": 1.5453,
      "step": 378
    },
    {
      "epoch": 1.547497446373851,
      "grad_norm": 0.8491654396057129,
      "learning_rate": 7.699088729061355e-06,
      "loss": 1.5566,
      "step": 379
    },
    {
      "epoch": 1.5515832482124616,
      "grad_norm": 0.7721633911132812,
      "learning_rate": 7.68397702130286e-06,
      "loss": 1.4093,
      "step": 380
    },
    {
      "epoch": 1.5556690500510726,
      "grad_norm": 1.0031704902648926,
      "learning_rate": 7.668830793617976e-06,
      "loss": 1.5286,
      "step": 381
    },
    {
      "epoch": 1.5597548518896833,
      "grad_norm": 0.7971066236495972,
      "learning_rate": 7.653650240809667e-06,
      "loss": 1.5055,
      "step": 382
    },
    {
      "epoch": 1.563840653728294,
      "grad_norm": 0.759024977684021,
      "learning_rate": 7.63843555812236e-06,
      "loss": 1.505,
      "step": 383
    },
    {
      "epoch": 1.567926455566905,
      "grad_norm": 0.7666231989860535,
      "learning_rate": 7.6231869412394495e-06,
      "loss": 1.4114,
      "step": 384
    },
    {
      "epoch": 1.572012257405516,
      "grad_norm": 0.8227419257164001,
      "learning_rate": 7.60790458628077e-06,
      "loss": 1.554,
      "step": 385
    },
    {
      "epoch": 1.5760980592441267,
      "grad_norm": 0.7599053382873535,
      "learning_rate": 7.592588689800077e-06,
      "loss": 1.3961,
      "step": 386
    },
    {
      "epoch": 1.5801838610827375,
      "grad_norm": 0.9195758104324341,
      "learning_rate": 7.577239448782523e-06,
      "loss": 1.5089,
      "step": 387
    },
    {
      "epoch": 1.5842696629213484,
      "grad_norm": 0.8005908131599426,
      "learning_rate": 7.56185706064212e-06,
      "loss": 1.4847,
      "step": 388
    },
    {
      "epoch": 1.5883554647599591,
      "grad_norm": 0.7134295105934143,
      "learning_rate": 7.546441723219198e-06,
      "loss": 1.4002,
      "step": 389
    },
    {
      "epoch": 1.5924412665985699,
      "grad_norm": 0.8225773572921753,
      "learning_rate": 7.530993634777868e-06,
      "loss": 1.5159,
      "step": 390
    },
    {
      "epoch": 1.5965270684371808,
      "grad_norm": 0.7929695248603821,
      "learning_rate": 7.5155129940034675e-06,
      "loss": 1.547,
      "step": 391
    },
    {
      "epoch": 1.6006128702757916,
      "grad_norm": 0.8277504444122314,
      "learning_rate": 7.500000000000001e-06,
      "loss": 1.5322,
      "step": 392
    },
    {
      "epoch": 1.6006128702757916,
      "eval_loss": 1.468184471130371,
      "eval_runtime": 18.6594,
      "eval_samples_per_second": 11.094,
      "eval_steps_per_second": 2.787,
      "step": 392
    },
    {
      "epoch": 1.6046986721144023,
      "grad_norm": 0.8096811175346375,
      "learning_rate": 7.484454852287586e-06,
      "loss": 1.5174,
      "step": 393
    },
    {
      "epoch": 1.6087844739530133,
      "grad_norm": 0.8587949275970459,
      "learning_rate": 7.468877750799887e-06,
      "loss": 1.4441,
      "step": 394
    },
    {
      "epoch": 1.6128702757916242,
      "grad_norm": 0.938616931438446,
      "learning_rate": 7.453268895881537e-06,
      "loss": 1.4792,
      "step": 395
    },
    {
      "epoch": 1.616956077630235,
      "grad_norm": 0.8473846316337585,
      "learning_rate": 7.437628488285568e-06,
      "loss": 1.429,
      "step": 396
    },
    {
      "epoch": 1.6210418794688457,
      "grad_norm": 0.8548424243927002,
      "learning_rate": 7.421956729170823e-06,
      "loss": 1.444,
      "step": 397
    },
    {
      "epoch": 1.6251276813074567,
      "grad_norm": 0.7680855989456177,
      "learning_rate": 7.40625382009938e-06,
      "loss": 1.4965,
      "step": 398
    },
    {
      "epoch": 1.6292134831460674,
      "grad_norm": 0.7970052361488342,
      "learning_rate": 7.390519963033942e-06,
      "loss": 1.5082,
      "step": 399
    },
    {
      "epoch": 1.6332992849846781,
      "grad_norm": 0.8279876708984375,
      "learning_rate": 7.374755360335253e-06,
      "loss": 1.557,
      "step": 400
    },
    {
      "epoch": 1.637385086823289,
      "grad_norm": 0.8594110012054443,
      "learning_rate": 7.358960214759492e-06,
      "loss": 1.3784,
      "step": 401
    },
    {
      "epoch": 1.6414708886619,
      "grad_norm": 0.7372906804084778,
      "learning_rate": 7.343134729455667e-06,
      "loss": 1.4283,
      "step": 402
    },
    {
      "epoch": 1.6455566905005106,
      "grad_norm": 0.7802755236625671,
      "learning_rate": 7.327279107962995e-06,
      "loss": 1.3508,
      "step": 403
    },
    {
      "epoch": 1.6496424923391215,
      "grad_norm": 0.6996581554412842,
      "learning_rate": 7.311393554208292e-06,
      "loss": 1.4353,
      "step": 404
    },
    {
      "epoch": 1.6537282941777325,
      "grad_norm": 0.8215770125389099,
      "learning_rate": 7.295478272503347e-06,
      "loss": 1.4911,
      "step": 405
    },
    {
      "epoch": 1.6578140960163432,
      "grad_norm": 0.8594485521316528,
      "learning_rate": 7.279533467542295e-06,
      "loss": 1.6251,
      "step": 406
    },
    {
      "epoch": 1.661899897854954,
      "grad_norm": 0.7741272449493408,
      "learning_rate": 7.2635593443989814e-06,
      "loss": 1.4638,
      "step": 407
    },
    {
      "epoch": 1.665985699693565,
      "grad_norm": 0.8204206824302673,
      "learning_rate": 7.24755610852433e-06,
      "loss": 1.4544,
      "step": 408
    },
    {
      "epoch": 1.6700715015321757,
      "grad_norm": 0.7870428562164307,
      "learning_rate": 7.2315239657436955e-06,
      "loss": 1.5119,
      "step": 409
    },
    {
      "epoch": 1.6741573033707864,
      "grad_norm": 0.8265226483345032,
      "learning_rate": 7.2154631222542205e-06,
      "loss": 1.5045,
      "step": 410
    },
    {
      "epoch": 1.6782431052093973,
      "grad_norm": 0.860124945640564,
      "learning_rate": 7.199373784622178e-06,
      "loss": 1.5328,
      "step": 411
    },
    {
      "epoch": 1.6823289070480083,
      "grad_norm": 0.8167992234230042,
      "learning_rate": 7.183256159780321e-06,
      "loss": 1.5701,
      "step": 412
    },
    {
      "epoch": 1.686414708886619,
      "grad_norm": 0.9587753415107727,
      "learning_rate": 7.167110455025214e-06,
      "loss": 1.4978,
      "step": 413
    },
    {
      "epoch": 1.6905005107252298,
      "grad_norm": 0.8179387450218201,
      "learning_rate": 7.150936878014578e-06,
      "loss": 1.5088,
      "step": 414
    },
    {
      "epoch": 1.6945863125638407,
      "grad_norm": 0.8254221677780151,
      "learning_rate": 7.134735636764606e-06,
      "loss": 1.3851,
      "step": 415
    },
    {
      "epoch": 1.6986721144024515,
      "grad_norm": 0.8247527480125427,
      "learning_rate": 7.118506939647295e-06,
      "loss": 1.5815,
      "step": 416
    },
    {
      "epoch": 1.7027579162410622,
      "grad_norm": 0.7893041968345642,
      "learning_rate": 7.102250995387767e-06,
      "loss": 1.5139,
      "step": 417
    },
    {
      "epoch": 1.7068437180796732,
      "grad_norm": 0.7710625529289246,
      "learning_rate": 7.085968013061585e-06,
      "loss": 1.3725,
      "step": 418
    },
    {
      "epoch": 1.7109295199182841,
      "grad_norm": 0.7905916571617126,
      "learning_rate": 7.069658202092056e-06,
      "loss": 1.4529,
      "step": 419
    },
    {
      "epoch": 1.7150153217568946,
      "grad_norm": 0.7867981195449829,
      "learning_rate": 7.053321772247546e-06,
      "loss": 1.4413,
      "step": 420
    },
    {
      "epoch": 1.7191011235955056,
      "grad_norm": 0.7845027446746826,
      "learning_rate": 7.036958933638779e-06,
      "loss": 1.4566,
      "step": 421
    },
    {
      "epoch": 1.7231869254341166,
      "grad_norm": 0.7521723508834839,
      "learning_rate": 7.020569896716137e-06,
      "loss": 1.4955,
      "step": 422
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.7882096767425537,
      "learning_rate": 7.00415487226695e-06,
      "loss": 1.5679,
      "step": 423
    },
    {
      "epoch": 1.731358529111338,
      "grad_norm": 0.7825708985328674,
      "learning_rate": 6.987714071412781e-06,
      "loss": 1.4377,
      "step": 424
    },
    {
      "epoch": 1.735444330949949,
      "grad_norm": 0.790661096572876,
      "learning_rate": 6.971247705606723e-06,
      "loss": 1.4175,
      "step": 425
    },
    {
      "epoch": 1.7395301327885597,
      "grad_norm": 0.8005431890487671,
      "learning_rate": 6.9547559866306695e-06,
      "loss": 1.5093,
      "step": 426
    },
    {
      "epoch": 1.7436159346271705,
      "grad_norm": 0.8137386441230774,
      "learning_rate": 6.938239126592592e-06,
      "loss": 1.5572,
      "step": 427
    },
    {
      "epoch": 1.7477017364657814,
      "grad_norm": 0.8061034679412842,
      "learning_rate": 6.9216973379238175e-06,
      "loss": 1.4461,
      "step": 428
    },
    {
      "epoch": 1.7517875383043924,
      "grad_norm": 0.7487004995346069,
      "learning_rate": 6.905130833376286e-06,
      "loss": 1.505,
      "step": 429
    },
    {
      "epoch": 1.7558733401430031,
      "grad_norm": 0.8036941289901733,
      "learning_rate": 6.888539826019824e-06,
      "loss": 1.4498,
      "step": 430
    },
    {
      "epoch": 1.7599591419816139,
      "grad_norm": 0.8062487840652466,
      "learning_rate": 6.871924529239404e-06,
      "loss": 1.5797,
      "step": 431
    },
    {
      "epoch": 1.7640449438202248,
      "grad_norm": 0.8720464110374451,
      "learning_rate": 6.855285156732389e-06,
      "loss": 1.3537,
      "step": 432
    },
    {
      "epoch": 1.7681307456588355,
      "grad_norm": 0.796607494354248,
      "learning_rate": 6.8386219225057945e-06,
      "loss": 1.4094,
      "step": 433
    },
    {
      "epoch": 1.7722165474974463,
      "grad_norm": 0.8626528978347778,
      "learning_rate": 6.821935040873538e-06,
      "loss": 1.5041,
      "step": 434
    },
    {
      "epoch": 1.7763023493360572,
      "grad_norm": 0.8847924470901489,
      "learning_rate": 6.805224726453672e-06,
      "loss": 1.5137,
      "step": 435
    },
    {
      "epoch": 1.780388151174668,
      "grad_norm": 0.787237286567688,
      "learning_rate": 6.788491194165629e-06,
      "loss": 1.4202,
      "step": 436
    },
    {
      "epoch": 1.7844739530132787,
      "grad_norm": 0.8023776412010193,
      "learning_rate": 6.77173465922746e-06,
      "loss": 1.4337,
      "step": 437
    },
    {
      "epoch": 1.7885597548518897,
      "grad_norm": 0.7890459299087524,
      "learning_rate": 6.754955337153066e-06,
      "loss": 1.4001,
      "step": 438
    },
    {
      "epoch": 1.7926455566905006,
      "grad_norm": 0.8324351906776428,
      "learning_rate": 6.738153443749421e-06,
      "loss": 1.5107,
      "step": 439
    },
    {
      "epoch": 1.7967313585291114,
      "grad_norm": 0.7804716229438782,
      "learning_rate": 6.721329195113802e-06,
      "loss": 1.5531,
      "step": 440
    },
    {
      "epoch": 1.800817160367722,
      "grad_norm": 0.756460964679718,
      "learning_rate": 6.704482807631004e-06,
      "loss": 1.3817,
      "step": 441
    },
    {
      "epoch": 1.804902962206333,
      "grad_norm": 0.7908895611763,
      "learning_rate": 6.687614497970567e-06,
      "loss": 1.5128,
      "step": 442
    },
    {
      "epoch": 1.8089887640449438,
      "grad_norm": 0.8520108461380005,
      "learning_rate": 6.670724483083977e-06,
      "loss": 1.4777,
      "step": 443
    },
    {
      "epoch": 1.8130745658835545,
      "grad_norm": 0.784737229347229,
      "learning_rate": 6.653812980201882e-06,
      "loss": 1.4442,
      "step": 444
    },
    {
      "epoch": 1.8171603677221655,
      "grad_norm": 0.7747485041618347,
      "learning_rate": 6.636880206831298e-06,
      "loss": 1.4849,
      "step": 445
    },
    {
      "epoch": 1.8212461695607765,
      "grad_norm": 0.8070043325424194,
      "learning_rate": 6.6199263807528136e-06,
      "loss": 1.4518,
      "step": 446
    },
    {
      "epoch": 1.825331971399387,
      "grad_norm": 0.8262349963188171,
      "learning_rate": 6.602951720017785e-06,
      "loss": 1.4426,
      "step": 447
    },
    {
      "epoch": 1.829417773237998,
      "grad_norm": 0.8646184802055359,
      "learning_rate": 6.585956442945531e-06,
      "loss": 1.4954,
      "step": 448
    },
    {
      "epoch": 1.8335035750766089,
      "grad_norm": 0.7425416707992554,
      "learning_rate": 6.568940768120529e-06,
      "loss": 1.4546,
      "step": 449
    },
    {
      "epoch": 1.8375893769152196,
      "grad_norm": 0.8200052976608276,
      "learning_rate": 6.551904914389601e-06,
      "loss": 1.5702,
      "step": 450
    },
    {
      "epoch": 1.8416751787538304,
      "grad_norm": 0.8488803505897522,
      "learning_rate": 6.534849100859101e-06,
      "loss": 1.4236,
      "step": 451
    },
    {
      "epoch": 1.8457609805924413,
      "grad_norm": 0.7637798190116882,
      "learning_rate": 6.5177735468920935e-06,
      "loss": 1.4237,
      "step": 452
    },
    {
      "epoch": 1.849846782431052,
      "grad_norm": 0.7972750663757324,
      "learning_rate": 6.500678472105535e-06,
      "loss": 1.5612,
      "step": 453
    },
    {
      "epoch": 1.8539325842696628,
      "grad_norm": 0.8331711292266846,
      "learning_rate": 6.483564096367452e-06,
      "loss": 1.5107,
      "step": 454
    },
    {
      "epoch": 1.8580183861082737,
      "grad_norm": 0.9890512228012085,
      "learning_rate": 6.466430639794105e-06,
      "loss": 1.5433,
      "step": 455
    },
    {
      "epoch": 1.8621041879468847,
      "grad_norm": 0.7646751999855042,
      "learning_rate": 6.449278322747164e-06,
      "loss": 1.4115,
      "step": 456
    },
    {
      "epoch": 1.8661899897854954,
      "grad_norm": 0.7846587300300598,
      "learning_rate": 6.432107365830872e-06,
      "loss": 1.5666,
      "step": 457
    },
    {
      "epoch": 1.8702757916241062,
      "grad_norm": 0.7564422488212585,
      "learning_rate": 6.414917989889212e-06,
      "loss": 1.5397,
      "step": 458
    },
    {
      "epoch": 1.8743615934627171,
      "grad_norm": 0.8977306485176086,
      "learning_rate": 6.39771041600306e-06,
      "loss": 1.4176,
      "step": 459
    },
    {
      "epoch": 1.8784473953013279,
      "grad_norm": 0.7944766879081726,
      "learning_rate": 6.380484865487346e-06,
      "loss": 1.3893,
      "step": 460
    },
    {
      "epoch": 1.8825331971399386,
      "grad_norm": 0.7919601798057556,
      "learning_rate": 6.3632415598882004e-06,
      "loss": 1.4729,
      "step": 461
    },
    {
      "epoch": 1.8866189989785496,
      "grad_norm": 0.8030367493629456,
      "learning_rate": 6.345980720980119e-06,
      "loss": 1.5214,
      "step": 462
    },
    {
      "epoch": 1.8907048008171605,
      "grad_norm": 0.8070223927497864,
      "learning_rate": 6.328702570763098e-06,
      "loss": 1.3337,
      "step": 463
    },
    {
      "epoch": 1.894790602655771,
      "grad_norm": 0.7925047874450684,
      "learning_rate": 6.311407331459781e-06,
      "loss": 1.4912,
      "step": 464
    },
    {
      "epoch": 1.898876404494382,
      "grad_norm": 0.8034906983375549,
      "learning_rate": 6.294095225512604e-06,
      "loss": 1.4131,
      "step": 465
    },
    {
      "epoch": 1.902962206332993,
      "grad_norm": 0.832209050655365,
      "learning_rate": 6.276766475580935e-06,
      "loss": 1.4689,
      "step": 466
    },
    {
      "epoch": 1.9070480081716037,
      "grad_norm": 0.847113311290741,
      "learning_rate": 6.259421304538207e-06,
      "loss": 1.4734,
      "step": 467
    },
    {
      "epoch": 1.9111338100102144,
      "grad_norm": 0.8094614148139954,
      "learning_rate": 6.242059935469051e-06,
      "loss": 1.4635,
      "step": 468
    },
    {
      "epoch": 1.9152196118488254,
      "grad_norm": 0.7947748899459839,
      "learning_rate": 6.224682591666431e-06,
      "loss": 1.441,
      "step": 469
    },
    {
      "epoch": 1.9193054136874361,
      "grad_norm": 0.8382415175437927,
      "learning_rate": 6.207289496628768e-06,
      "loss": 1.4771,
      "step": 470
    },
    {
      "epoch": 1.9233912155260469,
      "grad_norm": 0.8117170333862305,
      "learning_rate": 6.18988087405707e-06,
      "loss": 1.4552,
      "step": 471
    },
    {
      "epoch": 1.9274770173646578,
      "grad_norm": 0.798304557800293,
      "learning_rate": 6.1724569478520495e-06,
      "loss": 1.374,
      "step": 472
    },
    {
      "epoch": 1.9315628192032688,
      "grad_norm": 0.821649432182312,
      "learning_rate": 6.155017942111246e-06,
      "loss": 1.4781,
      "step": 473
    },
    {
      "epoch": 1.9356486210418795,
      "grad_norm": 0.7871229648590088,
      "learning_rate": 6.1375640811261486e-06,
      "loss": 1.3899,
      "step": 474
    },
    {
      "epoch": 1.9397344228804902,
      "grad_norm": 0.7953628897666931,
      "learning_rate": 6.120095589379299e-06,
      "loss": 1.5727,
      "step": 475
    },
    {
      "epoch": 1.9438202247191012,
      "grad_norm": 0.8006387948989868,
      "learning_rate": 6.102612691541422e-06,
      "loss": 1.4648,
      "step": 476
    },
    {
      "epoch": 1.947906026557712,
      "grad_norm": 0.8141502141952515,
      "learning_rate": 6.085115612468516e-06,
      "loss": 1.4707,
      "step": 477
    },
    {
      "epoch": 1.9519918283963227,
      "grad_norm": 0.8039414882659912,
      "learning_rate": 6.067604577198981e-06,
      "loss": 1.4359,
      "step": 478
    },
    {
      "epoch": 1.9560776302349336,
      "grad_norm": 0.778313398361206,
      "learning_rate": 6.0500798109507065e-06,
      "loss": 1.4472,
      "step": 479
    },
    {
      "epoch": 1.9601634320735446,
      "grad_norm": 0.8199807405471802,
      "learning_rate": 6.032541539118188e-06,
      "loss": 1.5435,
      "step": 480
    },
    {
      "epoch": 1.9642492339121551,
      "grad_norm": 0.9559541940689087,
      "learning_rate": 6.014989987269617e-06,
      "loss": 1.4644,
      "step": 481
    },
    {
      "epoch": 1.968335035750766,
      "grad_norm": 0.8902751207351685,
      "learning_rate": 5.997425381143994e-06,
      "loss": 1.4865,
      "step": 482
    },
    {
      "epoch": 1.972420837589377,
      "grad_norm": 0.8735816478729248,
      "learning_rate": 5.9798479466482106e-06,
      "loss": 1.5239,
      "step": 483
    },
    {
      "epoch": 1.9765066394279878,
      "grad_norm": 0.8058459162712097,
      "learning_rate": 5.96225790985415e-06,
      "loss": 1.4437,
      "step": 484
    },
    {
      "epoch": 1.9805924412665985,
      "grad_norm": 0.7943552732467651,
      "learning_rate": 5.944655496995783e-06,
      "loss": 1.476,
      "step": 485
    },
    {
      "epoch": 1.9846782431052095,
      "grad_norm": 0.7812731266021729,
      "learning_rate": 5.927040934466255e-06,
      "loss": 1.4245,
      "step": 486
    },
    {
      "epoch": 1.9887640449438202,
      "grad_norm": 0.7991986274719238,
      "learning_rate": 5.909414448814971e-06,
      "loss": 1.4751,
      "step": 487
    },
    {
      "epoch": 1.992849846782431,
      "grad_norm": 0.7881086468696594,
      "learning_rate": 5.891776266744686e-06,
      "loss": 1.4107,
      "step": 488
    },
    {
      "epoch": 1.996935648621042,
      "grad_norm": 0.7647078633308411,
      "learning_rate": 5.87412661510859e-06,
      "loss": 1.4187,
      "step": 489
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.9704172611236572,
      "learning_rate": 5.856465720907388e-06,
      "loss": 1.3997,
      "step": 490
    },
    {
      "epoch": 2.004085801838611,
      "grad_norm": 0.8278834223747253,
      "learning_rate": 5.838793811286381e-06,
      "loss": 1.4955,
      "step": 491
    },
    {
      "epoch": 2.0081716036772215,
      "grad_norm": 0.7457200884819031,
      "learning_rate": 5.821111113532545e-06,
      "loss": 1.4518,
      "step": 492
    },
    {
      "epoch": 2.0122574055158324,
      "grad_norm": 0.7733855843544006,
      "learning_rate": 5.803417855071603e-06,
      "loss": 1.5317,
      "step": 493
    },
    {
      "epoch": 2.0163432073544434,
      "grad_norm": 0.778010368347168,
      "learning_rate": 5.7857142634651135e-06,
      "loss": 1.4557,
      "step": 494
    },
    {
      "epoch": 2.0204290091930543,
      "grad_norm": 0.7967993021011353,
      "learning_rate": 5.7680005664075256e-06,
      "loss": 1.4677,
      "step": 495
    },
    {
      "epoch": 2.024514811031665,
      "grad_norm": 0.8295431137084961,
      "learning_rate": 5.7502769917232635e-06,
      "loss": 1.5087,
      "step": 496
    },
    {
      "epoch": 2.028600612870276,
      "grad_norm": 0.7427630424499512,
      "learning_rate": 5.732543767363794e-06,
      "loss": 1.4,
      "step": 497
    },
    {
      "epoch": 2.032686414708887,
      "grad_norm": 0.7718371152877808,
      "learning_rate": 5.7148011214046895e-06,
      "loss": 1.4067,
      "step": 498
    },
    {
      "epoch": 2.0367722165474973,
      "grad_norm": 0.7529418468475342,
      "learning_rate": 5.6970492820426994e-06,
      "loss": 1.4628,
      "step": 499
    },
    {
      "epoch": 2.0408580183861083,
      "grad_norm": 0.7760154008865356,
      "learning_rate": 5.679288477592815e-06,
      "loss": 1.4256,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 976,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2699402128492032.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
