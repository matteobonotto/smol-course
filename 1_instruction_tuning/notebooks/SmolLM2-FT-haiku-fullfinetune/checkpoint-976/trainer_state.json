{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.9846782431052095,
  "eval_steps": 196,
  "global_step": 976,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0040858018386108275,
      "grad_norm": 3.3829736709594727,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 2.5089,
      "step": 1
    },
    {
      "epoch": 0.008171603677221655,
      "grad_norm": 3.2491164207458496,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 2.4799,
      "step": 2
    },
    {
      "epoch": 0.012257405515832482,
      "grad_norm": 3.290837287902832,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 2.3185,
      "step": 3
    },
    {
      "epoch": 0.01634320735444331,
      "grad_norm": 3.1849443912506104,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 2.3057,
      "step": 4
    },
    {
      "epoch": 0.020429009193054137,
      "grad_norm": 3.328892946243286,
      "learning_rate": 5.000000000000001e-07,
      "loss": 2.3949,
      "step": 5
    },
    {
      "epoch": 0.024514811031664963,
      "grad_norm": 3.3742828369140625,
      "learning_rate": 6.000000000000001e-07,
      "loss": 2.4727,
      "step": 6
    },
    {
      "epoch": 0.028600612870275793,
      "grad_norm": 3.344877004623413,
      "learning_rate": 7.000000000000001e-07,
      "loss": 2.4281,
      "step": 7
    },
    {
      "epoch": 0.03268641470888662,
      "grad_norm": 3.237417697906494,
      "learning_rate": 8.000000000000001e-07,
      "loss": 2.5296,
      "step": 8
    },
    {
      "epoch": 0.03677221654749745,
      "grad_norm": 3.208129644393921,
      "learning_rate": 9.000000000000001e-07,
      "loss": 2.4847,
      "step": 9
    },
    {
      "epoch": 0.04085801838610827,
      "grad_norm": 3.245870351791382,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.394,
      "step": 10
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 3.204270362854004,
      "learning_rate": 1.1e-06,
      "loss": 2.4251,
      "step": 11
    },
    {
      "epoch": 0.049029622063329927,
      "grad_norm": 3.3433761596679688,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 2.431,
      "step": 12
    },
    {
      "epoch": 0.05311542390194075,
      "grad_norm": 3.3835549354553223,
      "learning_rate": 1.3e-06,
      "loss": 2.4456,
      "step": 13
    },
    {
      "epoch": 0.05720122574055159,
      "grad_norm": 3.36755633354187,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 2.5022,
      "step": 14
    },
    {
      "epoch": 0.06128702757916241,
      "grad_norm": 3.331289529800415,
      "learning_rate": 1.5e-06,
      "loss": 2.4943,
      "step": 15
    },
    {
      "epoch": 0.06537282941777324,
      "grad_norm": 3.1281816959381104,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 2.3366,
      "step": 16
    },
    {
      "epoch": 0.06945863125638406,
      "grad_norm": 3.1873154640197754,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 2.5708,
      "step": 17
    },
    {
      "epoch": 0.0735444330949949,
      "grad_norm": 3.2119953632354736,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 2.4829,
      "step": 18
    },
    {
      "epoch": 0.07763023493360573,
      "grad_norm": 3.1607604026794434,
      "learning_rate": 1.9000000000000002e-06,
      "loss": 2.4966,
      "step": 19
    },
    {
      "epoch": 0.08171603677221655,
      "grad_norm": 3.2040622234344482,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.4853,
      "step": 20
    },
    {
      "epoch": 0.08580183861082738,
      "grad_norm": 3.156081199645996,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 2.4293,
      "step": 21
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 3.155705690383911,
      "learning_rate": 2.2e-06,
      "loss": 2.3213,
      "step": 22
    },
    {
      "epoch": 0.09397344228804903,
      "grad_norm": 3.203162908554077,
      "learning_rate": 2.3000000000000004e-06,
      "loss": 2.4453,
      "step": 23
    },
    {
      "epoch": 0.09805924412665985,
      "grad_norm": 3.2644028663635254,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.4128,
      "step": 24
    },
    {
      "epoch": 0.10214504596527069,
      "grad_norm": 3.2368576526641846,
      "learning_rate": 2.5e-06,
      "loss": 2.3705,
      "step": 25
    },
    {
      "epoch": 0.1062308478038815,
      "grad_norm": 3.14498233795166,
      "learning_rate": 2.6e-06,
      "loss": 2.4845,
      "step": 26
    },
    {
      "epoch": 0.11031664964249234,
      "grad_norm": 3.1693668365478516,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 2.4444,
      "step": 27
    },
    {
      "epoch": 0.11440245148110317,
      "grad_norm": 3.3279011249542236,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 2.3683,
      "step": 28
    },
    {
      "epoch": 0.118488253319714,
      "grad_norm": 3.2898716926574707,
      "learning_rate": 2.9e-06,
      "loss": 2.4696,
      "step": 29
    },
    {
      "epoch": 0.12257405515832483,
      "grad_norm": 3.293219804763794,
      "learning_rate": 3e-06,
      "loss": 2.3695,
      "step": 30
    },
    {
      "epoch": 0.12665985699693566,
      "grad_norm": 3.105851411819458,
      "learning_rate": 3.1000000000000004e-06,
      "loss": 2.3508,
      "step": 31
    },
    {
      "epoch": 0.13074565883554648,
      "grad_norm": 3.2666871547698975,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 2.4466,
      "step": 32
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 3.132329225540161,
      "learning_rate": 3.3000000000000006e-06,
      "loss": 2.489,
      "step": 33
    },
    {
      "epoch": 0.13891726251276812,
      "grad_norm": 3.0588462352752686,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 2.405,
      "step": 34
    },
    {
      "epoch": 0.14300306435137897,
      "grad_norm": 3.1943771839141846,
      "learning_rate": 3.5e-06,
      "loss": 2.3765,
      "step": 35
    },
    {
      "epoch": 0.1470888661899898,
      "grad_norm": 3.1176047325134277,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 2.3619,
      "step": 36
    },
    {
      "epoch": 0.1511746680286006,
      "grad_norm": 3.16823673248291,
      "learning_rate": 3.7e-06,
      "loss": 2.3449,
      "step": 37
    },
    {
      "epoch": 0.15526046986721145,
      "grad_norm": 2.9607386589050293,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 2.3667,
      "step": 38
    },
    {
      "epoch": 0.15934627170582227,
      "grad_norm": 3.111638307571411,
      "learning_rate": 3.900000000000001e-06,
      "loss": 2.4315,
      "step": 39
    },
    {
      "epoch": 0.1634320735444331,
      "grad_norm": 2.9505045413970947,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.3377,
      "step": 40
    },
    {
      "epoch": 0.1675178753830439,
      "grad_norm": 3.1034393310546875,
      "learning_rate": 4.1e-06,
      "loss": 2.3025,
      "step": 41
    },
    {
      "epoch": 0.17160367722165476,
      "grad_norm": 3.0407190322875977,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 2.3749,
      "step": 42
    },
    {
      "epoch": 0.17568947906026558,
      "grad_norm": 2.979146718978882,
      "learning_rate": 4.3e-06,
      "loss": 2.3454,
      "step": 43
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 2.901794910430908,
      "learning_rate": 4.4e-06,
      "loss": 2.329,
      "step": 44
    },
    {
      "epoch": 0.18386108273748722,
      "grad_norm": 2.814213752746582,
      "learning_rate": 4.5e-06,
      "loss": 2.3967,
      "step": 45
    },
    {
      "epoch": 0.18794688457609807,
      "grad_norm": 2.9190244674682617,
      "learning_rate": 4.600000000000001e-06,
      "loss": 2.3115,
      "step": 46
    },
    {
      "epoch": 0.1920326864147089,
      "grad_norm": 3.0396504402160645,
      "learning_rate": 4.7e-06,
      "loss": 2.3389,
      "step": 47
    },
    {
      "epoch": 0.1961184882533197,
      "grad_norm": 2.8625245094299316,
      "learning_rate": 4.800000000000001e-06,
      "loss": 2.3661,
      "step": 48
    },
    {
      "epoch": 0.20020429009193055,
      "grad_norm": 2.877645254135132,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 2.4112,
      "step": 49
    },
    {
      "epoch": 0.20429009193054137,
      "grad_norm": 2.943817138671875,
      "learning_rate": 5e-06,
      "loss": 2.3444,
      "step": 50
    },
    {
      "epoch": 0.2083758937691522,
      "grad_norm": 2.7169415950775146,
      "learning_rate": 5.1e-06,
      "loss": 2.3719,
      "step": 51
    },
    {
      "epoch": 0.212461695607763,
      "grad_norm": 2.7281627655029297,
      "learning_rate": 5.2e-06,
      "loss": 2.2388,
      "step": 52
    },
    {
      "epoch": 0.21654749744637386,
      "grad_norm": 2.61973237991333,
      "learning_rate": 5.300000000000001e-06,
      "loss": 2.224,
      "step": 53
    },
    {
      "epoch": 0.22063329928498468,
      "grad_norm": 2.6662778854370117,
      "learning_rate": 5.400000000000001e-06,
      "loss": 2.1945,
      "step": 54
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 2.509321451187134,
      "learning_rate": 5.500000000000001e-06,
      "loss": 2.2927,
      "step": 55
    },
    {
      "epoch": 0.22880490296220635,
      "grad_norm": 2.3763692378997803,
      "learning_rate": 5.600000000000001e-06,
      "loss": 2.3623,
      "step": 56
    },
    {
      "epoch": 0.23289070480081717,
      "grad_norm": 2.3128390312194824,
      "learning_rate": 5.7e-06,
      "loss": 2.3144,
      "step": 57
    },
    {
      "epoch": 0.236976506639428,
      "grad_norm": 2.4734749794006348,
      "learning_rate": 5.8e-06,
      "loss": 2.1994,
      "step": 58
    },
    {
      "epoch": 0.2410623084780388,
      "grad_norm": 2.4007058143615723,
      "learning_rate": 5.9e-06,
      "loss": 2.2547,
      "step": 59
    },
    {
      "epoch": 0.24514811031664965,
      "grad_norm": 2.3849408626556396,
      "learning_rate": 6e-06,
      "loss": 2.2413,
      "step": 60
    },
    {
      "epoch": 0.24923391215526047,
      "grad_norm": 2.3451406955718994,
      "learning_rate": 6.1e-06,
      "loss": 2.2176,
      "step": 61
    },
    {
      "epoch": 0.2533197139938713,
      "grad_norm": 2.3156731128692627,
      "learning_rate": 6.200000000000001e-06,
      "loss": 2.3031,
      "step": 62
    },
    {
      "epoch": 0.2574055158324821,
      "grad_norm": 2.373661756515503,
      "learning_rate": 6.300000000000001e-06,
      "loss": 2.0754,
      "step": 63
    },
    {
      "epoch": 0.26149131767109296,
      "grad_norm": 2.112269163131714,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 2.166,
      "step": 64
    },
    {
      "epoch": 0.26557711950970375,
      "grad_norm": 2.2189927101135254,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 2.1825,
      "step": 65
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 2.077983856201172,
      "learning_rate": 6.600000000000001e-06,
      "loss": 2.3392,
      "step": 66
    },
    {
      "epoch": 0.27374872318692545,
      "grad_norm": 2.210522174835205,
      "learning_rate": 6.700000000000001e-06,
      "loss": 2.1832,
      "step": 67
    },
    {
      "epoch": 0.27783452502553624,
      "grad_norm": 2.177490472793579,
      "learning_rate": 6.800000000000001e-06,
      "loss": 2.2173,
      "step": 68
    },
    {
      "epoch": 0.2819203268641471,
      "grad_norm": 2.1432788372039795,
      "learning_rate": 6.9e-06,
      "loss": 2.2471,
      "step": 69
    },
    {
      "epoch": 0.28600612870275793,
      "grad_norm": 1.9484342336654663,
      "learning_rate": 7e-06,
      "loss": 2.217,
      "step": 70
    },
    {
      "epoch": 0.2900919305413687,
      "grad_norm": 1.8661788702011108,
      "learning_rate": 7.100000000000001e-06,
      "loss": 1.965,
      "step": 71
    },
    {
      "epoch": 0.2941777323799796,
      "grad_norm": 1.8478305339813232,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 2.2066,
      "step": 72
    },
    {
      "epoch": 0.2982635342185904,
      "grad_norm": 1.7833147048950195,
      "learning_rate": 7.3e-06,
      "loss": 1.9782,
      "step": 73
    },
    {
      "epoch": 0.3023493360572012,
      "grad_norm": 1.6863828897476196,
      "learning_rate": 7.4e-06,
      "loss": 2.113,
      "step": 74
    },
    {
      "epoch": 0.30643513789581206,
      "grad_norm": 1.712652564048767,
      "learning_rate": 7.500000000000001e-06,
      "loss": 2.1471,
      "step": 75
    },
    {
      "epoch": 0.3105209397344229,
      "grad_norm": 1.562754511833191,
      "learning_rate": 7.600000000000001e-06,
      "loss": 2.0002,
      "step": 76
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 1.533638596534729,
      "learning_rate": 7.7e-06,
      "loss": 2.1104,
      "step": 77
    },
    {
      "epoch": 0.31869254341164455,
      "grad_norm": 1.6071219444274902,
      "learning_rate": 7.800000000000002e-06,
      "loss": 2.0483,
      "step": 78
    },
    {
      "epoch": 0.32277834525025534,
      "grad_norm": 1.4763665199279785,
      "learning_rate": 7.9e-06,
      "loss": 2.0629,
      "step": 79
    },
    {
      "epoch": 0.3268641470888662,
      "grad_norm": 1.3456838130950928,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9614,
      "step": 80
    },
    {
      "epoch": 0.33094994892747703,
      "grad_norm": 1.438092589378357,
      "learning_rate": 8.1e-06,
      "loss": 2.0619,
      "step": 81
    },
    {
      "epoch": 0.3350357507660878,
      "grad_norm": 1.2771646976470947,
      "learning_rate": 8.2e-06,
      "loss": 2.0531,
      "step": 82
    },
    {
      "epoch": 0.3391215526046987,
      "grad_norm": 1.2244858741760254,
      "learning_rate": 8.3e-06,
      "loss": 1.9334,
      "step": 83
    },
    {
      "epoch": 0.3432073544433095,
      "grad_norm": 1.2445896863937378,
      "learning_rate": 8.400000000000001e-06,
      "loss": 2.0841,
      "step": 84
    },
    {
      "epoch": 0.3472931562819203,
      "grad_norm": 1.2144769430160522,
      "learning_rate": 8.5e-06,
      "loss": 1.8723,
      "step": 85
    },
    {
      "epoch": 0.35137895812053116,
      "grad_norm": 1.094497799873352,
      "learning_rate": 8.6e-06,
      "loss": 1.9161,
      "step": 86
    },
    {
      "epoch": 0.355464759959142,
      "grad_norm": 1.1723986864089966,
      "learning_rate": 8.700000000000001e-06,
      "loss": 1.9345,
      "step": 87
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 1.0887377262115479,
      "learning_rate": 8.8e-06,
      "loss": 1.9869,
      "step": 88
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 1.044213056564331,
      "learning_rate": 8.900000000000001e-06,
      "loss": 1.9782,
      "step": 89
    },
    {
      "epoch": 0.36772216547497444,
      "grad_norm": 1.0860035419464111,
      "learning_rate": 9e-06,
      "loss": 1.9602,
      "step": 90
    },
    {
      "epoch": 0.3718079673135853,
      "grad_norm": 1.0279744863510132,
      "learning_rate": 9.100000000000001e-06,
      "loss": 1.8569,
      "step": 91
    },
    {
      "epoch": 0.37589376915219613,
      "grad_norm": 1.072224497795105,
      "learning_rate": 9.200000000000002e-06,
      "loss": 1.9846,
      "step": 92
    },
    {
      "epoch": 0.3799795709908069,
      "grad_norm": 1.00211501121521,
      "learning_rate": 9.3e-06,
      "loss": 1.8289,
      "step": 93
    },
    {
      "epoch": 0.3840653728294178,
      "grad_norm": 1.0193885564804077,
      "learning_rate": 9.4e-06,
      "loss": 1.803,
      "step": 94
    },
    {
      "epoch": 0.3881511746680286,
      "grad_norm": 1.068361520767212,
      "learning_rate": 9.5e-06,
      "loss": 1.8208,
      "step": 95
    },
    {
      "epoch": 0.3922369765066394,
      "grad_norm": 0.9977104663848877,
      "learning_rate": 9.600000000000001e-06,
      "loss": 1.9059,
      "step": 96
    },
    {
      "epoch": 0.39632277834525026,
      "grad_norm": 0.9563716650009155,
      "learning_rate": 9.7e-06,
      "loss": 1.899,
      "step": 97
    },
    {
      "epoch": 0.4004085801838611,
      "grad_norm": 1.058883547782898,
      "learning_rate": 9.800000000000001e-06,
      "loss": 1.8806,
      "step": 98
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 1.005527138710022,
      "learning_rate": 9.9e-06,
      "loss": 1.7639,
      "step": 99
    },
    {
      "epoch": 0.40858018386108275,
      "grad_norm": 0.9955249428749084,
      "learning_rate": 1e-05,
      "loss": 1.8659,
      "step": 100
    },
    {
      "epoch": 0.41266598569969354,
      "grad_norm": 1.010312557220459,
      "learning_rate": 9.999967846291054e-06,
      "loss": 1.8895,
      "step": 101
    },
    {
      "epoch": 0.4167517875383044,
      "grad_norm": 1.1165413856506348,
      "learning_rate": 9.999871385577756e-06,
      "loss": 1.8746,
      "step": 102
    },
    {
      "epoch": 0.42083758937691523,
      "grad_norm": 1.3805980682373047,
      "learning_rate": 9.999710619100732e-06,
      "loss": 1.8606,
      "step": 103
    },
    {
      "epoch": 0.424923391215526,
      "grad_norm": 1.0794728994369507,
      "learning_rate": 9.999485548927686e-06,
      "loss": 1.8,
      "step": 104
    },
    {
      "epoch": 0.4290091930541369,
      "grad_norm": 1.0054545402526855,
      "learning_rate": 9.999196177953345e-06,
      "loss": 1.8217,
      "step": 105
    },
    {
      "epoch": 0.4330949948927477,
      "grad_norm": 0.9761620759963989,
      "learning_rate": 9.998842509899456e-06,
      "loss": 1.7438,
      "step": 106
    },
    {
      "epoch": 0.4371807967313585,
      "grad_norm": 0.9001764059066772,
      "learning_rate": 9.99842454931471e-06,
      "loss": 1.813,
      "step": 107
    },
    {
      "epoch": 0.44126659856996936,
      "grad_norm": 0.948833703994751,
      "learning_rate": 9.997942301574701e-06,
      "loss": 1.7688,
      "step": 108
    },
    {
      "epoch": 0.4453524004085802,
      "grad_norm": 0.9818925857543945,
      "learning_rate": 9.997395772881853e-06,
      "loss": 1.7147,
      "step": 109
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.9761221408843994,
      "learning_rate": 9.996784970265335e-06,
      "loss": 1.7671,
      "step": 110
    },
    {
      "epoch": 0.45352400408580185,
      "grad_norm": 0.9516489505767822,
      "learning_rate": 9.99610990158097e-06,
      "loss": 1.7461,
      "step": 111
    },
    {
      "epoch": 0.4576098059244127,
      "grad_norm": 0.8722044229507446,
      "learning_rate": 9.995370575511151e-06,
      "loss": 1.7357,
      "step": 112
    },
    {
      "epoch": 0.4616956077630235,
      "grad_norm": 0.8773142099380493,
      "learning_rate": 9.994567001564703e-06,
      "loss": 1.7653,
      "step": 113
    },
    {
      "epoch": 0.46578140960163433,
      "grad_norm": 0.8347426056861877,
      "learning_rate": 9.993699190076778e-06,
      "loss": 1.6817,
      "step": 114
    },
    {
      "epoch": 0.4698672114402451,
      "grad_norm": 0.8453010320663452,
      "learning_rate": 9.992767152208724e-06,
      "loss": 1.7029,
      "step": 115
    },
    {
      "epoch": 0.473953013278856,
      "grad_norm": 0.8705855011940002,
      "learning_rate": 9.991770899947925e-06,
      "loss": 1.8604,
      "step": 116
    },
    {
      "epoch": 0.4780388151174668,
      "grad_norm": 0.8153737187385559,
      "learning_rate": 9.99071044610767e-06,
      "loss": 1.6799,
      "step": 117
    },
    {
      "epoch": 0.4821246169560776,
      "grad_norm": 0.973566472530365,
      "learning_rate": 9.989585804326963e-06,
      "loss": 1.748,
      "step": 118
    },
    {
      "epoch": 0.48621041879468846,
      "grad_norm": 0.8062142133712769,
      "learning_rate": 9.988396989070365e-06,
      "loss": 1.7347,
      "step": 119
    },
    {
      "epoch": 0.4902962206332993,
      "grad_norm": 0.8926886916160583,
      "learning_rate": 9.98714401562781e-06,
      "loss": 1.688,
      "step": 120
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 1.0157231092453003,
      "learning_rate": 9.985826900114391e-06,
      "loss": 1.7779,
      "step": 121
    },
    {
      "epoch": 0.49846782431052095,
      "grad_norm": 0.882121741771698,
      "learning_rate": 9.984445659470166e-06,
      "loss": 1.7893,
      "step": 122
    },
    {
      "epoch": 0.5025536261491318,
      "grad_norm": 0.8479747772216797,
      "learning_rate": 9.983000311459943e-06,
      "loss": 1.6397,
      "step": 123
    },
    {
      "epoch": 0.5066394279877426,
      "grad_norm": 0.9136170744895935,
      "learning_rate": 9.98149087467304e-06,
      "loss": 1.6974,
      "step": 124
    },
    {
      "epoch": 0.5107252298263534,
      "grad_norm": 0.8176090121269226,
      "learning_rate": 9.979917368523053e-06,
      "loss": 1.5626,
      "step": 125
    },
    {
      "epoch": 0.5148110316649642,
      "grad_norm": 0.7951239347457886,
      "learning_rate": 9.978279813247605e-06,
      "loss": 1.7914,
      "step": 126
    },
    {
      "epoch": 0.5188968335035751,
      "grad_norm": 0.8319465517997742,
      "learning_rate": 9.97657822990809e-06,
      "loss": 1.6348,
      "step": 127
    },
    {
      "epoch": 0.5229826353421859,
      "grad_norm": 0.866348385810852,
      "learning_rate": 9.97481264038939e-06,
      "loss": 1.5998,
      "step": 128
    },
    {
      "epoch": 0.5270684371807968,
      "grad_norm": 0.8951460123062134,
      "learning_rate": 9.972983067399604e-06,
      "loss": 1.7466,
      "step": 129
    },
    {
      "epoch": 0.5311542390194075,
      "grad_norm": 0.9599961042404175,
      "learning_rate": 9.97108953446976e-06,
      "loss": 1.623,
      "step": 130
    },
    {
      "epoch": 0.5352400408580184,
      "grad_norm": 0.8499050736427307,
      "learning_rate": 9.969132065953499e-06,
      "loss": 1.6571,
      "step": 131
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.8590211868286133,
      "learning_rate": 9.967110687026769e-06,
      "loss": 1.7522,
      "step": 132
    },
    {
      "epoch": 0.54341164453524,
      "grad_norm": 0.8864648342132568,
      "learning_rate": 9.965025423687505e-06,
      "loss": 1.7311,
      "step": 133
    },
    {
      "epoch": 0.5474974463738509,
      "grad_norm": 0.7737080454826355,
      "learning_rate": 9.962876302755283e-06,
      "loss": 1.7091,
      "step": 134
    },
    {
      "epoch": 0.5515832482124617,
      "grad_norm": 0.8004711270332336,
      "learning_rate": 9.960663351870988e-06,
      "loss": 1.6638,
      "step": 135
    },
    {
      "epoch": 0.5556690500510725,
      "grad_norm": 0.8826137185096741,
      "learning_rate": 9.95838659949645e-06,
      "loss": 1.7382,
      "step": 136
    },
    {
      "epoch": 0.5597548518896833,
      "grad_norm": 0.8368663191795349,
      "learning_rate": 9.956046074914088e-06,
      "loss": 1.789,
      "step": 137
    },
    {
      "epoch": 0.5638406537282942,
      "grad_norm": 0.8223842978477478,
      "learning_rate": 9.953641808226513e-06,
      "loss": 1.5768,
      "step": 138
    },
    {
      "epoch": 0.567926455566905,
      "grad_norm": 0.8901197910308838,
      "learning_rate": 9.951173830356168e-06,
      "loss": 1.7281,
      "step": 139
    },
    {
      "epoch": 0.5720122574055159,
      "grad_norm": 0.8528128266334534,
      "learning_rate": 9.948642173044906e-06,
      "loss": 1.6319,
      "step": 140
    },
    {
      "epoch": 0.5760980592441267,
      "grad_norm": 0.8387688994407654,
      "learning_rate": 9.946046868853595e-06,
      "loss": 1.5486,
      "step": 141
    },
    {
      "epoch": 0.5801838610827375,
      "grad_norm": 0.8162190318107605,
      "learning_rate": 9.943387951161702e-06,
      "loss": 1.7066,
      "step": 142
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.8108418583869934,
      "learning_rate": 9.940665454166851e-06,
      "loss": 1.5913,
      "step": 143
    },
    {
      "epoch": 0.5883554647599591,
      "grad_norm": 0.7919254899024963,
      "learning_rate": 9.93787941288439e-06,
      "loss": 1.6546,
      "step": 144
    },
    {
      "epoch": 0.59244126659857,
      "grad_norm": 1.0231281518936157,
      "learning_rate": 9.935029863146946e-06,
      "loss": 1.7856,
      "step": 145
    },
    {
      "epoch": 0.5965270684371808,
      "grad_norm": 0.9042031168937683,
      "learning_rate": 9.932116841603954e-06,
      "loss": 1.8318,
      "step": 146
    },
    {
      "epoch": 0.6006128702757916,
      "grad_norm": 1.0128453969955444,
      "learning_rate": 9.929140385721193e-06,
      "loss": 1.7692,
      "step": 147
    },
    {
      "epoch": 0.6046986721144024,
      "grad_norm": 0.7685208916664124,
      "learning_rate": 9.926100533780304e-06,
      "loss": 1.612,
      "step": 148
    },
    {
      "epoch": 0.6087844739530133,
      "grad_norm": 0.820930004119873,
      "learning_rate": 9.92299732487829e-06,
      "loss": 1.6642,
      "step": 149
    },
    {
      "epoch": 0.6128702757916241,
      "grad_norm": 0.9259809255599976,
      "learning_rate": 9.919830798927024e-06,
      "loss": 1.7359,
      "step": 150
    },
    {
      "epoch": 0.616956077630235,
      "grad_norm": 0.8309621214866638,
      "learning_rate": 9.916600996652726e-06,
      "loss": 1.639,
      "step": 151
    },
    {
      "epoch": 0.6210418794688458,
      "grad_norm": 0.8183449506759644,
      "learning_rate": 9.913307959595443e-06,
      "loss": 1.68,
      "step": 152
    },
    {
      "epoch": 0.6251276813074566,
      "grad_norm": 0.9040635228157043,
      "learning_rate": 9.90995173010852e-06,
      "loss": 1.6539,
      "step": 153
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.8058280348777771,
      "learning_rate": 9.906532351358047e-06,
      "loss": 1.6781,
      "step": 154
    },
    {
      "epoch": 0.6332992849846782,
      "grad_norm": 0.8160686492919922,
      "learning_rate": 9.903049867322308e-06,
      "loss": 1.7503,
      "step": 155
    },
    {
      "epoch": 0.6373850868232891,
      "grad_norm": 0.862239420413971,
      "learning_rate": 9.899504322791212e-06,
      "loss": 1.7535,
      "step": 156
    },
    {
      "epoch": 0.6414708886618999,
      "grad_norm": 0.7590650320053101,
      "learning_rate": 9.895895763365722e-06,
      "loss": 1.6628,
      "step": 157
    },
    {
      "epoch": 0.6455566905005107,
      "grad_norm": 0.9287768006324768,
      "learning_rate": 9.892224235457269e-06,
      "loss": 1.6552,
      "step": 158
    },
    {
      "epoch": 0.6496424923391215,
      "grad_norm": 0.7802994847297668,
      "learning_rate": 9.888489786287146e-06,
      "loss": 1.5915,
      "step": 159
    },
    {
      "epoch": 0.6537282941777324,
      "grad_norm": 0.7706590294837952,
      "learning_rate": 9.88469246388591e-06,
      "loss": 1.5479,
      "step": 160
    },
    {
      "epoch": 0.6578140960163432,
      "grad_norm": 0.7894954681396484,
      "learning_rate": 9.880832317092762e-06,
      "loss": 1.5405,
      "step": 161
    },
    {
      "epoch": 0.6618998978549541,
      "grad_norm": 0.9521697759628296,
      "learning_rate": 9.876909395554915e-06,
      "loss": 1.5876,
      "step": 162
    },
    {
      "epoch": 0.6659856996935649,
      "grad_norm": 0.7764186859130859,
      "learning_rate": 9.872923749726959e-06,
      "loss": 1.5824,
      "step": 163
    },
    {
      "epoch": 0.6700715015321757,
      "grad_norm": 0.8229662179946899,
      "learning_rate": 9.868875430870217e-06,
      "loss": 1.5182,
      "step": 164
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 0.8251367807388306,
      "learning_rate": 9.86476449105207e-06,
      "loss": 1.6334,
      "step": 165
    },
    {
      "epoch": 0.6782431052093973,
      "grad_norm": 0.9041468501091003,
      "learning_rate": 9.860590983145307e-06,
      "loss": 1.6433,
      "step": 166
    },
    {
      "epoch": 0.6823289070480082,
      "grad_norm": 0.8367764949798584,
      "learning_rate": 9.85635496082743e-06,
      "loss": 1.6951,
      "step": 167
    },
    {
      "epoch": 0.686414708886619,
      "grad_norm": 0.8435304760932922,
      "learning_rate": 9.85205647857997e-06,
      "loss": 1.5691,
      "step": 168
    },
    {
      "epoch": 0.6905005107252298,
      "grad_norm": 0.8856136202812195,
      "learning_rate": 9.847695591687788e-06,
      "loss": 1.7521,
      "step": 169
    },
    {
      "epoch": 0.6945863125638406,
      "grad_norm": 0.8881165385246277,
      "learning_rate": 9.843272356238355e-06,
      "loss": 1.6593,
      "step": 170
    },
    {
      "epoch": 0.6986721144024515,
      "grad_norm": 0.844526469707489,
      "learning_rate": 9.838786829121045e-06,
      "loss": 1.6263,
      "step": 171
    },
    {
      "epoch": 0.7027579162410623,
      "grad_norm": 0.848063051700592,
      "learning_rate": 9.834239068026388e-06,
      "loss": 1.6838,
      "step": 172
    },
    {
      "epoch": 0.7068437180796732,
      "grad_norm": 0.9740725159645081,
      "learning_rate": 9.829629131445342e-06,
      "loss": 1.671,
      "step": 173
    },
    {
      "epoch": 0.710929519918284,
      "grad_norm": 0.8551496863365173,
      "learning_rate": 9.82495707866853e-06,
      "loss": 1.7248,
      "step": 174
    },
    {
      "epoch": 0.7150153217568948,
      "grad_norm": 0.8832868337631226,
      "learning_rate": 9.82022296978548e-06,
      "loss": 1.5535,
      "step": 175
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.8564367294311523,
      "learning_rate": 9.815426865683858e-06,
      "loss": 1.6053,
      "step": 176
    },
    {
      "epoch": 0.7231869254341164,
      "grad_norm": 0.8426526188850403,
      "learning_rate": 9.810568828048674e-06,
      "loss": 1.5508,
      "step": 177
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.7850543260574341,
      "learning_rate": 9.805648919361505e-06,
      "loss": 1.451,
      "step": 178
    },
    {
      "epoch": 0.7313585291113381,
      "grad_norm": 0.8240954279899597,
      "learning_rate": 9.800667202899672e-06,
      "loss": 1.5227,
      "step": 179
    },
    {
      "epoch": 0.7354443309499489,
      "grad_norm": 0.8508866429328918,
      "learning_rate": 9.79562374273544e-06,
      "loss": 1.6148,
      "step": 180
    },
    {
      "epoch": 0.7395301327885597,
      "grad_norm": 0.863655149936676,
      "learning_rate": 9.790518603735191e-06,
      "loss": 1.6512,
      "step": 181
    },
    {
      "epoch": 0.7436159346271706,
      "grad_norm": 0.8587683439254761,
      "learning_rate": 9.785351851558585e-06,
      "loss": 1.6248,
      "step": 182
    },
    {
      "epoch": 0.7477017364657814,
      "grad_norm": 0.8545947074890137,
      "learning_rate": 9.780123552657722e-06,
      "loss": 1.5823,
      "step": 183
    },
    {
      "epoch": 0.7517875383043923,
      "grad_norm": 0.826912522315979,
      "learning_rate": 9.774833774276278e-06,
      "loss": 1.6564,
      "step": 184
    },
    {
      "epoch": 0.7558733401430031,
      "grad_norm": 0.8710339665412903,
      "learning_rate": 9.769482584448654e-06,
      "loss": 1.6967,
      "step": 185
    },
    {
      "epoch": 0.7599591419816139,
      "grad_norm": 0.7784331440925598,
      "learning_rate": 9.76407005199909e-06,
      "loss": 1.4609,
      "step": 186
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 0.8147224187850952,
      "learning_rate": 9.758596246540782e-06,
      "loss": 1.5571,
      "step": 187
    },
    {
      "epoch": 0.7681307456588355,
      "grad_norm": 0.83614182472229,
      "learning_rate": 9.75306123847499e-06,
      "loss": 1.4844,
      "step": 188
    },
    {
      "epoch": 0.7722165474974464,
      "grad_norm": 0.8114571571350098,
      "learning_rate": 9.74746509899013e-06,
      "loss": 1.6736,
      "step": 189
    },
    {
      "epoch": 0.7763023493360572,
      "grad_norm": 0.799305260181427,
      "learning_rate": 9.741807900060858e-06,
      "loss": 1.6541,
      "step": 190
    },
    {
      "epoch": 0.780388151174668,
      "grad_norm": 0.8416231274604797,
      "learning_rate": 9.736089714447144e-06,
      "loss": 1.5901,
      "step": 191
    },
    {
      "epoch": 0.7844739530132788,
      "grad_norm": 0.846994161605835,
      "learning_rate": 9.73031061569334e-06,
      "loss": 1.5777,
      "step": 192
    },
    {
      "epoch": 0.7885597548518897,
      "grad_norm": 0.8148579001426697,
      "learning_rate": 9.724470678127226e-06,
      "loss": 1.4717,
      "step": 193
    },
    {
      "epoch": 0.7926455566905005,
      "grad_norm": 0.8310193419456482,
      "learning_rate": 9.718569976859068e-06,
      "loss": 1.6922,
      "step": 194
    },
    {
      "epoch": 0.7967313585291114,
      "grad_norm": 0.8231230974197388,
      "learning_rate": 9.712608587780634e-06,
      "loss": 1.6503,
      "step": 195
    },
    {
      "epoch": 0.8008171603677222,
      "grad_norm": 0.8353662490844727,
      "learning_rate": 9.706586587564236e-06,
      "loss": 1.681,
      "step": 196
    },
    {
      "epoch": 0.8008171603677222,
      "eval_loss": 1.5721359252929688,
      "eval_runtime": 18.5783,
      "eval_samples_per_second": 11.142,
      "eval_steps_per_second": 2.799,
      "step": 196
    },
    {
      "epoch": 0.804902962206333,
      "grad_norm": 0.8078488707542419,
      "learning_rate": 9.700504053661728e-06,
      "loss": 1.5936,
      "step": 197
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.8761959075927734,
      "learning_rate": 9.69436106430352e-06,
      "loss": 1.6603,
      "step": 198
    },
    {
      "epoch": 0.8130745658835546,
      "grad_norm": 0.8630024194717407,
      "learning_rate": 9.68815769849757e-06,
      "loss": 1.6273,
      "step": 199
    },
    {
      "epoch": 0.8171603677221655,
      "grad_norm": 0.8546648621559143,
      "learning_rate": 9.681894036028365e-06,
      "loss": 1.4874,
      "step": 200
    },
    {
      "epoch": 0.8212461695607763,
      "grad_norm": 0.9027106165885925,
      "learning_rate": 9.675570157455899e-06,
      "loss": 1.6791,
      "step": 201
    },
    {
      "epoch": 0.8253319713993871,
      "grad_norm": 0.8465160131454468,
      "learning_rate": 9.669186144114627e-06,
      "loss": 1.5772,
      "step": 202
    },
    {
      "epoch": 0.8294177732379979,
      "grad_norm": 0.8317087292671204,
      "learning_rate": 9.662742078112436e-06,
      "loss": 1.5413,
      "step": 203
    },
    {
      "epoch": 0.8335035750766088,
      "grad_norm": 0.8394920825958252,
      "learning_rate": 9.656238042329575e-06,
      "loss": 1.6293,
      "step": 204
    },
    {
      "epoch": 0.8375893769152196,
      "grad_norm": 0.8029077053070068,
      "learning_rate": 9.649674120417591e-06,
      "loss": 1.5882,
      "step": 205
    },
    {
      "epoch": 0.8416751787538305,
      "grad_norm": 0.7874294519424438,
      "learning_rate": 9.643050396798262e-06,
      "loss": 1.6176,
      "step": 206
    },
    {
      "epoch": 0.8457609805924413,
      "grad_norm": 0.8644412159919739,
      "learning_rate": 9.636366956662496e-06,
      "loss": 1.6649,
      "step": 207
    },
    {
      "epoch": 0.849846782431052,
      "grad_norm": 0.8101349472999573,
      "learning_rate": 9.62962388596925e-06,
      "loss": 1.6552,
      "step": 208
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.7655364274978638,
      "learning_rate": 9.622821271444418e-06,
      "loss": 1.524,
      "step": 209
    },
    {
      "epoch": 0.8580183861082737,
      "grad_norm": 0.8285725712776184,
      "learning_rate": 9.615959200579716e-06,
      "loss": 1.6133,
      "step": 210
    },
    {
      "epoch": 0.8621041879468846,
      "grad_norm": 0.7825216054916382,
      "learning_rate": 9.609037761631552e-06,
      "loss": 1.5411,
      "step": 211
    },
    {
      "epoch": 0.8661899897854954,
      "grad_norm": 0.8331167101860046,
      "learning_rate": 9.602057043619903e-06,
      "loss": 1.6185,
      "step": 212
    },
    {
      "epoch": 0.8702757916241062,
      "grad_norm": 0.7688961625099182,
      "learning_rate": 9.595017136327158e-06,
      "loss": 1.6032,
      "step": 213
    },
    {
      "epoch": 0.874361593462717,
      "grad_norm": 0.9999822378158569,
      "learning_rate": 9.587918130296969e-06,
      "loss": 1.5303,
      "step": 214
    },
    {
      "epoch": 0.8784473953013279,
      "grad_norm": 0.8608517646789551,
      "learning_rate": 9.580760116833086e-06,
      "loss": 1.6971,
      "step": 215
    },
    {
      "epoch": 0.8825331971399387,
      "grad_norm": 0.8009931445121765,
      "learning_rate": 9.57354318799818e-06,
      "loss": 1.4975,
      "step": 216
    },
    {
      "epoch": 0.8866189989785496,
      "grad_norm": 0.8063307404518127,
      "learning_rate": 9.566267436612662e-06,
      "loss": 1.5682,
      "step": 217
    },
    {
      "epoch": 0.8907048008171604,
      "grad_norm": 0.8491661548614502,
      "learning_rate": 9.558932956253492e-06,
      "loss": 1.5964,
      "step": 218
    },
    {
      "epoch": 0.8947906026557712,
      "grad_norm": 0.7991448640823364,
      "learning_rate": 9.551539841252969e-06,
      "loss": 1.5575,
      "step": 219
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.8133922219276428,
      "learning_rate": 9.544088186697515e-06,
      "loss": 1.6025,
      "step": 220
    },
    {
      "epoch": 0.9029622063329928,
      "grad_norm": 0.9020516276359558,
      "learning_rate": 9.536578088426468e-06,
      "loss": 1.6239,
      "step": 221
    },
    {
      "epoch": 0.9070480081716037,
      "grad_norm": 0.7939598560333252,
      "learning_rate": 9.529009643030831e-06,
      "loss": 1.6162,
      "step": 222
    },
    {
      "epoch": 0.9111338100102145,
      "grad_norm": 0.8187311887741089,
      "learning_rate": 9.521382947852042e-06,
      "loss": 1.5538,
      "step": 223
    },
    {
      "epoch": 0.9152196118488254,
      "grad_norm": 0.8332200646400452,
      "learning_rate": 9.513698100980715e-06,
      "loss": 1.6345,
      "step": 224
    },
    {
      "epoch": 0.9193054136874361,
      "grad_norm": 0.885047435760498,
      "learning_rate": 9.505955201255381e-06,
      "loss": 1.591,
      "step": 225
    },
    {
      "epoch": 0.923391215526047,
      "grad_norm": 0.9464643001556396,
      "learning_rate": 9.498154348261217e-06,
      "loss": 1.6149,
      "step": 226
    },
    {
      "epoch": 0.9274770173646578,
      "grad_norm": 0.7604774832725525,
      "learning_rate": 9.490295642328768e-06,
      "loss": 1.5415,
      "step": 227
    },
    {
      "epoch": 0.9315628192032687,
      "grad_norm": 0.774958610534668,
      "learning_rate": 9.482379184532652e-06,
      "loss": 1.5357,
      "step": 228
    },
    {
      "epoch": 0.9356486210418795,
      "grad_norm": 0.8544102311134338,
      "learning_rate": 9.474405076690257e-06,
      "loss": 1.5947,
      "step": 229
    },
    {
      "epoch": 0.9397344228804902,
      "grad_norm": 0.7783493995666504,
      "learning_rate": 9.466373421360442e-06,
      "loss": 1.5274,
      "step": 230
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 0.7830492854118347,
      "learning_rate": 9.45828432184221e-06,
      "loss": 1.4588,
      "step": 231
    },
    {
      "epoch": 0.947906026557712,
      "grad_norm": 0.829025149345398,
      "learning_rate": 9.450137882173385e-06,
      "loss": 1.6432,
      "step": 232
    },
    {
      "epoch": 0.9519918283963228,
      "grad_norm": 0.909126341342926,
      "learning_rate": 9.441934207129263e-06,
      "loss": 1.6836,
      "step": 233
    },
    {
      "epoch": 0.9560776302349336,
      "grad_norm": 0.7756729125976562,
      "learning_rate": 9.433673402221275e-06,
      "loss": 1.5496,
      "step": 234
    },
    {
      "epoch": 0.9601634320735445,
      "grad_norm": 0.8532222509384155,
      "learning_rate": 9.425355573695628e-06,
      "loss": 1.5463,
      "step": 235
    },
    {
      "epoch": 0.9642492339121552,
      "grad_norm": 0.8349085450172424,
      "learning_rate": 9.416980828531944e-06,
      "loss": 1.4966,
      "step": 236
    },
    {
      "epoch": 0.9683350357507661,
      "grad_norm": 0.8697748780250549,
      "learning_rate": 9.40854927444186e-06,
      "loss": 1.7189,
      "step": 237
    },
    {
      "epoch": 0.9724208375893769,
      "grad_norm": 1.0067797899246216,
      "learning_rate": 9.40006101986768e-06,
      "loss": 1.5666,
      "step": 238
    },
    {
      "epoch": 0.9765066394279878,
      "grad_norm": 0.8448037505149841,
      "learning_rate": 9.391516173980942e-06,
      "loss": 1.6128,
      "step": 239
    },
    {
      "epoch": 0.9805924412665986,
      "grad_norm": 0.9429301023483276,
      "learning_rate": 9.382914846681049e-06,
      "loss": 1.5469,
      "step": 240
    },
    {
      "epoch": 0.9846782431052093,
      "grad_norm": 0.7961666584014893,
      "learning_rate": 9.374257148593824e-06,
      "loss": 1.5416,
      "step": 241
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 0.8371102213859558,
      "learning_rate": 9.365543191070114e-06,
      "loss": 1.6245,
      "step": 242
    },
    {
      "epoch": 0.992849846782431,
      "grad_norm": 0.8211541175842285,
      "learning_rate": 9.356773086184338e-06,
      "loss": 1.4769,
      "step": 243
    },
    {
      "epoch": 0.9969356486210419,
      "grad_norm": 0.8450199365615845,
      "learning_rate": 9.347946946733055e-06,
      "loss": 1.5063,
      "step": 244
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8986855745315552,
      "learning_rate": 9.339064886233515e-06,
      "loss": 1.558,
      "step": 245
    },
    {
      "epoch": 1.0040858018386107,
      "grad_norm": 0.8158212304115295,
      "learning_rate": 9.330127018922195e-06,
      "loss": 1.4924,
      "step": 246
    },
    {
      "epoch": 1.0081716036772217,
      "grad_norm": 0.7308236956596375,
      "learning_rate": 9.321133459753322e-06,
      "loss": 1.4617,
      "step": 247
    },
    {
      "epoch": 1.0122574055158324,
      "grad_norm": 0.8010638356208801,
      "learning_rate": 9.312084324397416e-06,
      "loss": 1.5416,
      "step": 248
    },
    {
      "epoch": 1.0163432073544434,
      "grad_norm": 0.7734800577163696,
      "learning_rate": 9.30297972923978e-06,
      "loss": 1.4439,
      "step": 249
    },
    {
      "epoch": 1.0204290091930541,
      "grad_norm": 0.8287535905838013,
      "learning_rate": 9.293819791379016e-06,
      "loss": 1.5042,
      "step": 250
    },
    {
      "epoch": 1.0245148110316649,
      "grad_norm": 0.9475878477096558,
      "learning_rate": 9.284604628625514e-06,
      "loss": 1.654,
      "step": 251
    },
    {
      "epoch": 1.0286006128702758,
      "grad_norm": 0.7581089735031128,
      "learning_rate": 9.275334359499936e-06,
      "loss": 1.5769,
      "step": 252
    },
    {
      "epoch": 1.0326864147088866,
      "grad_norm": 0.833729088306427,
      "learning_rate": 9.266009103231702e-06,
      "loss": 1.5287,
      "step": 253
    },
    {
      "epoch": 1.0367722165474975,
      "grad_norm": 0.8135689496994019,
      "learning_rate": 9.256628979757435e-06,
      "loss": 1.561,
      "step": 254
    },
    {
      "epoch": 1.0408580183861083,
      "grad_norm": 0.877798318862915,
      "learning_rate": 9.247194109719446e-06,
      "loss": 1.5281,
      "step": 255
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 0.80463707447052,
      "learning_rate": 9.237704614464157e-06,
      "loss": 1.5551,
      "step": 256
    },
    {
      "epoch": 1.04902962206333,
      "grad_norm": 0.8087096214294434,
      "learning_rate": 9.228160616040555e-06,
      "loss": 1.5665,
      "step": 257
    },
    {
      "epoch": 1.0531154239019407,
      "grad_norm": 0.7685936689376831,
      "learning_rate": 9.218562237198624e-06,
      "loss": 1.4297,
      "step": 258
    },
    {
      "epoch": 1.0572012257405516,
      "grad_norm": 0.7632666230201721,
      "learning_rate": 9.208909601387748e-06,
      "loss": 1.5218,
      "step": 259
    },
    {
      "epoch": 1.0612870275791624,
      "grad_norm": 0.802725613117218,
      "learning_rate": 9.19920283275515e-06,
      "loss": 1.5163,
      "step": 260
    },
    {
      "epoch": 1.0653728294177733,
      "grad_norm": 0.8542250990867615,
      "learning_rate": 9.189442056144271e-06,
      "loss": 1.5225,
      "step": 261
    },
    {
      "epoch": 1.069458631256384,
      "grad_norm": 0.7980052828788757,
      "learning_rate": 9.179627397093184e-06,
      "loss": 1.5216,
      "step": 262
    },
    {
      "epoch": 1.0735444330949948,
      "grad_norm": 0.7846449017524719,
      "learning_rate": 9.169758981832964e-06,
      "loss": 1.5577,
      "step": 263
    },
    {
      "epoch": 1.0776302349336058,
      "grad_norm": 0.8401265740394592,
      "learning_rate": 9.15983693728607e-06,
      "loss": 1.4495,
      "step": 264
    },
    {
      "epoch": 1.0817160367722165,
      "grad_norm": 0.7411726713180542,
      "learning_rate": 9.149861391064714e-06,
      "loss": 1.4884,
      "step": 265
    },
    {
      "epoch": 1.0858018386108275,
      "grad_norm": 0.8734695315361023,
      "learning_rate": 9.139832471469224e-06,
      "loss": 1.5144,
      "step": 266
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 0.9423707723617554,
      "learning_rate": 9.12975030748638e-06,
      "loss": 1.6712,
      "step": 267
    },
    {
      "epoch": 1.093973442288049,
      "grad_norm": 0.8205363750457764,
      "learning_rate": 9.119615028787771e-06,
      "loss": 1.5803,
      "step": 268
    },
    {
      "epoch": 1.09805924412666,
      "grad_norm": 0.7784472107887268,
      "learning_rate": 9.10942676572812e-06,
      "loss": 1.5074,
      "step": 269
    },
    {
      "epoch": 1.1021450459652706,
      "grad_norm": 0.8008567690849304,
      "learning_rate": 9.0991856493436e-06,
      "loss": 1.4687,
      "step": 270
    },
    {
      "epoch": 1.1062308478038816,
      "grad_norm": 0.8330549001693726,
      "learning_rate": 9.088891811350164e-06,
      "loss": 1.4861,
      "step": 271
    },
    {
      "epoch": 1.1103166496424923,
      "grad_norm": 0.7925741672515869,
      "learning_rate": 9.07854538414184e-06,
      "loss": 1.5611,
      "step": 272
    },
    {
      "epoch": 1.1144024514811033,
      "grad_norm": 0.8315894603729248,
      "learning_rate": 9.06814650078903e-06,
      "loss": 1.4283,
      "step": 273
    },
    {
      "epoch": 1.118488253319714,
      "grad_norm": 0.7448769807815552,
      "learning_rate": 9.057695295036806e-06,
      "loss": 1.3893,
      "step": 274
    },
    {
      "epoch": 1.1225740551583248,
      "grad_norm": 0.8177236318588257,
      "learning_rate": 9.047191901303176e-06,
      "loss": 1.512,
      "step": 275
    },
    {
      "epoch": 1.1266598569969357,
      "grad_norm": 0.7749916315078735,
      "learning_rate": 9.036636454677363e-06,
      "loss": 1.6248,
      "step": 276
    },
    {
      "epoch": 1.1307456588355465,
      "grad_norm": 0.7649291753768921,
      "learning_rate": 9.026029090918076e-06,
      "loss": 1.5051,
      "step": 277
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 0.7827522158622742,
      "learning_rate": 9.015369946451749e-06,
      "loss": 1.5133,
      "step": 278
    },
    {
      "epoch": 1.1389172625127681,
      "grad_norm": 0.7906701564788818,
      "learning_rate": 9.004659158370788e-06,
      "loss": 1.5255,
      "step": 279
    },
    {
      "epoch": 1.1430030643513789,
      "grad_norm": 0.8074409365653992,
      "learning_rate": 8.993896864431825e-06,
      "loss": 1.6319,
      "step": 280
    },
    {
      "epoch": 1.1470888661899898,
      "grad_norm": 0.8793272972106934,
      "learning_rate": 8.983083203053924e-06,
      "loss": 1.5358,
      "step": 281
    },
    {
      "epoch": 1.1511746680286006,
      "grad_norm": 0.8079075217247009,
      "learning_rate": 8.972218313316812e-06,
      "loss": 1.4518,
      "step": 282
    },
    {
      "epoch": 1.1552604698672115,
      "grad_norm": 0.7733301520347595,
      "learning_rate": 8.96130233495909e-06,
      "loss": 1.4323,
      "step": 283
    },
    {
      "epoch": 1.1593462717058223,
      "grad_norm": 0.790723443031311,
      "learning_rate": 8.950335408376438e-06,
      "loss": 1.5589,
      "step": 284
    },
    {
      "epoch": 1.163432073544433,
      "grad_norm": 0.7364273071289062,
      "learning_rate": 8.939317674619797e-06,
      "loss": 1.5209,
      "step": 285
    },
    {
      "epoch": 1.167517875383044,
      "grad_norm": 0.9020221829414368,
      "learning_rate": 8.928249275393572e-06,
      "loss": 1.5173,
      "step": 286
    },
    {
      "epoch": 1.1716036772216547,
      "grad_norm": 0.7394547462463379,
      "learning_rate": 8.917130353053798e-06,
      "loss": 1.5851,
      "step": 287
    },
    {
      "epoch": 1.1756894790602657,
      "grad_norm": 0.8539962768554688,
      "learning_rate": 8.905961050606311e-06,
      "loss": 1.5613,
      "step": 288
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 0.8378245830535889,
      "learning_rate": 8.894741511704911e-06,
      "loss": 1.5371,
      "step": 289
    },
    {
      "epoch": 1.1838610827374871,
      "grad_norm": 0.874537467956543,
      "learning_rate": 8.883471880649514e-06,
      "loss": 1.5442,
      "step": 290
    },
    {
      "epoch": 1.187946884576098,
      "grad_norm": 0.8247148990631104,
      "learning_rate": 8.872152302384294e-06,
      "loss": 1.5779,
      "step": 291
    },
    {
      "epoch": 1.1920326864147088,
      "grad_norm": 0.7660832405090332,
      "learning_rate": 8.860782922495821e-06,
      "loss": 1.4738,
      "step": 292
    },
    {
      "epoch": 1.1961184882533198,
      "grad_norm": 0.8574827909469604,
      "learning_rate": 8.84936388721119e-06,
      "loss": 1.4742,
      "step": 293
    },
    {
      "epoch": 1.2002042900919305,
      "grad_norm": 0.7999985218048096,
      "learning_rate": 8.837895343396132e-06,
      "loss": 1.6662,
      "step": 294
    },
    {
      "epoch": 1.2042900919305413,
      "grad_norm": 0.9391121864318848,
      "learning_rate": 8.826377438553138e-06,
      "loss": 1.5503,
      "step": 295
    },
    {
      "epoch": 1.2083758937691522,
      "grad_norm": 0.8536441326141357,
      "learning_rate": 8.814810320819551e-06,
      "loss": 1.5576,
      "step": 296
    },
    {
      "epoch": 1.212461695607763,
      "grad_norm": 0.9065053462982178,
      "learning_rate": 8.803194138965665e-06,
      "loss": 1.6257,
      "step": 297
    },
    {
      "epoch": 1.216547497446374,
      "grad_norm": 0.7807546257972717,
      "learning_rate": 8.791529042392813e-06,
      "loss": 1.4919,
      "step": 298
    },
    {
      "epoch": 1.2206332992849847,
      "grad_norm": 0.7594956159591675,
      "learning_rate": 8.779815181131444e-06,
      "loss": 1.4702,
      "step": 299
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 0.7585300803184509,
      "learning_rate": 8.76805270583919e-06,
      "loss": 1.5555,
      "step": 300
    },
    {
      "epoch": 1.2288049029622063,
      "grad_norm": 0.7367005348205566,
      "learning_rate": 8.756241767798934e-06,
      "loss": 1.3994,
      "step": 301
    },
    {
      "epoch": 1.232890704800817,
      "grad_norm": 0.8184664845466614,
      "learning_rate": 8.744382518916866e-06,
      "loss": 1.6789,
      "step": 302
    },
    {
      "epoch": 1.236976506639428,
      "grad_norm": 0.7969249486923218,
      "learning_rate": 8.732475111720513e-06,
      "loss": 1.5229,
      "step": 303
    },
    {
      "epoch": 1.2410623084780388,
      "grad_norm": 0.7626066207885742,
      "learning_rate": 8.720519699356804e-06,
      "loss": 1.5233,
      "step": 304
    },
    {
      "epoch": 1.2451481103166497,
      "grad_norm": 0.804427981376648,
      "learning_rate": 8.708516435590075e-06,
      "loss": 1.5625,
      "step": 305
    },
    {
      "epoch": 1.2492339121552605,
      "grad_norm": 0.8922007083892822,
      "learning_rate": 8.696465474800109e-06,
      "loss": 1.517,
      "step": 306
    },
    {
      "epoch": 1.2533197139938714,
      "grad_norm": 0.8083730340003967,
      "learning_rate": 8.684366971980139e-06,
      "loss": 1.5177,
      "step": 307
    },
    {
      "epoch": 1.2574055158324822,
      "grad_norm": 0.813470184803009,
      "learning_rate": 8.672221082734857e-06,
      "loss": 1.5744,
      "step": 308
    },
    {
      "epoch": 1.261491317671093,
      "grad_norm": 0.7859700918197632,
      "learning_rate": 8.660027963278421e-06,
      "loss": 1.5099,
      "step": 309
    },
    {
      "epoch": 1.2655771195097039,
      "grad_norm": 0.751887857913971,
      "learning_rate": 8.647787770432439e-06,
      "loss": 1.5092,
      "step": 310
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 0.8037948608398438,
      "learning_rate": 8.635500661623946e-06,
      "loss": 1.4617,
      "step": 311
    },
    {
      "epoch": 1.2737487231869253,
      "grad_norm": 0.842064380645752,
      "learning_rate": 8.623166794883393e-06,
      "loss": 1.4511,
      "step": 312
    },
    {
      "epoch": 1.2778345250255363,
      "grad_norm": 0.7675585746765137,
      "learning_rate": 8.610786328842602e-06,
      "loss": 1.4774,
      "step": 313
    },
    {
      "epoch": 1.281920326864147,
      "grad_norm": 0.7021605372428894,
      "learning_rate": 8.598359422732736e-06,
      "loss": 1.4887,
      "step": 314
    },
    {
      "epoch": 1.286006128702758,
      "grad_norm": 0.8544597625732422,
      "learning_rate": 8.585886236382244e-06,
      "loss": 1.4611,
      "step": 315
    },
    {
      "epoch": 1.2900919305413687,
      "grad_norm": 0.754283607006073,
      "learning_rate": 8.573366930214807e-06,
      "loss": 1.5539,
      "step": 316
    },
    {
      "epoch": 1.2941777323799797,
      "grad_norm": 0.8407593965530396,
      "learning_rate": 8.560801665247273e-06,
      "loss": 1.4956,
      "step": 317
    },
    {
      "epoch": 1.2982635342185904,
      "grad_norm": 0.7430266737937927,
      "learning_rate": 8.548190603087594e-06,
      "loss": 1.6089,
      "step": 318
    },
    {
      "epoch": 1.3023493360572012,
      "grad_norm": 0.808535635471344,
      "learning_rate": 8.535533905932739e-06,
      "loss": 1.5348,
      "step": 319
    },
    {
      "epoch": 1.3064351378958121,
      "grad_norm": 0.8748639225959778,
      "learning_rate": 8.522831736566607e-06,
      "loss": 1.5126,
      "step": 320
    },
    {
      "epoch": 1.3105209397344229,
      "grad_norm": 0.8359003663063049,
      "learning_rate": 8.510084258357947e-06,
      "loss": 1.6569,
      "step": 321
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 0.861433744430542,
      "learning_rate": 8.497291635258235e-06,
      "loss": 1.468,
      "step": 322
    },
    {
      "epoch": 1.3186925434116445,
      "grad_norm": 0.8519131541252136,
      "learning_rate": 8.484454031799588e-06,
      "loss": 1.487,
      "step": 323
    },
    {
      "epoch": 1.3227783452502553,
      "grad_norm": 0.8033623099327087,
      "learning_rate": 8.471571613092626e-06,
      "loss": 1.4889,
      "step": 324
    },
    {
      "epoch": 1.3268641470888662,
      "grad_norm": 0.7894397377967834,
      "learning_rate": 8.458644544824371e-06,
      "loss": 1.5257,
      "step": 325
    },
    {
      "epoch": 1.330949948927477,
      "grad_norm": 0.879136323928833,
      "learning_rate": 8.445672993256095e-06,
      "loss": 1.6276,
      "step": 326
    },
    {
      "epoch": 1.335035750766088,
      "grad_norm": 0.7890186309814453,
      "learning_rate": 8.4326571252212e-06,
      "loss": 1.6048,
      "step": 327
    },
    {
      "epoch": 1.3391215526046987,
      "grad_norm": 0.7880071997642517,
      "learning_rate": 8.419597108123054e-06,
      "loss": 1.4107,
      "step": 328
    },
    {
      "epoch": 1.3432073544433094,
      "grad_norm": 0.8717417120933533,
      "learning_rate": 8.406493109932856e-06,
      "loss": 1.454,
      "step": 329
    },
    {
      "epoch": 1.3472931562819204,
      "grad_norm": 0.7909759283065796,
      "learning_rate": 8.393345299187463e-06,
      "loss": 1.5573,
      "step": 330
    },
    {
      "epoch": 1.351378958120531,
      "grad_norm": 0.7730523347854614,
      "learning_rate": 8.380153844987225e-06,
      "loss": 1.4424,
      "step": 331
    },
    {
      "epoch": 1.355464759959142,
      "grad_norm": 0.7944605946540833,
      "learning_rate": 8.366918916993817e-06,
      "loss": 1.6493,
      "step": 332
    },
    {
      "epoch": 1.3595505617977528,
      "grad_norm": 0.8007334470748901,
      "learning_rate": 8.353640685428045e-06,
      "loss": 1.4988,
      "step": 333
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.8276387453079224,
      "learning_rate": 8.340319321067668e-06,
      "loss": 1.6772,
      "step": 334
    },
    {
      "epoch": 1.3677221654749745,
      "grad_norm": 0.8055758476257324,
      "learning_rate": 8.326954995245194e-06,
      "loss": 1.4727,
      "step": 335
    },
    {
      "epoch": 1.3718079673135852,
      "grad_norm": 0.8051984906196594,
      "learning_rate": 8.313547879845682e-06,
      "loss": 1.46,
      "step": 336
    },
    {
      "epoch": 1.3758937691521962,
      "grad_norm": 0.7948698997497559,
      "learning_rate": 8.300098147304523e-06,
      "loss": 1.4501,
      "step": 337
    },
    {
      "epoch": 1.379979570990807,
      "grad_norm": 0.7521846294403076,
      "learning_rate": 8.286605970605234e-06,
      "loss": 1.4299,
      "step": 338
    },
    {
      "epoch": 1.3840653728294177,
      "grad_norm": 0.7203874588012695,
      "learning_rate": 8.273071523277223e-06,
      "loss": 1.4115,
      "step": 339
    },
    {
      "epoch": 1.3881511746680286,
      "grad_norm": 0.8733167052268982,
      "learning_rate": 8.259494979393563e-06,
      "loss": 1.5369,
      "step": 340
    },
    {
      "epoch": 1.3922369765066394,
      "grad_norm": 0.8101521730422974,
      "learning_rate": 8.245876513568749e-06,
      "loss": 1.4088,
      "step": 341
    },
    {
      "epoch": 1.3963227783452503,
      "grad_norm": 0.7971808910369873,
      "learning_rate": 8.232216300956457e-06,
      "loss": 1.4014,
      "step": 342
    },
    {
      "epoch": 1.400408580183861,
      "grad_norm": 0.7763895988464355,
      "learning_rate": 8.218514517247287e-06,
      "loss": 1.5291,
      "step": 343
    },
    {
      "epoch": 1.404494382022472,
      "grad_norm": 0.8161207437515259,
      "learning_rate": 8.204771338666504e-06,
      "loss": 1.4547,
      "step": 344
    },
    {
      "epoch": 1.4085801838610827,
      "grad_norm": 0.8467191457748413,
      "learning_rate": 8.190986941971773e-06,
      "loss": 1.4073,
      "step": 345
    },
    {
      "epoch": 1.4126659856996935,
      "grad_norm": 0.812650203704834,
      "learning_rate": 8.177161504450887e-06,
      "loss": 1.5296,
      "step": 346
    },
    {
      "epoch": 1.4167517875383044,
      "grad_norm": 0.8035727739334106,
      "learning_rate": 8.163295203919486e-06,
      "loss": 1.476,
      "step": 347
    },
    {
      "epoch": 1.4208375893769152,
      "grad_norm": 0.8378661274909973,
      "learning_rate": 8.149388218718763e-06,
      "loss": 1.5057,
      "step": 348
    },
    {
      "epoch": 1.424923391215526,
      "grad_norm": 0.7915577292442322,
      "learning_rate": 8.135440727713179e-06,
      "loss": 1.4347,
      "step": 349
    },
    {
      "epoch": 1.4290091930541369,
      "grad_norm": 0.7738305926322937,
      "learning_rate": 8.121452910288164e-06,
      "loss": 1.433,
      "step": 350
    },
    {
      "epoch": 1.4330949948927478,
      "grad_norm": 0.8240776658058167,
      "learning_rate": 8.107424946347801e-06,
      "loss": 1.4574,
      "step": 351
    },
    {
      "epoch": 1.4371807967313586,
      "grad_norm": 0.7924173474311829,
      "learning_rate": 8.093357016312518e-06,
      "loss": 1.4359,
      "step": 352
    },
    {
      "epoch": 1.4412665985699693,
      "grad_norm": 0.9301562905311584,
      "learning_rate": 8.079249301116765e-06,
      "loss": 1.6351,
      "step": 353
    },
    {
      "epoch": 1.4453524004085803,
      "grad_norm": 0.8300731182098389,
      "learning_rate": 8.06510198220669e-06,
      "loss": 1.5462,
      "step": 354
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 0.7852367758750916,
      "learning_rate": 8.050915241537802e-06,
      "loss": 1.4817,
      "step": 355
    },
    {
      "epoch": 1.4535240040858017,
      "grad_norm": 0.8947411179542542,
      "learning_rate": 8.036689261572636e-06,
      "loss": 1.5216,
      "step": 356
    },
    {
      "epoch": 1.4576098059244127,
      "grad_norm": 0.8475333452224731,
      "learning_rate": 8.022424225278394e-06,
      "loss": 1.5259,
      "step": 357
    },
    {
      "epoch": 1.4616956077630234,
      "grad_norm": 0.8591018915176392,
      "learning_rate": 8.008120316124612e-06,
      "loss": 1.5419,
      "step": 358
    },
    {
      "epoch": 1.4657814096016344,
      "grad_norm": 0.7490316033363342,
      "learning_rate": 7.993777718080782e-06,
      "loss": 1.5127,
      "step": 359
    },
    {
      "epoch": 1.4698672114402451,
      "grad_norm": 0.8247952461242676,
      "learning_rate": 7.97939661561399e-06,
      "loss": 1.4781,
      "step": 360
    },
    {
      "epoch": 1.473953013278856,
      "grad_norm": 0.825117826461792,
      "learning_rate": 7.964977193686551e-06,
      "loss": 1.4638,
      "step": 361
    },
    {
      "epoch": 1.4780388151174668,
      "grad_norm": 0.8420026898384094,
      "learning_rate": 7.950519637753623e-06,
      "loss": 1.5675,
      "step": 362
    },
    {
      "epoch": 1.4821246169560776,
      "grad_norm": 0.8234542012214661,
      "learning_rate": 7.936024133760825e-06,
      "loss": 1.5183,
      "step": 363
    },
    {
      "epoch": 1.4862104187946885,
      "grad_norm": 0.8512513041496277,
      "learning_rate": 7.921490868141843e-06,
      "loss": 1.5536,
      "step": 364
    },
    {
      "epoch": 1.4902962206332993,
      "grad_norm": 0.8005079627037048,
      "learning_rate": 7.906920027816034e-06,
      "loss": 1.4661,
      "step": 365
    },
    {
      "epoch": 1.49438202247191,
      "grad_norm": 0.7688696384429932,
      "learning_rate": 7.89231180018602e-06,
      "loss": 1.5462,
      "step": 366
    },
    {
      "epoch": 1.498467824310521,
      "grad_norm": 0.8760764002799988,
      "learning_rate": 7.877666373135287e-06,
      "loss": 1.4788,
      "step": 367
    },
    {
      "epoch": 1.502553626149132,
      "grad_norm": 0.7730899453163147,
      "learning_rate": 7.862983935025745e-06,
      "loss": 1.5326,
      "step": 368
    },
    {
      "epoch": 1.5066394279877426,
      "grad_norm": 0.7986984252929688,
      "learning_rate": 7.848264674695338e-06,
      "loss": 1.4203,
      "step": 369
    },
    {
      "epoch": 1.5107252298263534,
      "grad_norm": 0.8527814745903015,
      "learning_rate": 7.83350878145559e-06,
      "loss": 1.4705,
      "step": 370
    },
    {
      "epoch": 1.5148110316649643,
      "grad_norm": 0.7956666350364685,
      "learning_rate": 7.818716445089177e-06,
      "loss": 1.4976,
      "step": 371
    },
    {
      "epoch": 1.518896833503575,
      "grad_norm": 0.7266256809234619,
      "learning_rate": 7.80388785584749e-06,
      "loss": 1.4946,
      "step": 372
    },
    {
      "epoch": 1.5229826353421858,
      "grad_norm": 0.9460698962211609,
      "learning_rate": 7.789023204448189e-06,
      "loss": 1.6005,
      "step": 373
    },
    {
      "epoch": 1.5270684371807968,
      "grad_norm": 0.758091390132904,
      "learning_rate": 7.774122682072743e-06,
      "loss": 1.5159,
      "step": 374
    },
    {
      "epoch": 1.5311542390194075,
      "grad_norm": 0.8327276706695557,
      "learning_rate": 7.759186480363974e-06,
      "loss": 1.5752,
      "step": 375
    },
    {
      "epoch": 1.5352400408580182,
      "grad_norm": 0.8294745087623596,
      "learning_rate": 7.744214791423597e-06,
      "loss": 1.4798,
      "step": 376
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 0.7205362319946289,
      "learning_rate": 7.729207807809743e-06,
      "loss": 1.4719,
      "step": 377
    },
    {
      "epoch": 1.5434116445352402,
      "grad_norm": 1.0108789205551147,
      "learning_rate": 7.714165722534485e-06,
      "loss": 1.5453,
      "step": 378
    },
    {
      "epoch": 1.547497446373851,
      "grad_norm": 0.8491654396057129,
      "learning_rate": 7.699088729061355e-06,
      "loss": 1.5566,
      "step": 379
    },
    {
      "epoch": 1.5515832482124616,
      "grad_norm": 0.7721633911132812,
      "learning_rate": 7.68397702130286e-06,
      "loss": 1.4093,
      "step": 380
    },
    {
      "epoch": 1.5556690500510726,
      "grad_norm": 1.0031704902648926,
      "learning_rate": 7.668830793617976e-06,
      "loss": 1.5286,
      "step": 381
    },
    {
      "epoch": 1.5597548518896833,
      "grad_norm": 0.7971066236495972,
      "learning_rate": 7.653650240809667e-06,
      "loss": 1.5055,
      "step": 382
    },
    {
      "epoch": 1.563840653728294,
      "grad_norm": 0.759024977684021,
      "learning_rate": 7.63843555812236e-06,
      "loss": 1.505,
      "step": 383
    },
    {
      "epoch": 1.567926455566905,
      "grad_norm": 0.7666231989860535,
      "learning_rate": 7.6231869412394495e-06,
      "loss": 1.4114,
      "step": 384
    },
    {
      "epoch": 1.572012257405516,
      "grad_norm": 0.8227419257164001,
      "learning_rate": 7.60790458628077e-06,
      "loss": 1.554,
      "step": 385
    },
    {
      "epoch": 1.5760980592441267,
      "grad_norm": 0.7599053382873535,
      "learning_rate": 7.592588689800077e-06,
      "loss": 1.3961,
      "step": 386
    },
    {
      "epoch": 1.5801838610827375,
      "grad_norm": 0.9195758104324341,
      "learning_rate": 7.577239448782523e-06,
      "loss": 1.5089,
      "step": 387
    },
    {
      "epoch": 1.5842696629213484,
      "grad_norm": 0.8005908131599426,
      "learning_rate": 7.56185706064212e-06,
      "loss": 1.4847,
      "step": 388
    },
    {
      "epoch": 1.5883554647599591,
      "grad_norm": 0.7134295105934143,
      "learning_rate": 7.546441723219198e-06,
      "loss": 1.4002,
      "step": 389
    },
    {
      "epoch": 1.5924412665985699,
      "grad_norm": 0.8225773572921753,
      "learning_rate": 7.530993634777868e-06,
      "loss": 1.5159,
      "step": 390
    },
    {
      "epoch": 1.5965270684371808,
      "grad_norm": 0.7929695248603821,
      "learning_rate": 7.5155129940034675e-06,
      "loss": 1.547,
      "step": 391
    },
    {
      "epoch": 1.6006128702757916,
      "grad_norm": 0.8277504444122314,
      "learning_rate": 7.500000000000001e-06,
      "loss": 1.5322,
      "step": 392
    },
    {
      "epoch": 1.6006128702757916,
      "eval_loss": 1.468184471130371,
      "eval_runtime": 18.6594,
      "eval_samples_per_second": 11.094,
      "eval_steps_per_second": 2.787,
      "step": 392
    },
    {
      "epoch": 1.6046986721144023,
      "grad_norm": 0.8096811175346375,
      "learning_rate": 7.484454852287586e-06,
      "loss": 1.5174,
      "step": 393
    },
    {
      "epoch": 1.6087844739530133,
      "grad_norm": 0.8587949275970459,
      "learning_rate": 7.468877750799887e-06,
      "loss": 1.4441,
      "step": 394
    },
    {
      "epoch": 1.6128702757916242,
      "grad_norm": 0.938616931438446,
      "learning_rate": 7.453268895881537e-06,
      "loss": 1.4792,
      "step": 395
    },
    {
      "epoch": 1.616956077630235,
      "grad_norm": 0.8473846316337585,
      "learning_rate": 7.437628488285568e-06,
      "loss": 1.429,
      "step": 396
    },
    {
      "epoch": 1.6210418794688457,
      "grad_norm": 0.8548424243927002,
      "learning_rate": 7.421956729170823e-06,
      "loss": 1.444,
      "step": 397
    },
    {
      "epoch": 1.6251276813074567,
      "grad_norm": 0.7680855989456177,
      "learning_rate": 7.40625382009938e-06,
      "loss": 1.4965,
      "step": 398
    },
    {
      "epoch": 1.6292134831460674,
      "grad_norm": 0.7970052361488342,
      "learning_rate": 7.390519963033942e-06,
      "loss": 1.5082,
      "step": 399
    },
    {
      "epoch": 1.6332992849846781,
      "grad_norm": 0.8279876708984375,
      "learning_rate": 7.374755360335253e-06,
      "loss": 1.557,
      "step": 400
    },
    {
      "epoch": 1.637385086823289,
      "grad_norm": 0.8594110012054443,
      "learning_rate": 7.358960214759492e-06,
      "loss": 1.3784,
      "step": 401
    },
    {
      "epoch": 1.6414708886619,
      "grad_norm": 0.7372906804084778,
      "learning_rate": 7.343134729455667e-06,
      "loss": 1.4283,
      "step": 402
    },
    {
      "epoch": 1.6455566905005106,
      "grad_norm": 0.7802755236625671,
      "learning_rate": 7.327279107962995e-06,
      "loss": 1.3508,
      "step": 403
    },
    {
      "epoch": 1.6496424923391215,
      "grad_norm": 0.6996581554412842,
      "learning_rate": 7.311393554208292e-06,
      "loss": 1.4353,
      "step": 404
    },
    {
      "epoch": 1.6537282941777325,
      "grad_norm": 0.8215770125389099,
      "learning_rate": 7.295478272503347e-06,
      "loss": 1.4911,
      "step": 405
    },
    {
      "epoch": 1.6578140960163432,
      "grad_norm": 0.8594485521316528,
      "learning_rate": 7.279533467542295e-06,
      "loss": 1.6251,
      "step": 406
    },
    {
      "epoch": 1.661899897854954,
      "grad_norm": 0.7741272449493408,
      "learning_rate": 7.2635593443989814e-06,
      "loss": 1.4638,
      "step": 407
    },
    {
      "epoch": 1.665985699693565,
      "grad_norm": 0.8204206824302673,
      "learning_rate": 7.24755610852433e-06,
      "loss": 1.4544,
      "step": 408
    },
    {
      "epoch": 1.6700715015321757,
      "grad_norm": 0.7870428562164307,
      "learning_rate": 7.2315239657436955e-06,
      "loss": 1.5119,
      "step": 409
    },
    {
      "epoch": 1.6741573033707864,
      "grad_norm": 0.8265226483345032,
      "learning_rate": 7.2154631222542205e-06,
      "loss": 1.5045,
      "step": 410
    },
    {
      "epoch": 1.6782431052093973,
      "grad_norm": 0.860124945640564,
      "learning_rate": 7.199373784622178e-06,
      "loss": 1.5328,
      "step": 411
    },
    {
      "epoch": 1.6823289070480083,
      "grad_norm": 0.8167992234230042,
      "learning_rate": 7.183256159780321e-06,
      "loss": 1.5701,
      "step": 412
    },
    {
      "epoch": 1.686414708886619,
      "grad_norm": 0.9587753415107727,
      "learning_rate": 7.167110455025214e-06,
      "loss": 1.4978,
      "step": 413
    },
    {
      "epoch": 1.6905005107252298,
      "grad_norm": 0.8179387450218201,
      "learning_rate": 7.150936878014578e-06,
      "loss": 1.5088,
      "step": 414
    },
    {
      "epoch": 1.6945863125638407,
      "grad_norm": 0.8254221677780151,
      "learning_rate": 7.134735636764606e-06,
      "loss": 1.3851,
      "step": 415
    },
    {
      "epoch": 1.6986721144024515,
      "grad_norm": 0.8247527480125427,
      "learning_rate": 7.118506939647295e-06,
      "loss": 1.5815,
      "step": 416
    },
    {
      "epoch": 1.7027579162410622,
      "grad_norm": 0.7893041968345642,
      "learning_rate": 7.102250995387767e-06,
      "loss": 1.5139,
      "step": 417
    },
    {
      "epoch": 1.7068437180796732,
      "grad_norm": 0.7710625529289246,
      "learning_rate": 7.085968013061585e-06,
      "loss": 1.3725,
      "step": 418
    },
    {
      "epoch": 1.7109295199182841,
      "grad_norm": 0.7905916571617126,
      "learning_rate": 7.069658202092056e-06,
      "loss": 1.4529,
      "step": 419
    },
    {
      "epoch": 1.7150153217568946,
      "grad_norm": 0.7867981195449829,
      "learning_rate": 7.053321772247546e-06,
      "loss": 1.4413,
      "step": 420
    },
    {
      "epoch": 1.7191011235955056,
      "grad_norm": 0.7845027446746826,
      "learning_rate": 7.036958933638779e-06,
      "loss": 1.4566,
      "step": 421
    },
    {
      "epoch": 1.7231869254341166,
      "grad_norm": 0.7521723508834839,
      "learning_rate": 7.020569896716137e-06,
      "loss": 1.4955,
      "step": 422
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.7882096767425537,
      "learning_rate": 7.00415487226695e-06,
      "loss": 1.5679,
      "step": 423
    },
    {
      "epoch": 1.731358529111338,
      "grad_norm": 0.7825708985328674,
      "learning_rate": 6.987714071412781e-06,
      "loss": 1.4377,
      "step": 424
    },
    {
      "epoch": 1.735444330949949,
      "grad_norm": 0.790661096572876,
      "learning_rate": 6.971247705606723e-06,
      "loss": 1.4175,
      "step": 425
    },
    {
      "epoch": 1.7395301327885597,
      "grad_norm": 0.8005431890487671,
      "learning_rate": 6.9547559866306695e-06,
      "loss": 1.5093,
      "step": 426
    },
    {
      "epoch": 1.7436159346271705,
      "grad_norm": 0.8137386441230774,
      "learning_rate": 6.938239126592592e-06,
      "loss": 1.5572,
      "step": 427
    },
    {
      "epoch": 1.7477017364657814,
      "grad_norm": 0.8061034679412842,
      "learning_rate": 6.9216973379238175e-06,
      "loss": 1.4461,
      "step": 428
    },
    {
      "epoch": 1.7517875383043924,
      "grad_norm": 0.7487004995346069,
      "learning_rate": 6.905130833376286e-06,
      "loss": 1.505,
      "step": 429
    },
    {
      "epoch": 1.7558733401430031,
      "grad_norm": 0.8036941289901733,
      "learning_rate": 6.888539826019824e-06,
      "loss": 1.4498,
      "step": 430
    },
    {
      "epoch": 1.7599591419816139,
      "grad_norm": 0.8062487840652466,
      "learning_rate": 6.871924529239404e-06,
      "loss": 1.5797,
      "step": 431
    },
    {
      "epoch": 1.7640449438202248,
      "grad_norm": 0.8720464110374451,
      "learning_rate": 6.855285156732389e-06,
      "loss": 1.3537,
      "step": 432
    },
    {
      "epoch": 1.7681307456588355,
      "grad_norm": 0.796607494354248,
      "learning_rate": 6.8386219225057945e-06,
      "loss": 1.4094,
      "step": 433
    },
    {
      "epoch": 1.7722165474974463,
      "grad_norm": 0.8626528978347778,
      "learning_rate": 6.821935040873538e-06,
      "loss": 1.5041,
      "step": 434
    },
    {
      "epoch": 1.7763023493360572,
      "grad_norm": 0.8847924470901489,
      "learning_rate": 6.805224726453672e-06,
      "loss": 1.5137,
      "step": 435
    },
    {
      "epoch": 1.780388151174668,
      "grad_norm": 0.787237286567688,
      "learning_rate": 6.788491194165629e-06,
      "loss": 1.4202,
      "step": 436
    },
    {
      "epoch": 1.7844739530132787,
      "grad_norm": 0.8023776412010193,
      "learning_rate": 6.77173465922746e-06,
      "loss": 1.4337,
      "step": 437
    },
    {
      "epoch": 1.7885597548518897,
      "grad_norm": 0.7890459299087524,
      "learning_rate": 6.754955337153066e-06,
      "loss": 1.4001,
      "step": 438
    },
    {
      "epoch": 1.7926455566905006,
      "grad_norm": 0.8324351906776428,
      "learning_rate": 6.738153443749421e-06,
      "loss": 1.5107,
      "step": 439
    },
    {
      "epoch": 1.7967313585291114,
      "grad_norm": 0.7804716229438782,
      "learning_rate": 6.721329195113802e-06,
      "loss": 1.5531,
      "step": 440
    },
    {
      "epoch": 1.800817160367722,
      "grad_norm": 0.756460964679718,
      "learning_rate": 6.704482807631004e-06,
      "loss": 1.3817,
      "step": 441
    },
    {
      "epoch": 1.804902962206333,
      "grad_norm": 0.7908895611763,
      "learning_rate": 6.687614497970567e-06,
      "loss": 1.5128,
      "step": 442
    },
    {
      "epoch": 1.8089887640449438,
      "grad_norm": 0.8520108461380005,
      "learning_rate": 6.670724483083977e-06,
      "loss": 1.4777,
      "step": 443
    },
    {
      "epoch": 1.8130745658835545,
      "grad_norm": 0.784737229347229,
      "learning_rate": 6.653812980201882e-06,
      "loss": 1.4442,
      "step": 444
    },
    {
      "epoch": 1.8171603677221655,
      "grad_norm": 0.7747485041618347,
      "learning_rate": 6.636880206831298e-06,
      "loss": 1.4849,
      "step": 445
    },
    {
      "epoch": 1.8212461695607765,
      "grad_norm": 0.8070043325424194,
      "learning_rate": 6.6199263807528136e-06,
      "loss": 1.4518,
      "step": 446
    },
    {
      "epoch": 1.825331971399387,
      "grad_norm": 0.8262349963188171,
      "learning_rate": 6.602951720017785e-06,
      "loss": 1.4426,
      "step": 447
    },
    {
      "epoch": 1.829417773237998,
      "grad_norm": 0.8646184802055359,
      "learning_rate": 6.585956442945531e-06,
      "loss": 1.4954,
      "step": 448
    },
    {
      "epoch": 1.8335035750766089,
      "grad_norm": 0.7425416707992554,
      "learning_rate": 6.568940768120529e-06,
      "loss": 1.4546,
      "step": 449
    },
    {
      "epoch": 1.8375893769152196,
      "grad_norm": 0.8200052976608276,
      "learning_rate": 6.551904914389601e-06,
      "loss": 1.5702,
      "step": 450
    },
    {
      "epoch": 1.8416751787538304,
      "grad_norm": 0.8488803505897522,
      "learning_rate": 6.534849100859101e-06,
      "loss": 1.4236,
      "step": 451
    },
    {
      "epoch": 1.8457609805924413,
      "grad_norm": 0.7637798190116882,
      "learning_rate": 6.5177735468920935e-06,
      "loss": 1.4237,
      "step": 452
    },
    {
      "epoch": 1.849846782431052,
      "grad_norm": 0.7972750663757324,
      "learning_rate": 6.500678472105535e-06,
      "loss": 1.5612,
      "step": 453
    },
    {
      "epoch": 1.8539325842696628,
      "grad_norm": 0.8331711292266846,
      "learning_rate": 6.483564096367452e-06,
      "loss": 1.5107,
      "step": 454
    },
    {
      "epoch": 1.8580183861082737,
      "grad_norm": 0.9890512228012085,
      "learning_rate": 6.466430639794105e-06,
      "loss": 1.5433,
      "step": 455
    },
    {
      "epoch": 1.8621041879468847,
      "grad_norm": 0.7646751999855042,
      "learning_rate": 6.449278322747164e-06,
      "loss": 1.4115,
      "step": 456
    },
    {
      "epoch": 1.8661899897854954,
      "grad_norm": 0.7846587300300598,
      "learning_rate": 6.432107365830872e-06,
      "loss": 1.5666,
      "step": 457
    },
    {
      "epoch": 1.8702757916241062,
      "grad_norm": 0.7564422488212585,
      "learning_rate": 6.414917989889212e-06,
      "loss": 1.5397,
      "step": 458
    },
    {
      "epoch": 1.8743615934627171,
      "grad_norm": 0.8977306485176086,
      "learning_rate": 6.39771041600306e-06,
      "loss": 1.4176,
      "step": 459
    },
    {
      "epoch": 1.8784473953013279,
      "grad_norm": 0.7944766879081726,
      "learning_rate": 6.380484865487346e-06,
      "loss": 1.3893,
      "step": 460
    },
    {
      "epoch": 1.8825331971399386,
      "grad_norm": 0.7919601798057556,
      "learning_rate": 6.3632415598882004e-06,
      "loss": 1.4729,
      "step": 461
    },
    {
      "epoch": 1.8866189989785496,
      "grad_norm": 0.8030367493629456,
      "learning_rate": 6.345980720980119e-06,
      "loss": 1.5214,
      "step": 462
    },
    {
      "epoch": 1.8907048008171605,
      "grad_norm": 0.8070223927497864,
      "learning_rate": 6.328702570763098e-06,
      "loss": 1.3337,
      "step": 463
    },
    {
      "epoch": 1.894790602655771,
      "grad_norm": 0.7925047874450684,
      "learning_rate": 6.311407331459781e-06,
      "loss": 1.4912,
      "step": 464
    },
    {
      "epoch": 1.898876404494382,
      "grad_norm": 0.8034906983375549,
      "learning_rate": 6.294095225512604e-06,
      "loss": 1.4131,
      "step": 465
    },
    {
      "epoch": 1.902962206332993,
      "grad_norm": 0.832209050655365,
      "learning_rate": 6.276766475580935e-06,
      "loss": 1.4689,
      "step": 466
    },
    {
      "epoch": 1.9070480081716037,
      "grad_norm": 0.847113311290741,
      "learning_rate": 6.259421304538207e-06,
      "loss": 1.4734,
      "step": 467
    },
    {
      "epoch": 1.9111338100102144,
      "grad_norm": 0.8094614148139954,
      "learning_rate": 6.242059935469051e-06,
      "loss": 1.4635,
      "step": 468
    },
    {
      "epoch": 1.9152196118488254,
      "grad_norm": 0.7947748899459839,
      "learning_rate": 6.224682591666431e-06,
      "loss": 1.441,
      "step": 469
    },
    {
      "epoch": 1.9193054136874361,
      "grad_norm": 0.8382415175437927,
      "learning_rate": 6.207289496628768e-06,
      "loss": 1.4771,
      "step": 470
    },
    {
      "epoch": 1.9233912155260469,
      "grad_norm": 0.8117170333862305,
      "learning_rate": 6.18988087405707e-06,
      "loss": 1.4552,
      "step": 471
    },
    {
      "epoch": 1.9274770173646578,
      "grad_norm": 0.798304557800293,
      "learning_rate": 6.1724569478520495e-06,
      "loss": 1.374,
      "step": 472
    },
    {
      "epoch": 1.9315628192032688,
      "grad_norm": 0.821649432182312,
      "learning_rate": 6.155017942111246e-06,
      "loss": 1.4781,
      "step": 473
    },
    {
      "epoch": 1.9356486210418795,
      "grad_norm": 0.7871229648590088,
      "learning_rate": 6.1375640811261486e-06,
      "loss": 1.3899,
      "step": 474
    },
    {
      "epoch": 1.9397344228804902,
      "grad_norm": 0.7953628897666931,
      "learning_rate": 6.120095589379299e-06,
      "loss": 1.5727,
      "step": 475
    },
    {
      "epoch": 1.9438202247191012,
      "grad_norm": 0.8006387948989868,
      "learning_rate": 6.102612691541422e-06,
      "loss": 1.4648,
      "step": 476
    },
    {
      "epoch": 1.947906026557712,
      "grad_norm": 0.8141502141952515,
      "learning_rate": 6.085115612468516e-06,
      "loss": 1.4707,
      "step": 477
    },
    {
      "epoch": 1.9519918283963227,
      "grad_norm": 0.8039414882659912,
      "learning_rate": 6.067604577198981e-06,
      "loss": 1.4359,
      "step": 478
    },
    {
      "epoch": 1.9560776302349336,
      "grad_norm": 0.778313398361206,
      "learning_rate": 6.0500798109507065e-06,
      "loss": 1.4472,
      "step": 479
    },
    {
      "epoch": 1.9601634320735446,
      "grad_norm": 0.8199807405471802,
      "learning_rate": 6.032541539118188e-06,
      "loss": 1.5435,
      "step": 480
    },
    {
      "epoch": 1.9642492339121551,
      "grad_norm": 0.9559541940689087,
      "learning_rate": 6.014989987269617e-06,
      "loss": 1.4644,
      "step": 481
    },
    {
      "epoch": 1.968335035750766,
      "grad_norm": 0.8902751207351685,
      "learning_rate": 5.997425381143994e-06,
      "loss": 1.4865,
      "step": 482
    },
    {
      "epoch": 1.972420837589377,
      "grad_norm": 0.8735816478729248,
      "learning_rate": 5.9798479466482106e-06,
      "loss": 1.5239,
      "step": 483
    },
    {
      "epoch": 1.9765066394279878,
      "grad_norm": 0.8058459162712097,
      "learning_rate": 5.96225790985415e-06,
      "loss": 1.4437,
      "step": 484
    },
    {
      "epoch": 1.9805924412665985,
      "grad_norm": 0.7943552732467651,
      "learning_rate": 5.944655496995783e-06,
      "loss": 1.476,
      "step": 485
    },
    {
      "epoch": 1.9846782431052095,
      "grad_norm": 0.7812731266021729,
      "learning_rate": 5.927040934466255e-06,
      "loss": 1.4245,
      "step": 486
    },
    {
      "epoch": 1.9887640449438202,
      "grad_norm": 0.7991986274719238,
      "learning_rate": 5.909414448814971e-06,
      "loss": 1.4751,
      "step": 487
    },
    {
      "epoch": 1.992849846782431,
      "grad_norm": 0.7881086468696594,
      "learning_rate": 5.891776266744686e-06,
      "loss": 1.4107,
      "step": 488
    },
    {
      "epoch": 1.996935648621042,
      "grad_norm": 0.7647078633308411,
      "learning_rate": 5.87412661510859e-06,
      "loss": 1.4187,
      "step": 489
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.9704172611236572,
      "learning_rate": 5.856465720907388e-06,
      "loss": 1.3997,
      "step": 490
    },
    {
      "epoch": 2.004085801838611,
      "grad_norm": 0.8278834223747253,
      "learning_rate": 5.838793811286381e-06,
      "loss": 1.4955,
      "step": 491
    },
    {
      "epoch": 2.0081716036772215,
      "grad_norm": 0.7457200884819031,
      "learning_rate": 5.821111113532545e-06,
      "loss": 1.4518,
      "step": 492
    },
    {
      "epoch": 2.0122574055158324,
      "grad_norm": 0.7733855843544006,
      "learning_rate": 5.803417855071603e-06,
      "loss": 1.5317,
      "step": 493
    },
    {
      "epoch": 2.0163432073544434,
      "grad_norm": 0.778010368347168,
      "learning_rate": 5.7857142634651135e-06,
      "loss": 1.4557,
      "step": 494
    },
    {
      "epoch": 2.0204290091930543,
      "grad_norm": 0.7967993021011353,
      "learning_rate": 5.7680005664075256e-06,
      "loss": 1.4677,
      "step": 495
    },
    {
      "epoch": 2.024514811031665,
      "grad_norm": 0.8295431137084961,
      "learning_rate": 5.7502769917232635e-06,
      "loss": 1.5087,
      "step": 496
    },
    {
      "epoch": 2.028600612870276,
      "grad_norm": 0.7427630424499512,
      "learning_rate": 5.732543767363794e-06,
      "loss": 1.4,
      "step": 497
    },
    {
      "epoch": 2.032686414708887,
      "grad_norm": 0.7718371152877808,
      "learning_rate": 5.7148011214046895e-06,
      "loss": 1.4067,
      "step": 498
    },
    {
      "epoch": 2.0367722165474973,
      "grad_norm": 0.7529418468475342,
      "learning_rate": 5.6970492820426994e-06,
      "loss": 1.4628,
      "step": 499
    },
    {
      "epoch": 2.0408580183861083,
      "grad_norm": 0.7760154008865356,
      "learning_rate": 5.679288477592815e-06,
      "loss": 1.4256,
      "step": 500
    },
    {
      "epoch": 2.044943820224719,
      "grad_norm": 0.8119626641273499,
      "learning_rate": 5.661518936485329e-06,
      "loss": 1.4443,
      "step": 501
    },
    {
      "epoch": 2.0490296220633297,
      "grad_norm": 0.7675799131393433,
      "learning_rate": 5.643740887262905e-06,
      "loss": 1.4515,
      "step": 502
    },
    {
      "epoch": 2.0531154239019407,
      "grad_norm": 0.7641684412956238,
      "learning_rate": 5.62595455857763e-06,
      "loss": 1.5079,
      "step": 503
    },
    {
      "epoch": 2.0572012257405516,
      "grad_norm": 0.7782613635063171,
      "learning_rate": 5.608160179188079e-06,
      "loss": 1.4117,
      "step": 504
    },
    {
      "epoch": 2.0612870275791626,
      "grad_norm": 0.7193266153335571,
      "learning_rate": 5.59035797795637e-06,
      "loss": 1.4498,
      "step": 505
    },
    {
      "epoch": 2.065372829417773,
      "grad_norm": 0.8541254997253418,
      "learning_rate": 5.572548183845222e-06,
      "loss": 1.5565,
      "step": 506
    },
    {
      "epoch": 2.069458631256384,
      "grad_norm": 0.9539790153503418,
      "learning_rate": 5.554731025915009e-06,
      "loss": 1.4091,
      "step": 507
    },
    {
      "epoch": 2.073544433094995,
      "grad_norm": 0.8412865400314331,
      "learning_rate": 5.536906733320816e-06,
      "loss": 1.4038,
      "step": 508
    },
    {
      "epoch": 2.0776302349336055,
      "grad_norm": 0.7877163887023926,
      "learning_rate": 5.519075535309488e-06,
      "loss": 1.6061,
      "step": 509
    },
    {
      "epoch": 2.0817160367722165,
      "grad_norm": 0.7711313962936401,
      "learning_rate": 5.501237661216687e-06,
      "loss": 1.3973,
      "step": 510
    },
    {
      "epoch": 2.0858018386108275,
      "grad_norm": 0.708228588104248,
      "learning_rate": 5.483393340463938e-06,
      "loss": 1.434,
      "step": 511
    },
    {
      "epoch": 2.0898876404494384,
      "grad_norm": 0.8697804808616638,
      "learning_rate": 5.465542802555677e-06,
      "loss": 1.5482,
      "step": 512
    },
    {
      "epoch": 2.093973442288049,
      "grad_norm": 0.7709740400314331,
      "learning_rate": 5.4476862770763054e-06,
      "loss": 1.4229,
      "step": 513
    },
    {
      "epoch": 2.09805924412666,
      "grad_norm": 0.7933114171028137,
      "learning_rate": 5.429823993687234e-06,
      "loss": 1.5153,
      "step": 514
    },
    {
      "epoch": 2.102145045965271,
      "grad_norm": 0.7529512047767639,
      "learning_rate": 5.411956182123924e-06,
      "loss": 1.4808,
      "step": 515
    },
    {
      "epoch": 2.1062308478038814,
      "grad_norm": 0.7534725666046143,
      "learning_rate": 5.394083072192944e-06,
      "loss": 1.4558,
      "step": 516
    },
    {
      "epoch": 2.1103166496424923,
      "grad_norm": 0.747339129447937,
      "learning_rate": 5.376204893769e-06,
      "loss": 1.4971,
      "step": 517
    },
    {
      "epoch": 2.1144024514811033,
      "grad_norm": 0.726996123790741,
      "learning_rate": 5.358321876791995e-06,
      "loss": 1.3926,
      "step": 518
    },
    {
      "epoch": 2.118488253319714,
      "grad_norm": 0.7779844403266907,
      "learning_rate": 5.340434251264055e-06,
      "loss": 1.4329,
      "step": 519
    },
    {
      "epoch": 2.1225740551583248,
      "grad_norm": 0.8480616807937622,
      "learning_rate": 5.322542247246583e-06,
      "loss": 1.4089,
      "step": 520
    },
    {
      "epoch": 2.1266598569969357,
      "grad_norm": 0.7168214321136475,
      "learning_rate": 5.304646094857293e-06,
      "loss": 1.5416,
      "step": 521
    },
    {
      "epoch": 2.1307456588355467,
      "grad_norm": 0.7803927659988403,
      "learning_rate": 5.286746024267257e-06,
      "loss": 1.3346,
      "step": 522
    },
    {
      "epoch": 2.134831460674157,
      "grad_norm": 0.7364909648895264,
      "learning_rate": 5.26884226569794e-06,
      "loss": 1.3549,
      "step": 523
    },
    {
      "epoch": 2.138917262512768,
      "grad_norm": 0.7719917297363281,
      "learning_rate": 5.2509350494182365e-06,
      "loss": 1.4255,
      "step": 524
    },
    {
      "epoch": 2.143003064351379,
      "grad_norm": 0.8680902719497681,
      "learning_rate": 5.233024605741515e-06,
      "loss": 1.3714,
      "step": 525
    },
    {
      "epoch": 2.1470888661899896,
      "grad_norm": 0.7697949409484863,
      "learning_rate": 5.215111165022653e-06,
      "loss": 1.5036,
      "step": 526
    },
    {
      "epoch": 2.1511746680286006,
      "grad_norm": 0.7679203152656555,
      "learning_rate": 5.197194957655075e-06,
      "loss": 1.4139,
      "step": 527
    },
    {
      "epoch": 2.1552604698672115,
      "grad_norm": 0.847908079624176,
      "learning_rate": 5.179276214067788e-06,
      "loss": 1.4683,
      "step": 528
    },
    {
      "epoch": 2.1593462717058225,
      "grad_norm": 0.8147554397583008,
      "learning_rate": 5.161355164722416e-06,
      "loss": 1.4815,
      "step": 529
    },
    {
      "epoch": 2.163432073544433,
      "grad_norm": 0.7995564937591553,
      "learning_rate": 5.143432040110243e-06,
      "loss": 1.4011,
      "step": 530
    },
    {
      "epoch": 2.167517875383044,
      "grad_norm": 0.8995360732078552,
      "learning_rate": 5.125507070749243e-06,
      "loss": 1.45,
      "step": 531
    },
    {
      "epoch": 2.171603677221655,
      "grad_norm": 0.7635642886161804,
      "learning_rate": 5.107580487181112e-06,
      "loss": 1.4663,
      "step": 532
    },
    {
      "epoch": 2.1756894790602654,
      "grad_norm": 0.7898370623588562,
      "learning_rate": 5.089652519968312e-06,
      "loss": 1.4644,
      "step": 533
    },
    {
      "epoch": 2.1797752808988764,
      "grad_norm": 0.8059664368629456,
      "learning_rate": 5.071723399691098e-06,
      "loss": 1.3758,
      "step": 534
    },
    {
      "epoch": 2.1838610827374874,
      "grad_norm": 0.7711989283561707,
      "learning_rate": 5.0537933569445585e-06,
      "loss": 1.4001,
      "step": 535
    },
    {
      "epoch": 2.187946884576098,
      "grad_norm": 0.7970855832099915,
      "learning_rate": 5.035862622335641e-06,
      "loss": 1.4838,
      "step": 536
    },
    {
      "epoch": 2.192032686414709,
      "grad_norm": 0.7698221802711487,
      "learning_rate": 5.017931426480194e-06,
      "loss": 1.4735,
      "step": 537
    },
    {
      "epoch": 2.19611848825332,
      "grad_norm": 0.8439189791679382,
      "learning_rate": 5e-06,
      "loss": 1.4678,
      "step": 538
    },
    {
      "epoch": 2.2002042900919307,
      "grad_norm": 0.7615057826042175,
      "learning_rate": 4.982068573519807e-06,
      "loss": 1.2595,
      "step": 539
    },
    {
      "epoch": 2.2042900919305413,
      "grad_norm": 0.7941576242446899,
      "learning_rate": 4.964137377664362e-06,
      "loss": 1.4735,
      "step": 540
    },
    {
      "epoch": 2.208375893769152,
      "grad_norm": 0.8927475810050964,
      "learning_rate": 4.946206643055443e-06,
      "loss": 1.3635,
      "step": 541
    },
    {
      "epoch": 2.212461695607763,
      "grad_norm": 0.7634361982345581,
      "learning_rate": 4.9282766003089025e-06,
      "loss": 1.3974,
      "step": 542
    },
    {
      "epoch": 2.2165474974463737,
      "grad_norm": 0.7929884195327759,
      "learning_rate": 4.91034748003169e-06,
      "loss": 1.4369,
      "step": 543
    },
    {
      "epoch": 2.2206332992849847,
      "grad_norm": 0.8600212335586548,
      "learning_rate": 4.89241951281889e-06,
      "loss": 1.4899,
      "step": 544
    },
    {
      "epoch": 2.2247191011235956,
      "grad_norm": 0.7336891889572144,
      "learning_rate": 4.87449292925076e-06,
      "loss": 1.474,
      "step": 545
    },
    {
      "epoch": 2.2288049029622066,
      "grad_norm": 0.7560427784919739,
      "learning_rate": 4.856567959889758e-06,
      "loss": 1.5449,
      "step": 546
    },
    {
      "epoch": 2.232890704800817,
      "grad_norm": 0.8228916525840759,
      "learning_rate": 4.838644835277585e-06,
      "loss": 1.4234,
      "step": 547
    },
    {
      "epoch": 2.236976506639428,
      "grad_norm": 0.7671748995780945,
      "learning_rate": 4.8207237859322144e-06,
      "loss": 1.4804,
      "step": 548
    },
    {
      "epoch": 2.241062308478039,
      "grad_norm": 0.7805154919624329,
      "learning_rate": 4.8028050423449265e-06,
      "loss": 1.374,
      "step": 549
    },
    {
      "epoch": 2.2451481103166495,
      "grad_norm": 0.8122634291648865,
      "learning_rate": 4.784888834977347e-06,
      "loss": 1.4361,
      "step": 550
    },
    {
      "epoch": 2.2492339121552605,
      "grad_norm": 0.8864636421203613,
      "learning_rate": 4.766975394258487e-06,
      "loss": 1.394,
      "step": 551
    },
    {
      "epoch": 2.2533197139938714,
      "grad_norm": 0.7382509708404541,
      "learning_rate": 4.749064950581765e-06,
      "loss": 1.4535,
      "step": 552
    },
    {
      "epoch": 2.257405515832482,
      "grad_norm": 0.8326717019081116,
      "learning_rate": 4.731157734302063e-06,
      "loss": 1.4368,
      "step": 553
    },
    {
      "epoch": 2.261491317671093,
      "grad_norm": 0.7986046075820923,
      "learning_rate": 4.7132539757327435e-06,
      "loss": 1.4845,
      "step": 554
    },
    {
      "epoch": 2.265577119509704,
      "grad_norm": 0.7387350797653198,
      "learning_rate": 4.695353905142708e-06,
      "loss": 1.4572,
      "step": 555
    },
    {
      "epoch": 2.2696629213483144,
      "grad_norm": 0.8069339394569397,
      "learning_rate": 4.6774577527534195e-06,
      "loss": 1.459,
      "step": 556
    },
    {
      "epoch": 2.2737487231869253,
      "grad_norm": 0.7561095356941223,
      "learning_rate": 4.659565748735947e-06,
      "loss": 1.3973,
      "step": 557
    },
    {
      "epoch": 2.2778345250255363,
      "grad_norm": 0.8133448362350464,
      "learning_rate": 4.641678123208005e-06,
      "loss": 1.4156,
      "step": 558
    },
    {
      "epoch": 2.2819203268641473,
      "grad_norm": 0.8072361946105957,
      "learning_rate": 4.623795106231001e-06,
      "loss": 1.3418,
      "step": 559
    },
    {
      "epoch": 2.2860061287027578,
      "grad_norm": 0.7252103686332703,
      "learning_rate": 4.6059169278070576e-06,
      "loss": 1.4169,
      "step": 560
    },
    {
      "epoch": 2.2900919305413687,
      "grad_norm": 0.7783358097076416,
      "learning_rate": 4.5880438178760785e-06,
      "loss": 1.3408,
      "step": 561
    },
    {
      "epoch": 2.2941777323799797,
      "grad_norm": 0.7332931160926819,
      "learning_rate": 4.570176006312769e-06,
      "loss": 1.4051,
      "step": 562
    },
    {
      "epoch": 2.2982635342185906,
      "grad_norm": 0.8016212582588196,
      "learning_rate": 4.552313722923695e-06,
      "loss": 1.5122,
      "step": 563
    },
    {
      "epoch": 2.302349336057201,
      "grad_norm": 0.8037737011909485,
      "learning_rate": 4.5344571974443255e-06,
      "loss": 1.5433,
      "step": 564
    },
    {
      "epoch": 2.306435137895812,
      "grad_norm": 0.8187606930732727,
      "learning_rate": 4.516606659536063e-06,
      "loss": 1.4456,
      "step": 565
    },
    {
      "epoch": 2.310520939734423,
      "grad_norm": 0.7766631841659546,
      "learning_rate": 4.498762338783314e-06,
      "loss": 1.5094,
      "step": 566
    },
    {
      "epoch": 2.3146067415730336,
      "grad_norm": 0.7670491933822632,
      "learning_rate": 4.4809244646905135e-06,
      "loss": 1.3609,
      "step": 567
    },
    {
      "epoch": 2.3186925434116445,
      "grad_norm": 0.7942546606063843,
      "learning_rate": 4.463093266679185e-06,
      "loss": 1.4596,
      "step": 568
    },
    {
      "epoch": 2.3227783452502555,
      "grad_norm": 0.7925419211387634,
      "learning_rate": 4.445268974084992e-06,
      "loss": 1.3741,
      "step": 569
    },
    {
      "epoch": 2.326864147088866,
      "grad_norm": 0.7272902727127075,
      "learning_rate": 4.42745181615478e-06,
      "loss": 1.453,
      "step": 570
    },
    {
      "epoch": 2.330949948927477,
      "grad_norm": 0.7514861226081848,
      "learning_rate": 4.40964202204363e-06,
      "loss": 1.4258,
      "step": 571
    },
    {
      "epoch": 2.335035750766088,
      "grad_norm": 0.7698717713356018,
      "learning_rate": 4.391839820811923e-06,
      "loss": 1.3916,
      "step": 572
    },
    {
      "epoch": 2.3391215526046985,
      "grad_norm": 0.8088445663452148,
      "learning_rate": 4.374045441422371e-06,
      "loss": 1.4104,
      "step": 573
    },
    {
      "epoch": 2.3432073544433094,
      "grad_norm": 0.7762229442596436,
      "learning_rate": 4.356259112737096e-06,
      "loss": 1.4253,
      "step": 574
    },
    {
      "epoch": 2.3472931562819204,
      "grad_norm": 0.7995090484619141,
      "learning_rate": 4.338481063514674e-06,
      "loss": 1.3401,
      "step": 575
    },
    {
      "epoch": 2.3513789581205313,
      "grad_norm": 0.7949746251106262,
      "learning_rate": 4.3207115224071874e-06,
      "loss": 1.5136,
      "step": 576
    },
    {
      "epoch": 2.355464759959142,
      "grad_norm": 0.8109560012817383,
      "learning_rate": 4.302950717957304e-06,
      "loss": 1.4616,
      "step": 577
    },
    {
      "epoch": 2.359550561797753,
      "grad_norm": 0.7586766481399536,
      "learning_rate": 4.285198878595312e-06,
      "loss": 1.4719,
      "step": 578
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.8148349523544312,
      "learning_rate": 4.267456232636207e-06,
      "loss": 1.4899,
      "step": 579
    },
    {
      "epoch": 2.3677221654749743,
      "grad_norm": 0.7689338326454163,
      "learning_rate": 4.249723008276737e-06,
      "loss": 1.3109,
      "step": 580
    },
    {
      "epoch": 2.3718079673135852,
      "grad_norm": 0.7162046432495117,
      "learning_rate": 4.231999433592476e-06,
      "loss": 1.407,
      "step": 581
    },
    {
      "epoch": 2.375893769152196,
      "grad_norm": 0.808781087398529,
      "learning_rate": 4.214285736534887e-06,
      "loss": 1.3438,
      "step": 582
    },
    {
      "epoch": 2.379979570990807,
      "grad_norm": 0.8586607575416565,
      "learning_rate": 4.196582144928398e-06,
      "loss": 1.4541,
      "step": 583
    },
    {
      "epoch": 2.3840653728294177,
      "grad_norm": 0.7923722863197327,
      "learning_rate": 4.178888886467457e-06,
      "loss": 1.4621,
      "step": 584
    },
    {
      "epoch": 2.3881511746680286,
      "grad_norm": 0.7641045451164246,
      "learning_rate": 4.161206188713621e-06,
      "loss": 1.4798,
      "step": 585
    },
    {
      "epoch": 2.3922369765066396,
      "grad_norm": 0.740394115447998,
      "learning_rate": 4.143534279092613e-06,
      "loss": 1.4178,
      "step": 586
    },
    {
      "epoch": 2.39632277834525,
      "grad_norm": 0.753838837146759,
      "learning_rate": 4.1258733848914105e-06,
      "loss": 1.3793,
      "step": 587
    },
    {
      "epoch": 2.400408580183861,
      "grad_norm": 0.7369697690010071,
      "learning_rate": 4.108223733255316e-06,
      "loss": 1.4095,
      "step": 588
    },
    {
      "epoch": 2.400408580183861,
      "eval_loss": 1.4318039417266846,
      "eval_runtime": 18.5883,
      "eval_samples_per_second": 11.136,
      "eval_steps_per_second": 2.797,
      "step": 588
    },
    {
      "epoch": 2.404494382022472,
      "grad_norm": 0.7984912991523743,
      "learning_rate": 4.090585551185031e-06,
      "loss": 1.4923,
      "step": 589
    },
    {
      "epoch": 2.4085801838610825,
      "grad_norm": 0.7529894709587097,
      "learning_rate": 4.072959065533745e-06,
      "loss": 1.4568,
      "step": 590
    },
    {
      "epoch": 2.4126659856996935,
      "grad_norm": 0.8603731989860535,
      "learning_rate": 4.0553445030042175e-06,
      "loss": 1.4411,
      "step": 591
    },
    {
      "epoch": 2.4167517875383044,
      "grad_norm": 0.7584493160247803,
      "learning_rate": 4.037742090145851e-06,
      "loss": 1.4756,
      "step": 592
    },
    {
      "epoch": 2.4208375893769154,
      "grad_norm": 0.7783717513084412,
      "learning_rate": 4.020152053351791e-06,
      "loss": 1.4426,
      "step": 593
    },
    {
      "epoch": 2.424923391215526,
      "grad_norm": 0.7934224009513855,
      "learning_rate": 4.002574618856007e-06,
      "loss": 1.5415,
      "step": 594
    },
    {
      "epoch": 2.429009193054137,
      "grad_norm": 0.9405944347381592,
      "learning_rate": 3.985010012730382e-06,
      "loss": 1.4483,
      "step": 595
    },
    {
      "epoch": 2.433094994892748,
      "grad_norm": 0.8583495020866394,
      "learning_rate": 3.967458460881815e-06,
      "loss": 1.3726,
      "step": 596
    },
    {
      "epoch": 2.4371807967313583,
      "grad_norm": 0.8845856189727783,
      "learning_rate": 3.949920189049294e-06,
      "loss": 1.4896,
      "step": 597
    },
    {
      "epoch": 2.4412665985699693,
      "grad_norm": 0.8183164000511169,
      "learning_rate": 3.93239542280102e-06,
      "loss": 1.4743,
      "step": 598
    },
    {
      "epoch": 2.4453524004085803,
      "grad_norm": 0.7854394316673279,
      "learning_rate": 3.914884387531485e-06,
      "loss": 1.5041,
      "step": 599
    },
    {
      "epoch": 2.449438202247191,
      "grad_norm": 0.7497026324272156,
      "learning_rate": 3.89738730845858e-06,
      "loss": 1.3797,
      "step": 600
    },
    {
      "epoch": 2.4535240040858017,
      "grad_norm": 0.805474579334259,
      "learning_rate": 3.879904410620703e-06,
      "loss": 1.4413,
      "step": 601
    },
    {
      "epoch": 2.4576098059244127,
      "grad_norm": 0.7120321989059448,
      "learning_rate": 3.862435918873854e-06,
      "loss": 1.355,
      "step": 602
    },
    {
      "epoch": 2.4616956077630237,
      "grad_norm": 0.7933729887008667,
      "learning_rate": 3.844982057888754e-06,
      "loss": 1.3751,
      "step": 603
    },
    {
      "epoch": 2.465781409601634,
      "grad_norm": 0.7811634540557861,
      "learning_rate": 3.827543052147952e-06,
      "loss": 1.4681,
      "step": 604
    },
    {
      "epoch": 2.469867211440245,
      "grad_norm": 0.7764067053794861,
      "learning_rate": 3.8101191259429315e-06,
      "loss": 1.4622,
      "step": 605
    },
    {
      "epoch": 2.473953013278856,
      "grad_norm": 0.8030002117156982,
      "learning_rate": 3.792710503371232e-06,
      "loss": 1.4621,
      "step": 606
    },
    {
      "epoch": 2.4780388151174666,
      "grad_norm": 0.7951022386550903,
      "learning_rate": 3.775317408333571e-06,
      "loss": 1.3684,
      "step": 607
    },
    {
      "epoch": 2.4821246169560776,
      "grad_norm": 0.7324254512786865,
      "learning_rate": 3.75794006453095e-06,
      "loss": 1.355,
      "step": 608
    },
    {
      "epoch": 2.4862104187946885,
      "grad_norm": 0.8471472859382629,
      "learning_rate": 3.740578695461795e-06,
      "loss": 1.4305,
      "step": 609
    },
    {
      "epoch": 2.4902962206332995,
      "grad_norm": 0.7524358630180359,
      "learning_rate": 3.7232335244190656e-06,
      "loss": 1.496,
      "step": 610
    },
    {
      "epoch": 2.49438202247191,
      "grad_norm": 0.770146369934082,
      "learning_rate": 3.705904774487396e-06,
      "loss": 1.4686,
      "step": 611
    },
    {
      "epoch": 2.498467824310521,
      "grad_norm": 0.8497075438499451,
      "learning_rate": 3.6885926685402213e-06,
      "loss": 1.4293,
      "step": 612
    },
    {
      "epoch": 2.502553626149132,
      "grad_norm": 0.7887583374977112,
      "learning_rate": 3.6712974292369035e-06,
      "loss": 1.4433,
      "step": 613
    },
    {
      "epoch": 2.506639427987743,
      "grad_norm": 0.7547279000282288,
      "learning_rate": 3.654019279019881e-06,
      "loss": 1.4442,
      "step": 614
    },
    {
      "epoch": 2.5107252298263534,
      "grad_norm": 0.7113548517227173,
      "learning_rate": 3.6367584401118004e-06,
      "loss": 1.4652,
      "step": 615
    },
    {
      "epoch": 2.5148110316649643,
      "grad_norm": 0.7680639028549194,
      "learning_rate": 3.6195151345126556e-06,
      "loss": 1.5307,
      "step": 616
    },
    {
      "epoch": 2.5188968335035753,
      "grad_norm": 0.7555155754089355,
      "learning_rate": 3.6022895839969406e-06,
      "loss": 1.433,
      "step": 617
    },
    {
      "epoch": 2.522982635342186,
      "grad_norm": 0.8380140066146851,
      "learning_rate": 3.5850820101107885e-06,
      "loss": 1.4609,
      "step": 618
    },
    {
      "epoch": 2.5270684371807968,
      "grad_norm": 0.7503949403762817,
      "learning_rate": 3.5678926341691283e-06,
      "loss": 1.5075,
      "step": 619
    },
    {
      "epoch": 2.5311542390194077,
      "grad_norm": 0.7451485991477966,
      "learning_rate": 3.5507216772528392e-06,
      "loss": 1.4189,
      "step": 620
    },
    {
      "epoch": 2.5352400408580182,
      "grad_norm": 0.7691320180892944,
      "learning_rate": 3.5335693602058974e-06,
      "loss": 1.4182,
      "step": 621
    },
    {
      "epoch": 2.539325842696629,
      "grad_norm": 0.9071232080459595,
      "learning_rate": 3.5164359036325483e-06,
      "loss": 1.4164,
      "step": 622
    },
    {
      "epoch": 2.54341164453524,
      "grad_norm": 0.7529043555259705,
      "learning_rate": 3.499321527894466e-06,
      "loss": 1.4573,
      "step": 623
    },
    {
      "epoch": 2.5474974463738507,
      "grad_norm": 0.7551432847976685,
      "learning_rate": 3.4822264531079074e-06,
      "loss": 1.3604,
      "step": 624
    },
    {
      "epoch": 2.5515832482124616,
      "grad_norm": 0.7682088017463684,
      "learning_rate": 3.4651508991409016e-06,
      "loss": 1.4202,
      "step": 625
    },
    {
      "epoch": 2.5556690500510726,
      "grad_norm": 0.7868478894233704,
      "learning_rate": 3.4480950856104002e-06,
      "loss": 1.5228,
      "step": 626
    },
    {
      "epoch": 2.559754851889683,
      "grad_norm": 0.8041873574256897,
      "learning_rate": 3.431059231879472e-06,
      "loss": 1.3795,
      "step": 627
    },
    {
      "epoch": 2.563840653728294,
      "grad_norm": 0.7600099444389343,
      "learning_rate": 3.4140435570544708e-06,
      "loss": 1.386,
      "step": 628
    },
    {
      "epoch": 2.567926455566905,
      "grad_norm": 0.8049479126930237,
      "learning_rate": 3.397048279982217e-06,
      "loss": 1.4632,
      "step": 629
    },
    {
      "epoch": 2.572012257405516,
      "grad_norm": 0.7780748605728149,
      "learning_rate": 3.380073619247187e-06,
      "loss": 1.484,
      "step": 630
    },
    {
      "epoch": 2.576098059244127,
      "grad_norm": 0.8410512804985046,
      "learning_rate": 3.363119793168704e-06,
      "loss": 1.521,
      "step": 631
    },
    {
      "epoch": 2.5801838610827375,
      "grad_norm": 0.8733108639717102,
      "learning_rate": 3.3461870197981205e-06,
      "loss": 1.4456,
      "step": 632
    },
    {
      "epoch": 2.5842696629213484,
      "grad_norm": 0.7465800642967224,
      "learning_rate": 3.329275516916026e-06,
      "loss": 1.3959,
      "step": 633
    },
    {
      "epoch": 2.5883554647599594,
      "grad_norm": 0.7471359372138977,
      "learning_rate": 3.3123855020294344e-06,
      "loss": 1.5088,
      "step": 634
    },
    {
      "epoch": 2.59244126659857,
      "grad_norm": 0.7907223701477051,
      "learning_rate": 3.295517192368996e-06,
      "loss": 1.3014,
      "step": 635
    },
    {
      "epoch": 2.596527068437181,
      "grad_norm": 0.7407903075218201,
      "learning_rate": 3.2786708048862e-06,
      "loss": 1.3689,
      "step": 636
    },
    {
      "epoch": 2.600612870275792,
      "grad_norm": 0.7914044857025146,
      "learning_rate": 3.26184655625058e-06,
      "loss": 1.5001,
      "step": 637
    },
    {
      "epoch": 2.6046986721144023,
      "grad_norm": 0.7902120351791382,
      "learning_rate": 3.2450446628469346e-06,
      "loss": 1.6027,
      "step": 638
    },
    {
      "epoch": 2.6087844739530133,
      "grad_norm": 0.7562587857246399,
      "learning_rate": 3.2282653407725416e-06,
      "loss": 1.4449,
      "step": 639
    },
    {
      "epoch": 2.6128702757916242,
      "grad_norm": 0.8046965003013611,
      "learning_rate": 3.2115088058343725e-06,
      "loss": 1.5908,
      "step": 640
    },
    {
      "epoch": 2.6169560776302347,
      "grad_norm": 0.7415843605995178,
      "learning_rate": 3.1947752735463306e-06,
      "loss": 1.3895,
      "step": 641
    },
    {
      "epoch": 2.6210418794688457,
      "grad_norm": 0.898190975189209,
      "learning_rate": 3.1780649591264635e-06,
      "loss": 1.5549,
      "step": 642
    },
    {
      "epoch": 2.6251276813074567,
      "grad_norm": 0.7643707394599915,
      "learning_rate": 3.161378077494205e-06,
      "loss": 1.4006,
      "step": 643
    },
    {
      "epoch": 2.629213483146067,
      "grad_norm": 0.7552963495254517,
      "learning_rate": 3.144714843267613e-06,
      "loss": 1.4708,
      "step": 644
    },
    {
      "epoch": 2.633299284984678,
      "grad_norm": 0.8003584146499634,
      "learning_rate": 3.1280754707605974e-06,
      "loss": 1.444,
      "step": 645
    },
    {
      "epoch": 2.637385086823289,
      "grad_norm": 0.7790706157684326,
      "learning_rate": 3.111460173980175e-06,
      "loss": 1.5319,
      "step": 646
    },
    {
      "epoch": 2.6414708886619,
      "grad_norm": 0.8682537078857422,
      "learning_rate": 3.0948691666237165e-06,
      "loss": 1.5607,
      "step": 647
    },
    {
      "epoch": 2.6455566905005106,
      "grad_norm": 0.8326361179351807,
      "learning_rate": 3.0783026620761846e-06,
      "loss": 1.4068,
      "step": 648
    },
    {
      "epoch": 2.6496424923391215,
      "grad_norm": 0.7674587368965149,
      "learning_rate": 3.06176087340741e-06,
      "loss": 1.3569,
      "step": 649
    },
    {
      "epoch": 2.6537282941777325,
      "grad_norm": 0.814817488193512,
      "learning_rate": 3.045244013369333e-06,
      "loss": 1.3813,
      "step": 650
    },
    {
      "epoch": 2.6578140960163434,
      "grad_norm": 0.8512741327285767,
      "learning_rate": 3.028752294393278e-06,
      "loss": 1.3793,
      "step": 651
    },
    {
      "epoch": 2.661899897854954,
      "grad_norm": 0.8461268544197083,
      "learning_rate": 3.0122859285872214e-06,
      "loss": 1.314,
      "step": 652
    },
    {
      "epoch": 2.665985699693565,
      "grad_norm": 0.7769490480422974,
      "learning_rate": 2.9958451277330525e-06,
      "loss": 1.4844,
      "step": 653
    },
    {
      "epoch": 2.670071501532176,
      "grad_norm": 0.8026915788650513,
      "learning_rate": 2.9794301032838634e-06,
      "loss": 1.48,
      "step": 654
    },
    {
      "epoch": 2.6741573033707864,
      "grad_norm": 0.8877707123756409,
      "learning_rate": 2.9630410663612226e-06,
      "loss": 1.459,
      "step": 655
    },
    {
      "epoch": 2.6782431052093973,
      "grad_norm": 0.7798885703086853,
      "learning_rate": 2.9466782277524554e-06,
      "loss": 1.508,
      "step": 656
    },
    {
      "epoch": 2.6823289070480083,
      "grad_norm": 0.7689723968505859,
      "learning_rate": 2.930341797907947e-06,
      "loss": 1.4265,
      "step": 657
    },
    {
      "epoch": 2.686414708886619,
      "grad_norm": 0.8250020146369934,
      "learning_rate": 2.914031986938417e-06,
      "loss": 1.5063,
      "step": 658
    },
    {
      "epoch": 2.6905005107252298,
      "grad_norm": 0.7480664253234863,
      "learning_rate": 2.8977490046122326e-06,
      "loss": 1.5186,
      "step": 659
    },
    {
      "epoch": 2.6945863125638407,
      "grad_norm": 0.800717294216156,
      "learning_rate": 2.8814930603527067e-06,
      "loss": 1.4256,
      "step": 660
    },
    {
      "epoch": 2.6986721144024512,
      "grad_norm": 0.7885717749595642,
      "learning_rate": 2.865264363235396e-06,
      "loss": 1.2855,
      "step": 661
    },
    {
      "epoch": 2.702757916241062,
      "grad_norm": 0.8169288039207458,
      "learning_rate": 2.8490631219854224e-06,
      "loss": 1.4895,
      "step": 662
    },
    {
      "epoch": 2.706843718079673,
      "grad_norm": 0.8069636821746826,
      "learning_rate": 2.832889544974787e-06,
      "loss": 1.4541,
      "step": 663
    },
    {
      "epoch": 2.710929519918284,
      "grad_norm": 0.7582389116287231,
      "learning_rate": 2.816743840219681e-06,
      "loss": 1.4201,
      "step": 664
    },
    {
      "epoch": 2.7150153217568946,
      "grad_norm": 0.868631899356842,
      "learning_rate": 2.800626215377824e-06,
      "loss": 1.4567,
      "step": 665
    },
    {
      "epoch": 2.7191011235955056,
      "grad_norm": 0.7675390243530273,
      "learning_rate": 2.7845368777457803e-06,
      "loss": 1.372,
      "step": 666
    },
    {
      "epoch": 2.7231869254341166,
      "grad_norm": 0.8514899015426636,
      "learning_rate": 2.7684760342563045e-06,
      "loss": 1.4272,
      "step": 667
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.7723225951194763,
      "learning_rate": 2.7524438914756714e-06,
      "loss": 1.3688,
      "step": 668
    },
    {
      "epoch": 2.731358529111338,
      "grad_norm": 0.7298558354377747,
      "learning_rate": 2.7364406556010194e-06,
      "loss": 1.4872,
      "step": 669
    },
    {
      "epoch": 2.735444330949949,
      "grad_norm": 0.765245795249939,
      "learning_rate": 2.720466532457707e-06,
      "loss": 1.4999,
      "step": 670
    },
    {
      "epoch": 2.73953013278856,
      "grad_norm": 0.7865787148475647,
      "learning_rate": 2.7045217274966534e-06,
      "loss": 1.3125,
      "step": 671
    },
    {
      "epoch": 2.7436159346271705,
      "grad_norm": 0.8089457154273987,
      "learning_rate": 2.6886064457917094e-06,
      "loss": 1.3361,
      "step": 672
    },
    {
      "epoch": 2.7477017364657814,
      "grad_norm": 0.8256738185882568,
      "learning_rate": 2.6727208920370063e-06,
      "loss": 1.4951,
      "step": 673
    },
    {
      "epoch": 2.7517875383043924,
      "grad_norm": 0.7842279672622681,
      "learning_rate": 2.6568652705443355e-06,
      "loss": 1.3071,
      "step": 674
    },
    {
      "epoch": 2.755873340143003,
      "grad_norm": 0.8309034705162048,
      "learning_rate": 2.641039785240509e-06,
      "loss": 1.4238,
      "step": 675
    },
    {
      "epoch": 2.759959141981614,
      "grad_norm": 0.8059126734733582,
      "learning_rate": 2.6252446396647503e-06,
      "loss": 1.3129,
      "step": 676
    },
    {
      "epoch": 2.764044943820225,
      "grad_norm": 0.920028567314148,
      "learning_rate": 2.609480036966061e-06,
      "loss": 1.4686,
      "step": 677
    },
    {
      "epoch": 2.7681307456588353,
      "grad_norm": 0.7633463144302368,
      "learning_rate": 2.5937461799006215e-06,
      "loss": 1.4469,
      "step": 678
    },
    {
      "epoch": 2.7722165474974463,
      "grad_norm": 0.7746725678443909,
      "learning_rate": 2.578043270829178e-06,
      "loss": 1.548,
      "step": 679
    },
    {
      "epoch": 2.7763023493360572,
      "grad_norm": 0.7843210697174072,
      "learning_rate": 2.5623715117144337e-06,
      "loss": 1.4419,
      "step": 680
    },
    {
      "epoch": 2.7803881511746678,
      "grad_norm": 0.8316723704338074,
      "learning_rate": 2.5467311041184655e-06,
      "loss": 1.5369,
      "step": 681
    },
    {
      "epoch": 2.7844739530132787,
      "grad_norm": 0.8241320848464966,
      "learning_rate": 2.531122249200114e-06,
      "loss": 1.4291,
      "step": 682
    },
    {
      "epoch": 2.7885597548518897,
      "grad_norm": 0.7971199750900269,
      "learning_rate": 2.515545147712414e-06,
      "loss": 1.4765,
      "step": 683
    },
    {
      "epoch": 2.7926455566905006,
      "grad_norm": 0.7953081130981445,
      "learning_rate": 2.5000000000000015e-06,
      "loss": 1.3955,
      "step": 684
    },
    {
      "epoch": 2.7967313585291116,
      "grad_norm": 0.7790589332580566,
      "learning_rate": 2.4844870059965337e-06,
      "loss": 1.4467,
      "step": 685
    },
    {
      "epoch": 2.800817160367722,
      "grad_norm": 0.7747247815132141,
      "learning_rate": 2.469006365222132e-06,
      "loss": 1.2755,
      "step": 686
    },
    {
      "epoch": 2.804902962206333,
      "grad_norm": 0.8017767071723938,
      "learning_rate": 2.453558276780804e-06,
      "loss": 1.5155,
      "step": 687
    },
    {
      "epoch": 2.808988764044944,
      "grad_norm": 0.694990873336792,
      "learning_rate": 2.438142939357882e-06,
      "loss": 1.3682,
      "step": 688
    },
    {
      "epoch": 2.8130745658835545,
      "grad_norm": 0.8371564149856567,
      "learning_rate": 2.4227605512174796e-06,
      "loss": 1.4408,
      "step": 689
    },
    {
      "epoch": 2.8171603677221655,
      "grad_norm": 0.755506157875061,
      "learning_rate": 2.4074113101999245e-06,
      "loss": 1.4543,
      "step": 690
    },
    {
      "epoch": 2.8212461695607765,
      "grad_norm": 0.78868567943573,
      "learning_rate": 2.392095413719231e-06,
      "loss": 1.3909,
      "step": 691
    },
    {
      "epoch": 2.825331971399387,
      "grad_norm": 0.7788638472557068,
      "learning_rate": 2.3768130587605513e-06,
      "loss": 1.5066,
      "step": 692
    },
    {
      "epoch": 2.829417773237998,
      "grad_norm": 0.8612346053123474,
      "learning_rate": 2.3615644418776397e-06,
      "loss": 1.3825,
      "step": 693
    },
    {
      "epoch": 2.833503575076609,
      "grad_norm": 0.8130613565444946,
      "learning_rate": 2.346349759190332e-06,
      "loss": 1.413,
      "step": 694
    },
    {
      "epoch": 2.8375893769152194,
      "grad_norm": 0.7889130115509033,
      "learning_rate": 2.331169206382024e-06,
      "loss": 1.4422,
      "step": 695
    },
    {
      "epoch": 2.8416751787538304,
      "grad_norm": 0.8065751791000366,
      "learning_rate": 2.316022978697143e-06,
      "loss": 1.3794,
      "step": 696
    },
    {
      "epoch": 2.8457609805924413,
      "grad_norm": 0.788135826587677,
      "learning_rate": 2.3009112709386454e-06,
      "loss": 1.4384,
      "step": 697
    },
    {
      "epoch": 2.849846782431052,
      "grad_norm": 0.7287472486495972,
      "learning_rate": 2.2858342774655174e-06,
      "loss": 1.469,
      "step": 698
    },
    {
      "epoch": 2.853932584269663,
      "grad_norm": 0.8103553056716919,
      "learning_rate": 2.270792192190259e-06,
      "loss": 1.3991,
      "step": 699
    },
    {
      "epoch": 2.8580183861082737,
      "grad_norm": 0.7869590520858765,
      "learning_rate": 2.2557852085764053e-06,
      "loss": 1.3836,
      "step": 700
    },
    {
      "epoch": 2.8621041879468847,
      "grad_norm": 0.7596094012260437,
      "learning_rate": 2.240813519636028e-06,
      "loss": 1.4042,
      "step": 701
    },
    {
      "epoch": 2.8661899897854957,
      "grad_norm": 0.7380961775779724,
      "learning_rate": 2.2258773179272588e-06,
      "loss": 1.4271,
      "step": 702
    },
    {
      "epoch": 2.870275791624106,
      "grad_norm": 0.8198264241218567,
      "learning_rate": 2.2109767955518135e-06,
      "loss": 1.495,
      "step": 703
    },
    {
      "epoch": 2.874361593462717,
      "grad_norm": 0.751962423324585,
      "learning_rate": 2.1961121441525113e-06,
      "loss": 1.485,
      "step": 704
    },
    {
      "epoch": 2.878447395301328,
      "grad_norm": 0.7590723037719727,
      "learning_rate": 2.181283554910827e-06,
      "loss": 1.451,
      "step": 705
    },
    {
      "epoch": 2.8825331971399386,
      "grad_norm": 0.7284148335456848,
      "learning_rate": 2.1664912185444127e-06,
      "loss": 1.4122,
      "step": 706
    },
    {
      "epoch": 2.8866189989785496,
      "grad_norm": 0.7810165882110596,
      "learning_rate": 2.1517353253046623e-06,
      "loss": 1.4001,
      "step": 707
    },
    {
      "epoch": 2.8907048008171605,
      "grad_norm": 0.808103621006012,
      "learning_rate": 2.137016064974256e-06,
      "loss": 1.5096,
      "step": 708
    },
    {
      "epoch": 2.894790602655771,
      "grad_norm": 0.8443530201911926,
      "learning_rate": 2.1223336268647154e-06,
      "loss": 1.4116,
      "step": 709
    },
    {
      "epoch": 2.898876404494382,
      "grad_norm": 0.7895494103431702,
      "learning_rate": 2.1076881998139794e-06,
      "loss": 1.3556,
      "step": 710
    },
    {
      "epoch": 2.902962206332993,
      "grad_norm": 0.771905779838562,
      "learning_rate": 2.093079972183968e-06,
      "loss": 1.5016,
      "step": 711
    },
    {
      "epoch": 2.9070480081716035,
      "grad_norm": 0.8408382534980774,
      "learning_rate": 2.0785091318581577e-06,
      "loss": 1.3034,
      "step": 712
    },
    {
      "epoch": 2.9111338100102144,
      "grad_norm": 0.7647413611412048,
      "learning_rate": 2.0639758662391767e-06,
      "loss": 1.3715,
      "step": 713
    },
    {
      "epoch": 2.9152196118488254,
      "grad_norm": 0.7355188727378845,
      "learning_rate": 2.049480362246378e-06,
      "loss": 1.4567,
      "step": 714
    },
    {
      "epoch": 2.919305413687436,
      "grad_norm": 0.8160449862480164,
      "learning_rate": 2.035022806313449e-06,
      "loss": 1.4104,
      "step": 715
    },
    {
      "epoch": 2.923391215526047,
      "grad_norm": 0.7299528121948242,
      "learning_rate": 2.0206033843860113e-06,
      "loss": 1.2651,
      "step": 716
    },
    {
      "epoch": 2.927477017364658,
      "grad_norm": 0.7307971715927124,
      "learning_rate": 2.0062222819192186e-06,
      "loss": 1.4782,
      "step": 717
    },
    {
      "epoch": 2.9315628192032688,
      "grad_norm": 0.7657196521759033,
      "learning_rate": 1.991879683875386e-06,
      "loss": 1.4068,
      "step": 718
    },
    {
      "epoch": 2.9356486210418797,
      "grad_norm": 0.68636155128479,
      "learning_rate": 1.9775757747216055e-06,
      "loss": 1.2778,
      "step": 719
    },
    {
      "epoch": 2.9397344228804902,
      "grad_norm": 0.8294392228126526,
      "learning_rate": 1.9633107384273668e-06,
      "loss": 1.4728,
      "step": 720
    },
    {
      "epoch": 2.943820224719101,
      "grad_norm": 0.8051753640174866,
      "learning_rate": 1.9490847584621993e-06,
      "loss": 1.4386,
      "step": 721
    },
    {
      "epoch": 2.947906026557712,
      "grad_norm": 0.8132407665252686,
      "learning_rate": 1.9348980177933126e-06,
      "loss": 1.5106,
      "step": 722
    },
    {
      "epoch": 2.9519918283963227,
      "grad_norm": 0.8275883793830872,
      "learning_rate": 1.920750698883237e-06,
      "loss": 1.4626,
      "step": 723
    },
    {
      "epoch": 2.9560776302349336,
      "grad_norm": 0.7060334086418152,
      "learning_rate": 1.9066429836874844e-06,
      "loss": 1.3998,
      "step": 724
    },
    {
      "epoch": 2.9601634320735446,
      "grad_norm": 0.8164902925491333,
      "learning_rate": 1.8925750536522003e-06,
      "loss": 1.4537,
      "step": 725
    },
    {
      "epoch": 2.964249233912155,
      "grad_norm": 0.7806494235992432,
      "learning_rate": 1.8785470897118362e-06,
      "loss": 1.4549,
      "step": 726
    },
    {
      "epoch": 2.968335035750766,
      "grad_norm": 0.7902433276176453,
      "learning_rate": 1.8645592722868223e-06,
      "loss": 1.4543,
      "step": 727
    },
    {
      "epoch": 2.972420837589377,
      "grad_norm": 0.8309162259101868,
      "learning_rate": 1.850611781281239e-06,
      "loss": 1.3957,
      "step": 728
    },
    {
      "epoch": 2.9765066394279875,
      "grad_norm": 0.8646507859230042,
      "learning_rate": 1.8367047960805168e-06,
      "loss": 1.4886,
      "step": 729
    },
    {
      "epoch": 2.9805924412665985,
      "grad_norm": 0.8007318377494812,
      "learning_rate": 1.8228384955491136e-06,
      "loss": 1.4332,
      "step": 730
    },
    {
      "epoch": 2.9846782431052095,
      "grad_norm": 0.7940223813056946,
      "learning_rate": 1.809013058028228e-06,
      "loss": 1.485,
      "step": 731
    },
    {
      "epoch": 2.98876404494382,
      "grad_norm": 0.7968471050262451,
      "learning_rate": 1.7952286613334986e-06,
      "loss": 1.3904,
      "step": 732
    },
    {
      "epoch": 2.992849846782431,
      "grad_norm": 0.7268911600112915,
      "learning_rate": 1.7814854827527144e-06,
      "loss": 1.3421,
      "step": 733
    },
    {
      "epoch": 2.996935648621042,
      "grad_norm": 0.773209273815155,
      "learning_rate": 1.7677836990435427e-06,
      "loss": 1.3977,
      "step": 734
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.9911782741546631,
      "learning_rate": 1.7541234864312517e-06,
      "loss": 1.4478,
      "step": 735
    },
    {
      "epoch": 3.004085801838611,
      "grad_norm": 0.773913562297821,
      "learning_rate": 1.7405050206064372e-06,
      "loss": 1.3231,
      "step": 736
    },
    {
      "epoch": 3.0081716036772215,
      "grad_norm": 0.7619055509567261,
      "learning_rate": 1.7269284767227784e-06,
      "loss": 1.3387,
      "step": 737
    },
    {
      "epoch": 3.0122574055158324,
      "grad_norm": 0.764866292476654,
      "learning_rate": 1.7133940293947671e-06,
      "loss": 1.43,
      "step": 738
    },
    {
      "epoch": 3.0163432073544434,
      "grad_norm": 0.8697372674942017,
      "learning_rate": 1.6999018526954775e-06,
      "loss": 1.3818,
      "step": 739
    },
    {
      "epoch": 3.0204290091930543,
      "grad_norm": 0.8606742024421692,
      "learning_rate": 1.68645212015432e-06,
      "loss": 1.401,
      "step": 740
    },
    {
      "epoch": 3.024514811031665,
      "grad_norm": 0.8044888973236084,
      "learning_rate": 1.6730450047548064e-06,
      "loss": 1.4502,
      "step": 741
    },
    {
      "epoch": 3.028600612870276,
      "grad_norm": 0.7379196882247925,
      "learning_rate": 1.6596806789323317e-06,
      "loss": 1.4188,
      "step": 742
    },
    {
      "epoch": 3.032686414708887,
      "grad_norm": 0.8113938570022583,
      "learning_rate": 1.6463593145719559e-06,
      "loss": 1.5142,
      "step": 743
    },
    {
      "epoch": 3.0367722165474973,
      "grad_norm": 0.7383604645729065,
      "learning_rate": 1.6330810830061833e-06,
      "loss": 1.3536,
      "step": 744
    },
    {
      "epoch": 3.0408580183861083,
      "grad_norm": 0.8765743374824524,
      "learning_rate": 1.6198461550127758e-06,
      "loss": 1.4248,
      "step": 745
    },
    {
      "epoch": 3.044943820224719,
      "grad_norm": 0.817627489566803,
      "learning_rate": 1.6066547008125399e-06,
      "loss": 1.4378,
      "step": 746
    },
    {
      "epoch": 3.0490296220633297,
      "grad_norm": 0.8222845196723938,
      "learning_rate": 1.5935068900671459e-06,
      "loss": 1.3859,
      "step": 747
    },
    {
      "epoch": 3.0531154239019407,
      "grad_norm": 0.7402341961860657,
      "learning_rate": 1.5804028918769488e-06,
      "loss": 1.4508,
      "step": 748
    },
    {
      "epoch": 3.0572012257405516,
      "grad_norm": 0.7387097477912903,
      "learning_rate": 1.5673428747788033e-06,
      "loss": 1.3738,
      "step": 749
    },
    {
      "epoch": 3.0612870275791626,
      "grad_norm": 0.7834795117378235,
      "learning_rate": 1.5543270067439059e-06,
      "loss": 1.3489,
      "step": 750
    },
    {
      "epoch": 3.065372829417773,
      "grad_norm": 0.7651243805885315,
      "learning_rate": 1.5413554551756321e-06,
      "loss": 1.372,
      "step": 751
    },
    {
      "epoch": 3.069458631256384,
      "grad_norm": 0.9438737034797668,
      "learning_rate": 1.5284283869073753e-06,
      "loss": 1.4992,
      "step": 752
    },
    {
      "epoch": 3.073544433094995,
      "grad_norm": 0.8586205244064331,
      "learning_rate": 1.5155459682004154e-06,
      "loss": 1.4097,
      "step": 753
    },
    {
      "epoch": 3.0776302349336055,
      "grad_norm": 0.7411280870437622,
      "learning_rate": 1.5027083647417657e-06,
      "loss": 1.3781,
      "step": 754
    },
    {
      "epoch": 3.0817160367722165,
      "grad_norm": 0.8016560077667236,
      "learning_rate": 1.4899157416420535e-06,
      "loss": 1.3512,
      "step": 755
    },
    {
      "epoch": 3.0858018386108275,
      "grad_norm": 0.8790906667709351,
      "learning_rate": 1.4771682634333933e-06,
      "loss": 1.4398,
      "step": 756
    },
    {
      "epoch": 3.0898876404494384,
      "grad_norm": 0.7821241617202759,
      "learning_rate": 1.4644660940672628e-06,
      "loss": 1.495,
      "step": 757
    },
    {
      "epoch": 3.093973442288049,
      "grad_norm": 0.7885897159576416,
      "learning_rate": 1.4518093969124063e-06,
      "loss": 1.3931,
      "step": 758
    },
    {
      "epoch": 3.09805924412666,
      "grad_norm": 0.7429192662239075,
      "learning_rate": 1.4391983347527288e-06,
      "loss": 1.3155,
      "step": 759
    },
    {
      "epoch": 3.102145045965271,
      "grad_norm": 0.7864023447036743,
      "learning_rate": 1.4266330697851955e-06,
      "loss": 1.2969,
      "step": 760
    },
    {
      "epoch": 3.1062308478038814,
      "grad_norm": 0.743766725063324,
      "learning_rate": 1.4141137636177566e-06,
      "loss": 1.3273,
      "step": 761
    },
    {
      "epoch": 3.1103166496424923,
      "grad_norm": 0.7933885455131531,
      "learning_rate": 1.4016405772672647e-06,
      "loss": 1.4279,
      "step": 762
    },
    {
      "epoch": 3.1144024514811033,
      "grad_norm": 0.7384201288223267,
      "learning_rate": 1.3892136711573983e-06,
      "loss": 1.5002,
      "step": 763
    },
    {
      "epoch": 3.118488253319714,
      "grad_norm": 0.7391879558563232,
      "learning_rate": 1.3768332051166089e-06,
      "loss": 1.3402,
      "step": 764
    },
    {
      "epoch": 3.1225740551583248,
      "grad_norm": 0.798334002494812,
      "learning_rate": 1.3644993383760553e-06,
      "loss": 1.2718,
      "step": 765
    },
    {
      "epoch": 3.1266598569969357,
      "grad_norm": 0.7845279574394226,
      "learning_rate": 1.3522122295675616e-06,
      "loss": 1.3693,
      "step": 766
    },
    {
      "epoch": 3.1307456588355467,
      "grad_norm": 0.7906732559204102,
      "learning_rate": 1.339972036721579e-06,
      "loss": 1.4281,
      "step": 767
    },
    {
      "epoch": 3.134831460674157,
      "grad_norm": 0.7584381103515625,
      "learning_rate": 1.327778917265144e-06,
      "loss": 1.491,
      "step": 768
    },
    {
      "epoch": 3.138917262512768,
      "grad_norm": 0.8075645565986633,
      "learning_rate": 1.3156330280198637e-06,
      "loss": 1.4694,
      "step": 769
    },
    {
      "epoch": 3.143003064351379,
      "grad_norm": 0.6884862184524536,
      "learning_rate": 1.3035345251998933e-06,
      "loss": 1.3598,
      "step": 770
    },
    {
      "epoch": 3.1470888661899896,
      "grad_norm": 0.7982008457183838,
      "learning_rate": 1.2914835644099255e-06,
      "loss": 1.3788,
      "step": 771
    },
    {
      "epoch": 3.1511746680286006,
      "grad_norm": 0.8352728486061096,
      "learning_rate": 1.2794803006431984e-06,
      "loss": 1.5616,
      "step": 772
    },
    {
      "epoch": 3.1552604698672115,
      "grad_norm": 0.784273087978363,
      "learning_rate": 1.2675248882794883e-06,
      "loss": 1.499,
      "step": 773
    },
    {
      "epoch": 3.1593462717058225,
      "grad_norm": 0.7440959215164185,
      "learning_rate": 1.2556174810831368e-06,
      "loss": 1.3321,
      "step": 774
    },
    {
      "epoch": 3.163432073544433,
      "grad_norm": 0.6973153948783875,
      "learning_rate": 1.2437582322010672e-06,
      "loss": 1.4335,
      "step": 775
    },
    {
      "epoch": 3.167517875383044,
      "grad_norm": 0.768988847732544,
      "learning_rate": 1.2319472941608118e-06,
      "loss": 1.4812,
      "step": 776
    },
    {
      "epoch": 3.171603677221655,
      "grad_norm": 0.8381926417350769,
      "learning_rate": 1.2201848188685573e-06,
      "loss": 1.4557,
      "step": 777
    },
    {
      "epoch": 3.1756894790602654,
      "grad_norm": 0.8008327484130859,
      "learning_rate": 1.2084709576071885e-06,
      "loss": 1.4198,
      "step": 778
    },
    {
      "epoch": 3.1797752808988764,
      "grad_norm": 0.7654620409011841,
      "learning_rate": 1.1968058610343359e-06,
      "loss": 1.4685,
      "step": 779
    },
    {
      "epoch": 3.1838610827374874,
      "grad_norm": 0.8225075006484985,
      "learning_rate": 1.1851896791804507e-06,
      "loss": 1.4815,
      "step": 780
    },
    {
      "epoch": 3.187946884576098,
      "grad_norm": 0.7508679628372192,
      "learning_rate": 1.1736225614468627e-06,
      "loss": 1.5657,
      "step": 781
    },
    {
      "epoch": 3.192032686414709,
      "grad_norm": 0.9311937093734741,
      "learning_rate": 1.162104656603868e-06,
      "loss": 1.4002,
      "step": 782
    },
    {
      "epoch": 3.19611848825332,
      "grad_norm": 0.7042275071144104,
      "learning_rate": 1.1506361127888117e-06,
      "loss": 1.3731,
      "step": 783
    },
    {
      "epoch": 3.2002042900919307,
      "grad_norm": 0.7600952386856079,
      "learning_rate": 1.1392170775041788e-06,
      "loss": 1.5087,
      "step": 784
    },
    {
      "epoch": 3.2002042900919307,
      "eval_loss": 1.4196075201034546,
      "eval_runtime": 18.5514,
      "eval_samples_per_second": 11.158,
      "eval_steps_per_second": 2.803,
      "step": 784
    },
    {
      "epoch": 3.2042900919305413,
      "grad_norm": 0.7616256475448608,
      "learning_rate": 1.127847697615706e-06,
      "loss": 1.3632,
      "step": 785
    },
    {
      "epoch": 3.208375893769152,
      "grad_norm": 0.7814708352088928,
      "learning_rate": 1.1165281193504873e-06,
      "loss": 1.2949,
      "step": 786
    },
    {
      "epoch": 3.212461695607763,
      "grad_norm": 0.8632522821426392,
      "learning_rate": 1.1052584882950896e-06,
      "loss": 1.5254,
      "step": 787
    },
    {
      "epoch": 3.2165474974463737,
      "grad_norm": 0.7806894779205322,
      "learning_rate": 1.0940389493936903e-06,
      "loss": 1.5217,
      "step": 788
    },
    {
      "epoch": 3.2206332992849847,
      "grad_norm": 0.7602083086967468,
      "learning_rate": 1.0828696469462023e-06,
      "loss": 1.3844,
      "step": 789
    },
    {
      "epoch": 3.2247191011235956,
      "grad_norm": 0.7640348672866821,
      "learning_rate": 1.0717507246064273e-06,
      "loss": 1.4025,
      "step": 790
    },
    {
      "epoch": 3.2288049029622066,
      "grad_norm": 0.7987479567527771,
      "learning_rate": 1.060682325380204e-06,
      "loss": 1.4412,
      "step": 791
    },
    {
      "epoch": 3.232890704800817,
      "grad_norm": 0.7327021956443787,
      "learning_rate": 1.049664591623563e-06,
      "loss": 1.4307,
      "step": 792
    },
    {
      "epoch": 3.236976506639428,
      "grad_norm": 0.765346348285675,
      "learning_rate": 1.0386976650409102e-06,
      "loss": 1.4201,
      "step": 793
    },
    {
      "epoch": 3.241062308478039,
      "grad_norm": 0.8034964203834534,
      "learning_rate": 1.027781686683189e-06,
      "loss": 1.4067,
      "step": 794
    },
    {
      "epoch": 3.2451481103166495,
      "grad_norm": 0.8075528144836426,
      "learning_rate": 1.016916796946078e-06,
      "loss": 1.4508,
      "step": 795
    },
    {
      "epoch": 3.2492339121552605,
      "grad_norm": 0.802391767501831,
      "learning_rate": 1.0061031355681766e-06,
      "loss": 1.5091,
      "step": 796
    },
    {
      "epoch": 3.2533197139938714,
      "grad_norm": 0.7127800583839417,
      "learning_rate": 9.953408416292125e-07,
      "loss": 1.4027,
      "step": 797
    },
    {
      "epoch": 3.257405515832482,
      "grad_norm": 0.8315708041191101,
      "learning_rate": 9.846300535482534e-07,
      "loss": 1.4837,
      "step": 798
    },
    {
      "epoch": 3.261491317671093,
      "grad_norm": 0.7631393074989319,
      "learning_rate": 9.739709090819254e-07,
      "loss": 1.4475,
      "step": 799
    },
    {
      "epoch": 3.265577119509704,
      "grad_norm": 0.7386719584465027,
      "learning_rate": 9.633635453226376e-07,
      "loss": 1.4353,
      "step": 800
    },
    {
      "epoch": 3.2696629213483144,
      "grad_norm": 0.7616199254989624,
      "learning_rate": 9.528080986968258e-07,
      "loss": 1.4499,
      "step": 801
    },
    {
      "epoch": 3.2737487231869253,
      "grad_norm": 0.7866300344467163,
      "learning_rate": 9.423047049631956e-07,
      "loss": 1.3686,
      "step": 802
    },
    {
      "epoch": 3.2778345250255363,
      "grad_norm": 0.814328134059906,
      "learning_rate": 9.318534992109696e-07,
      "loss": 1.4389,
      "step": 803
    },
    {
      "epoch": 3.2819203268641473,
      "grad_norm": 0.741157591342926,
      "learning_rate": 9.214546158581622e-07,
      "loss": 1.5572,
      "step": 804
    },
    {
      "epoch": 3.2860061287027578,
      "grad_norm": 0.7445455193519592,
      "learning_rate": 9.111081886498374e-07,
      "loss": 1.392,
      "step": 805
    },
    {
      "epoch": 3.2900919305413687,
      "grad_norm": 0.7201120257377625,
      "learning_rate": 9.00814350656401e-07,
      "loss": 1.3273,
      "step": 806
    },
    {
      "epoch": 3.2941777323799797,
      "grad_norm": 0.8488322496414185,
      "learning_rate": 8.905732342718825e-07,
      "loss": 1.4223,
      "step": 807
    },
    {
      "epoch": 3.2982635342185906,
      "grad_norm": 0.841353714466095,
      "learning_rate": 8.803849712122292e-07,
      "loss": 1.5131,
      "step": 808
    },
    {
      "epoch": 3.302349336057201,
      "grad_norm": 0.766183078289032,
      "learning_rate": 8.7024969251362e-07,
      "loss": 1.407,
      "step": 809
    },
    {
      "epoch": 3.306435137895812,
      "grad_norm": 0.8302446007728577,
      "learning_rate": 8.601675285307781e-07,
      "loss": 1.3529,
      "step": 810
    },
    {
      "epoch": 3.310520939734423,
      "grad_norm": 0.8029419183731079,
      "learning_rate": 8.501386089352858e-07,
      "loss": 1.3908,
      "step": 811
    },
    {
      "epoch": 3.3146067415730336,
      "grad_norm": 0.7437186241149902,
      "learning_rate": 8.401630627139317e-07,
      "loss": 1.41,
      "step": 812
    },
    {
      "epoch": 3.3186925434116445,
      "grad_norm": 0.7217497825622559,
      "learning_rate": 8.302410181670367e-07,
      "loss": 1.4179,
      "step": 813
    },
    {
      "epoch": 3.3227783452502555,
      "grad_norm": 0.8076943755149841,
      "learning_rate": 8.203726029068149e-07,
      "loss": 1.4452,
      "step": 814
    },
    {
      "epoch": 3.326864147088866,
      "grad_norm": 0.7553761601448059,
      "learning_rate": 8.105579438557287e-07,
      "loss": 1.3404,
      "step": 815
    },
    {
      "epoch": 3.330949948927477,
      "grad_norm": 0.7468945980072021,
      "learning_rate": 8.007971672448511e-07,
      "loss": 1.3213,
      "step": 816
    },
    {
      "epoch": 3.335035750766088,
      "grad_norm": 0.7703331112861633,
      "learning_rate": 7.910903986122537e-07,
      "loss": 1.2639,
      "step": 817
    },
    {
      "epoch": 3.3391215526046985,
      "grad_norm": 0.7270731329917908,
      "learning_rate": 7.814377628013786e-07,
      "loss": 1.473,
      "step": 818
    },
    {
      "epoch": 3.3432073544433094,
      "grad_norm": 0.7600666284561157,
      "learning_rate": 7.718393839594456e-07,
      "loss": 1.4928,
      "step": 819
    },
    {
      "epoch": 3.3472931562819204,
      "grad_norm": 0.7886441946029663,
      "learning_rate": 7.622953855358456e-07,
      "loss": 1.3742,
      "step": 820
    },
    {
      "epoch": 3.3513789581205313,
      "grad_norm": 0.7272607684135437,
      "learning_rate": 7.528058902805557e-07,
      "loss": 1.3881,
      "step": 821
    },
    {
      "epoch": 3.355464759959142,
      "grad_norm": 0.7705265879631042,
      "learning_rate": 7.433710202425653e-07,
      "loss": 1.595,
      "step": 822
    },
    {
      "epoch": 3.359550561797753,
      "grad_norm": 0.7915868759155273,
      "learning_rate": 7.339908967683007e-07,
      "loss": 1.4257,
      "step": 823
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 0.7385451793670654,
      "learning_rate": 7.246656405000646e-07,
      "loss": 1.3912,
      "step": 824
    },
    {
      "epoch": 3.3677221654749743,
      "grad_norm": 0.8437273502349854,
      "learning_rate": 7.153953713744877e-07,
      "loss": 1.4153,
      "step": 825
    },
    {
      "epoch": 3.3718079673135852,
      "grad_norm": 0.7858589887619019,
      "learning_rate": 7.061802086209857e-07,
      "loss": 1.3926,
      "step": 826
    },
    {
      "epoch": 3.375893769152196,
      "grad_norm": 0.8221063613891602,
      "learning_rate": 6.970202707602209e-07,
      "loss": 1.3903,
      "step": 827
    },
    {
      "epoch": 3.379979570990807,
      "grad_norm": 0.7316210865974426,
      "learning_rate": 6.879156756025851e-07,
      "loss": 1.4267,
      "step": 828
    },
    {
      "epoch": 3.3840653728294177,
      "grad_norm": 0.8056968450546265,
      "learning_rate": 6.788665402466782e-07,
      "loss": 1.4904,
      "step": 829
    },
    {
      "epoch": 3.3881511746680286,
      "grad_norm": 0.900403618812561,
      "learning_rate": 6.698729810778065e-07,
      "loss": 1.6087,
      "step": 830
    },
    {
      "epoch": 3.3922369765066396,
      "grad_norm": 0.7287196516990662,
      "learning_rate": 6.609351137664854e-07,
      "loss": 1.4225,
      "step": 831
    },
    {
      "epoch": 3.39632277834525,
      "grad_norm": 0.7301058173179626,
      "learning_rate": 6.52053053266945e-07,
      "loss": 1.2832,
      "step": 832
    },
    {
      "epoch": 3.400408580183861,
      "grad_norm": 0.8558632135391235,
      "learning_rate": 6.432269138156632e-07,
      "loss": 1.3961,
      "step": 833
    },
    {
      "epoch": 3.404494382022472,
      "grad_norm": 0.9139606356620789,
      "learning_rate": 6.344568089298875e-07,
      "loss": 1.465,
      "step": 834
    },
    {
      "epoch": 3.4085801838610825,
      "grad_norm": 0.8996487855911255,
      "learning_rate": 6.257428514061764e-07,
      "loss": 1.5064,
      "step": 835
    },
    {
      "epoch": 3.4126659856996935,
      "grad_norm": 0.7400582432746887,
      "learning_rate": 6.170851533189537e-07,
      "loss": 1.4433,
      "step": 836
    },
    {
      "epoch": 3.4167517875383044,
      "grad_norm": 0.7786813378334045,
      "learning_rate": 6.084838260190584e-07,
      "loss": 1.4878,
      "step": 837
    },
    {
      "epoch": 3.4208375893769154,
      "grad_norm": 0.739766538143158,
      "learning_rate": 5.999389801323219e-07,
      "loss": 1.3074,
      "step": 838
    },
    {
      "epoch": 3.424923391215526,
      "grad_norm": 0.7754446864128113,
      "learning_rate": 5.914507255581397e-07,
      "loss": 1.469,
      "step": 839
    },
    {
      "epoch": 3.429009193054137,
      "grad_norm": 0.8335386514663696,
      "learning_rate": 5.830191714680578e-07,
      "loss": 1.4504,
      "step": 840
    },
    {
      "epoch": 3.433094994892748,
      "grad_norm": 0.7822410464286804,
      "learning_rate": 5.746444263043715e-07,
      "loss": 1.4282,
      "step": 841
    },
    {
      "epoch": 3.4371807967313583,
      "grad_norm": 0.7392374873161316,
      "learning_rate": 5.663265977787263e-07,
      "loss": 1.4148,
      "step": 842
    },
    {
      "epoch": 3.4412665985699693,
      "grad_norm": 0.7957004308700562,
      "learning_rate": 5.5806579287074e-07,
      "loss": 1.4357,
      "step": 843
    },
    {
      "epoch": 3.4453524004085803,
      "grad_norm": 0.7860425114631653,
      "learning_rate": 5.498621178266167e-07,
      "loss": 1.4458,
      "step": 844
    },
    {
      "epoch": 3.449438202247191,
      "grad_norm": 0.7769101858139038,
      "learning_rate": 5.417156781577903e-07,
      "loss": 1.3078,
      "step": 845
    },
    {
      "epoch": 3.4535240040858017,
      "grad_norm": 0.7761691808700562,
      "learning_rate": 5.336265786395589e-07,
      "loss": 1.5464,
      "step": 846
    },
    {
      "epoch": 3.4576098059244127,
      "grad_norm": 0.7599836587905884,
      "learning_rate": 5.255949233097451e-07,
      "loss": 1.3997,
      "step": 847
    },
    {
      "epoch": 3.4616956077630237,
      "grad_norm": 0.751038134098053,
      "learning_rate": 5.176208154673502e-07,
      "loss": 1.4325,
      "step": 848
    },
    {
      "epoch": 3.465781409601634,
      "grad_norm": 0.758948802947998,
      "learning_rate": 5.097043576712324e-07,
      "loss": 1.3654,
      "step": 849
    },
    {
      "epoch": 3.469867211440245,
      "grad_norm": 0.7425212860107422,
      "learning_rate": 5.018456517387837e-07,
      "loss": 1.3697,
      "step": 850
    },
    {
      "epoch": 3.473953013278856,
      "grad_norm": 0.7583261132240295,
      "learning_rate": 4.940447987446207e-07,
      "loss": 1.4626,
      "step": 851
    },
    {
      "epoch": 3.4780388151174666,
      "grad_norm": 0.7569425702095032,
      "learning_rate": 4.86301899019287e-07,
      "loss": 1.336,
      "step": 852
    },
    {
      "epoch": 3.4821246169560776,
      "grad_norm": 0.7879655361175537,
      "learning_rate": 4.786170521479588e-07,
      "loss": 1.4429,
      "step": 853
    },
    {
      "epoch": 3.4862104187946885,
      "grad_norm": 0.7860598564147949,
      "learning_rate": 4.709903569691687e-07,
      "loss": 1.4837,
      "step": 854
    },
    {
      "epoch": 3.4902962206332995,
      "grad_norm": 0.7419418096542358,
      "learning_rate": 4.634219115735328e-07,
      "loss": 1.3975,
      "step": 855
    },
    {
      "epoch": 3.49438202247191,
      "grad_norm": 0.8371996879577637,
      "learning_rate": 4.5591181330248534e-07,
      "loss": 1.3491,
      "step": 856
    },
    {
      "epoch": 3.498467824310521,
      "grad_norm": 0.7052649259567261,
      "learning_rate": 4.4846015874703274e-07,
      "loss": 1.3468,
      "step": 857
    },
    {
      "epoch": 3.502553626149132,
      "grad_norm": 0.8162820339202881,
      "learning_rate": 4.4106704374650865e-07,
      "loss": 1.2212,
      "step": 858
    },
    {
      "epoch": 3.506639427987743,
      "grad_norm": 0.7681933045387268,
      "learning_rate": 4.3373256338733847e-07,
      "loss": 1.4105,
      "step": 859
    },
    {
      "epoch": 3.5107252298263534,
      "grad_norm": 0.7224563956260681,
      "learning_rate": 4.2645681200182197e-07,
      "loss": 1.4234,
      "step": 860
    },
    {
      "epoch": 3.5148110316649643,
      "grad_norm": 0.7740587592124939,
      "learning_rate": 4.1923988316691535e-07,
      "loss": 1.329,
      "step": 861
    },
    {
      "epoch": 3.5188968335035753,
      "grad_norm": 0.79328852891922,
      "learning_rate": 4.1208186970303097e-07,
      "loss": 1.542,
      "step": 862
    },
    {
      "epoch": 3.522982635342186,
      "grad_norm": 0.7277581095695496,
      "learning_rate": 4.04982863672842e-07,
      "loss": 1.372,
      "step": 863
    },
    {
      "epoch": 3.5270684371807968,
      "grad_norm": 0.7943098545074463,
      "learning_rate": 3.9794295638009683e-07,
      "loss": 1.3744,
      "step": 864
    },
    {
      "epoch": 3.5311542390194077,
      "grad_norm": 0.7272734642028809,
      "learning_rate": 3.90962238368448e-07,
      "loss": 1.3817,
      "step": 865
    },
    {
      "epoch": 3.5352400408580182,
      "grad_norm": 0.7041359543800354,
      "learning_rate": 3.840407994202849e-07,
      "loss": 1.3936,
      "step": 866
    },
    {
      "epoch": 3.539325842696629,
      "grad_norm": 0.7788806557655334,
      "learning_rate": 3.771787285555828e-07,
      "loss": 1.3575,
      "step": 867
    },
    {
      "epoch": 3.54341164453524,
      "grad_norm": 0.8021373748779297,
      "learning_rate": 3.70376114030751e-07,
      "loss": 1.4923,
      "step": 868
    },
    {
      "epoch": 3.5474974463738507,
      "grad_norm": 0.7446374893188477,
      "learning_rate": 3.6363304333750627e-07,
      "loss": 1.4399,
      "step": 869
    },
    {
      "epoch": 3.5515832482124616,
      "grad_norm": 0.8506928086280823,
      "learning_rate": 3.5694960320174056e-07,
      "loss": 1.381,
      "step": 870
    },
    {
      "epoch": 3.5556690500510726,
      "grad_norm": 0.7424331307411194,
      "learning_rate": 3.503258795824105e-07,
      "loss": 1.4477,
      "step": 871
    },
    {
      "epoch": 3.559754851889683,
      "grad_norm": 0.7611809968948364,
      "learning_rate": 3.4376195767042706e-07,
      "loss": 1.3954,
      "step": 872
    },
    {
      "epoch": 3.563840653728294,
      "grad_norm": 0.792959988117218,
      "learning_rate": 3.372579218875649e-07,
      "loss": 1.51,
      "step": 873
    },
    {
      "epoch": 3.567926455566905,
      "grad_norm": 0.8450161814689636,
      "learning_rate": 3.308138558853746e-07,
      "loss": 1.3813,
      "step": 874
    },
    {
      "epoch": 3.572012257405516,
      "grad_norm": 0.8053036332130432,
      "learning_rate": 3.244298425441034e-07,
      "loss": 1.4438,
      "step": 875
    },
    {
      "epoch": 3.576098059244127,
      "grad_norm": 0.794947624206543,
      "learning_rate": 3.181059639716355e-07,
      "loss": 1.4916,
      "step": 876
    },
    {
      "epoch": 3.5801838610827375,
      "grad_norm": 0.7275768518447876,
      "learning_rate": 3.1184230150243025e-07,
      "loss": 1.4653,
      "step": 877
    },
    {
      "epoch": 3.5842696629213484,
      "grad_norm": 0.7427545785903931,
      "learning_rate": 3.0563893569648007e-07,
      "loss": 1.3361,
      "step": 878
    },
    {
      "epoch": 3.5883554647599594,
      "grad_norm": 0.7391684055328369,
      "learning_rate": 2.994959463382735e-07,
      "loss": 1.3695,
      "step": 879
    },
    {
      "epoch": 3.59244126659857,
      "grad_norm": 0.772568941116333,
      "learning_rate": 2.934134124357646e-07,
      "loss": 1.4732,
      "step": 880
    },
    {
      "epoch": 3.596527068437181,
      "grad_norm": 0.7379555702209473,
      "learning_rate": 2.873914122193655e-07,
      "loss": 1.3794,
      "step": 881
    },
    {
      "epoch": 3.600612870275792,
      "grad_norm": 0.6997471451759338,
      "learning_rate": 2.814300231409334e-07,
      "loss": 1.3709,
      "step": 882
    },
    {
      "epoch": 3.6046986721144023,
      "grad_norm": 0.7489771246910095,
      "learning_rate": 2.755293218727739e-07,
      "loss": 1.357,
      "step": 883
    },
    {
      "epoch": 3.6087844739530133,
      "grad_norm": 0.8192427158355713,
      "learning_rate": 2.696893843066617e-07,
      "loss": 1.3688,
      "step": 884
    },
    {
      "epoch": 3.6128702757916242,
      "grad_norm": 0.8154657483100891,
      "learning_rate": 2.6391028555285635e-07,
      "loss": 1.4404,
      "step": 885
    },
    {
      "epoch": 3.6169560776302347,
      "grad_norm": 0.8008753061294556,
      "learning_rate": 2.5819209993914185e-07,
      "loss": 1.3408,
      "step": 886
    },
    {
      "epoch": 3.6210418794688457,
      "grad_norm": 0.7697859406471252,
      "learning_rate": 2.525349010098699e-07,
      "loss": 1.4212,
      "step": 887
    },
    {
      "epoch": 3.6251276813074567,
      "grad_norm": 0.7676495909690857,
      "learning_rate": 2.469387615250096e-07,
      "loss": 1.4,
      "step": 888
    },
    {
      "epoch": 3.629213483146067,
      "grad_norm": 0.769176721572876,
      "learning_rate": 2.4140375345921895e-07,
      "loss": 1.3821,
      "step": 889
    },
    {
      "epoch": 3.633299284984678,
      "grad_norm": 0.8161249756813049,
      "learning_rate": 2.359299480009114e-07,
      "loss": 1.467,
      "step": 890
    },
    {
      "epoch": 3.637385086823289,
      "grad_norm": 0.7568839192390442,
      "learning_rate": 2.3051741555134788e-07,
      "loss": 1.3695,
      "step": 891
    },
    {
      "epoch": 3.6414708886619,
      "grad_norm": 0.7336366176605225,
      "learning_rate": 2.2516622572372416e-07,
      "loss": 1.392,
      "step": 892
    },
    {
      "epoch": 3.6455566905005106,
      "grad_norm": 0.7450525760650635,
      "learning_rate": 2.1987644734228097e-07,
      "loss": 1.4656,
      "step": 893
    },
    {
      "epoch": 3.6496424923391215,
      "grad_norm": 0.7393321394920349,
      "learning_rate": 2.146481484414159e-07,
      "loss": 1.4142,
      "step": 894
    },
    {
      "epoch": 3.6537282941777325,
      "grad_norm": 0.8214421272277832,
      "learning_rate": 2.094813962648101e-07,
      "loss": 1.4359,
      "step": 895
    },
    {
      "epoch": 3.6578140960163434,
      "grad_norm": 0.8094964027404785,
      "learning_rate": 2.0437625726456024e-07,
      "loss": 1.3997,
      "step": 896
    },
    {
      "epoch": 3.661899897854954,
      "grad_norm": 0.7425174713134766,
      "learning_rate": 1.9933279710032894e-07,
      "loss": 1.3201,
      "step": 897
    },
    {
      "epoch": 3.665985699693565,
      "grad_norm": 0.8250062465667725,
      "learning_rate": 1.9435108063849684e-07,
      "loss": 1.3757,
      "step": 898
    },
    {
      "epoch": 3.670071501532176,
      "grad_norm": 0.822384774684906,
      "learning_rate": 1.8943117195132643e-07,
      "loss": 1.4376,
      "step": 899
    },
    {
      "epoch": 3.6741573033707864,
      "grad_norm": 0.7373620867729187,
      "learning_rate": 1.84573134316145e-07,
      "loss": 1.4746,
      "step": 900
    },
    {
      "epoch": 3.6782431052093973,
      "grad_norm": 0.7783380746841431,
      "learning_rate": 1.7977703021452185e-07,
      "loss": 1.4984,
      "step": 901
    },
    {
      "epoch": 3.6823289070480083,
      "grad_norm": 0.7568515539169312,
      "learning_rate": 1.7504292133147194e-07,
      "loss": 1.4484,
      "step": 902
    },
    {
      "epoch": 3.686414708886619,
      "grad_norm": 0.7812298536300659,
      "learning_rate": 1.7037086855465902e-07,
      "loss": 1.4418,
      "step": 903
    },
    {
      "epoch": 3.6905005107252298,
      "grad_norm": 0.8562673926353455,
      "learning_rate": 1.6576093197361253e-07,
      "loss": 1.3783,
      "step": 904
    },
    {
      "epoch": 3.6945863125638407,
      "grad_norm": 0.741338312625885,
      "learning_rate": 1.612131708789566e-07,
      "loss": 1.2742,
      "step": 905
    },
    {
      "epoch": 3.6986721144024512,
      "grad_norm": 0.8125036954879761,
      "learning_rate": 1.5672764376164607e-07,
      "loss": 1.4391,
      "step": 906
    },
    {
      "epoch": 3.702757916241062,
      "grad_norm": 0.7145969867706299,
      "learning_rate": 1.523044083122138e-07,
      "loss": 1.5059,
      "step": 907
    },
    {
      "epoch": 3.706843718079673,
      "grad_norm": 0.8390145897865295,
      "learning_rate": 1.4794352142003088e-07,
      "loss": 1.4818,
      "step": 908
    },
    {
      "epoch": 3.710929519918284,
      "grad_norm": 0.8322778344154358,
      "learning_rate": 1.4364503917257078e-07,
      "loss": 1.4758,
      "step": 909
    },
    {
      "epoch": 3.7150153217568946,
      "grad_norm": 0.7448611259460449,
      "learning_rate": 1.39409016854693e-07,
      "loss": 1.4327,
      "step": 910
    },
    {
      "epoch": 3.7191011235955056,
      "grad_norm": 0.8370977640151978,
      "learning_rate": 1.3523550894793013e-07,
      "loss": 1.4786,
      "step": 911
    },
    {
      "epoch": 3.7231869254341166,
      "grad_norm": 0.7196459174156189,
      "learning_rate": 1.3112456912978467e-07,
      "loss": 1.3163,
      "step": 912
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 0.8356635570526123,
      "learning_rate": 1.2707625027304104e-07,
      "loss": 1.3577,
      "step": 913
    },
    {
      "epoch": 3.731358529111338,
      "grad_norm": 0.7817670106887817,
      "learning_rate": 1.2309060444508746e-07,
      "loss": 1.4627,
      "step": 914
    },
    {
      "epoch": 3.735444330949949,
      "grad_norm": 0.7172181010246277,
      "learning_rate": 1.1916768290724079e-07,
      "loss": 1.4322,
      "step": 915
    },
    {
      "epoch": 3.73953013278856,
      "grad_norm": 0.7758324146270752,
      "learning_rate": 1.1530753611409151e-07,
      "loss": 1.3636,
      "step": 916
    },
    {
      "epoch": 3.7436159346271705,
      "grad_norm": 0.7217952609062195,
      "learning_rate": 1.1151021371285597e-07,
      "loss": 1.4068,
      "step": 917
    },
    {
      "epoch": 3.7477017364657814,
      "grad_norm": 0.7030974626541138,
      "learning_rate": 1.0777576454273242e-07,
      "loss": 1.4032,
      "step": 918
    },
    {
      "epoch": 3.7517875383043924,
      "grad_norm": 0.7929919958114624,
      "learning_rate": 1.041042366342787e-07,
      "loss": 1.3966,
      "step": 919
    },
    {
      "epoch": 3.755873340143003,
      "grad_norm": 0.7501806020736694,
      "learning_rate": 1.004956772087895e-07,
      "loss": 1.3686,
      "step": 920
    },
    {
      "epoch": 3.759959141981614,
      "grad_norm": 0.7442554235458374,
      "learning_rate": 9.695013267769337e-08,
      "loss": 1.3287,
      "step": 921
    },
    {
      "epoch": 3.764044943820225,
      "grad_norm": 0.7362738251686096,
      "learning_rate": 9.346764864195335e-08,
      "loss": 1.3601,
      "step": 922
    },
    {
      "epoch": 3.7681307456588353,
      "grad_norm": 0.7980939745903015,
      "learning_rate": 9.004826989148008e-08,
      "loss": 1.5041,
      "step": 923
    },
    {
      "epoch": 3.7722165474974463,
      "grad_norm": 0.7476233839988708,
      "learning_rate": 8.669204040455737e-08,
      "loss": 1.4711,
      "step": 924
    },
    {
      "epoch": 3.7763023493360572,
      "grad_norm": 0.7983424067497253,
      "learning_rate": 8.339900334727536e-08,
      "loss": 1.4498,
      "step": 925
    },
    {
      "epoch": 3.7803881511746678,
      "grad_norm": 0.7897741198539734,
      "learning_rate": 8.016920107297654e-08,
      "loss": 1.4554,
      "step": 926
    },
    {
      "epoch": 3.7844739530132787,
      "grad_norm": 0.767196774482727,
      "learning_rate": 7.700267512171011e-08,
      "loss": 1.4921,
      "step": 927
    },
    {
      "epoch": 3.7885597548518897,
      "grad_norm": 0.7897652387619019,
      "learning_rate": 7.389946621969679e-08,
      "loss": 1.3608,
      "step": 928
    },
    {
      "epoch": 3.7926455566905006,
      "grad_norm": 0.7920624613761902,
      "learning_rate": 7.085961427880705e-08,
      "loss": 1.4421,
      "step": 929
    },
    {
      "epoch": 3.7967313585291116,
      "grad_norm": 0.7749043703079224,
      "learning_rate": 6.788315839604765e-08,
      "loss": 1.3709,
      "step": 930
    },
    {
      "epoch": 3.800817160367722,
      "grad_norm": 0.8241469264030457,
      "learning_rate": 6.497013685305586e-08,
      "loss": 1.4301,
      "step": 931
    },
    {
      "epoch": 3.804902962206333,
      "grad_norm": 0.8041728138923645,
      "learning_rate": 6.212058711561165e-08,
      "loss": 1.3831,
      "step": 932
    },
    {
      "epoch": 3.808988764044944,
      "grad_norm": 0.7825815081596375,
      "learning_rate": 5.933454583315068e-08,
      "loss": 1.4465,
      "step": 933
    },
    {
      "epoch": 3.8130745658835545,
      "grad_norm": 0.839251697063446,
      "learning_rate": 5.661204883829763e-08,
      "loss": 1.5142,
      "step": 934
    },
    {
      "epoch": 3.8171603677221655,
      "grad_norm": 0.7272753715515137,
      "learning_rate": 5.395313114640421e-08,
      "loss": 1.4814,
      "step": 935
    },
    {
      "epoch": 3.8212461695607765,
      "grad_norm": 0.7981514930725098,
      "learning_rate": 5.135782695509461e-08,
      "loss": 1.4983,
      "step": 936
    },
    {
      "epoch": 3.825331971399387,
      "grad_norm": 0.7423074841499329,
      "learning_rate": 4.8826169643832464e-08,
      "loss": 1.3747,
      "step": 937
    },
    {
      "epoch": 3.829417773237998,
      "grad_norm": 0.8285096883773804,
      "learning_rate": 4.635819177348677e-08,
      "loss": 1.4461,
      "step": 938
    },
    {
      "epoch": 3.833503575076609,
      "grad_norm": 0.7236446738243103,
      "learning_rate": 4.3953925085913894e-08,
      "loss": 1.4399,
      "step": 939
    },
    {
      "epoch": 3.8375893769152194,
      "grad_norm": 0.7268850207328796,
      "learning_rate": 4.1613400503550114e-08,
      "loss": 1.4196,
      "step": 940
    },
    {
      "epoch": 3.8416751787538304,
      "grad_norm": 0.7648739218711853,
      "learning_rate": 3.9336648129014166e-08,
      "loss": 1.4369,
      "step": 941
    },
    {
      "epoch": 3.8457609805924413,
      "grad_norm": 0.8422081470489502,
      "learning_rate": 3.71236972447192e-08,
      "loss": 1.3813,
      "step": 942
    },
    {
      "epoch": 3.849846782431052,
      "grad_norm": 0.8283671736717224,
      "learning_rate": 3.4974576312497564e-08,
      "loss": 1.4659,
      "step": 943
    },
    {
      "epoch": 3.853932584269663,
      "grad_norm": 0.8174403309822083,
      "learning_rate": 3.2889312973231616e-08,
      "loss": 1.473,
      "step": 944
    },
    {
      "epoch": 3.8580183861082737,
      "grad_norm": 0.7713686227798462,
      "learning_rate": 3.086793404650179e-08,
      "loss": 1.4479,
      "step": 945
    },
    {
      "epoch": 3.8621041879468847,
      "grad_norm": 0.7708502411842346,
      "learning_rate": 2.8910465530240793e-08,
      "loss": 1.4114,
      "step": 946
    },
    {
      "epoch": 3.8661899897854957,
      "grad_norm": 0.6922297477722168,
      "learning_rate": 2.7016932600396618e-08,
      "loss": 1.4386,
      "step": 947
    },
    {
      "epoch": 3.870275791624106,
      "grad_norm": 0.7953127026557922,
      "learning_rate": 2.5187359610612805e-08,
      "loss": 1.4305,
      "step": 948
    },
    {
      "epoch": 3.874361593462717,
      "grad_norm": 0.7290569543838501,
      "learning_rate": 2.3421770091912044e-08,
      "loss": 1.4012,
      "step": 949
    },
    {
      "epoch": 3.878447395301328,
      "grad_norm": 0.7306029200553894,
      "learning_rate": 2.1720186752395288e-08,
      "loss": 1.3525,
      "step": 950
    },
    {
      "epoch": 3.8825331971399386,
      "grad_norm": 0.8024385571479797,
      "learning_rate": 2.0082631476948113e-08,
      "loss": 1.3941,
      "step": 951
    },
    {
      "epoch": 3.8866189989785496,
      "grad_norm": 0.802801251411438,
      "learning_rate": 1.850912532696092e-08,
      "loss": 1.5308,
      "step": 952
    },
    {
      "epoch": 3.8907048008171605,
      "grad_norm": 0.7608079314231873,
      "learning_rate": 1.6999688540057513e-08,
      "loss": 1.3916,
      "step": 953
    },
    {
      "epoch": 3.894790602655771,
      "grad_norm": 0.835681140422821,
      "learning_rate": 1.5554340529834733e-08,
      "loss": 1.4094,
      "step": 954
    },
    {
      "epoch": 3.898876404494382,
      "grad_norm": 0.802079975605011,
      "learning_rate": 1.4173099885610997e-08,
      "loss": 1.367,
      "step": 955
    },
    {
      "epoch": 3.902962206332993,
      "grad_norm": 0.7754142880439758,
      "learning_rate": 1.2855984372191488e-08,
      "loss": 1.3993,
      "step": 956
    },
    {
      "epoch": 3.9070480081716035,
      "grad_norm": 0.7569400072097778,
      "learning_rate": 1.1603010929635006e-08,
      "loss": 1.4451,
      "step": 957
    },
    {
      "epoch": 3.9111338100102144,
      "grad_norm": 0.7907391786575317,
      "learning_rate": 1.041419567303914e-08,
      "loss": 1.4169,
      "step": 958
    },
    {
      "epoch": 3.9152196118488254,
      "grad_norm": 0.8018848896026611,
      "learning_rate": 9.289553892331549e-09,
      "loss": 1.4418,
      "step": 959
    },
    {
      "epoch": 3.919305413687436,
      "grad_norm": 0.7735564112663269,
      "learning_rate": 8.229100052074557e-09,
      "loss": 1.4946,
      "step": 960
    },
    {
      "epoch": 3.923391215526047,
      "grad_norm": 0.720868706703186,
      "learning_rate": 7.2328477912769756e-09,
      "loss": 1.4238,
      "step": 961
    },
    {
      "epoch": 3.927477017364658,
      "grad_norm": 0.7901839017868042,
      "learning_rate": 6.300809923222017e-09,
      "loss": 1.4223,
      "step": 962
    },
    {
      "epoch": 3.9315628192032688,
      "grad_norm": 0.7720843553543091,
      "learning_rate": 5.432998435298542e-09,
      "loss": 1.3691,
      "step": 963
    },
    {
      "epoch": 3.9356486210418797,
      "grad_norm": 0.7464596033096313,
      "learning_rate": 4.629424488850065e-09,
      "loss": 1.4317,
      "step": 964
    },
    {
      "epoch": 3.9397344228804902,
      "grad_norm": 0.822101354598999,
      "learning_rate": 3.890098419029875e-09,
      "loss": 1.4994,
      "step": 965
    },
    {
      "epoch": 3.943820224719101,
      "grad_norm": 0.7383795976638794,
      "learning_rate": 3.215029734667252e-09,
      "loss": 1.3485,
      "step": 966
    },
    {
      "epoch": 3.947906026557712,
      "grad_norm": 0.7236939072608948,
      "learning_rate": 2.604227118148117e-09,
      "loss": 1.421,
      "step": 967
    },
    {
      "epoch": 3.9519918283963227,
      "grad_norm": 0.819514811038971,
      "learning_rate": 2.05769842529957e-09,
      "loss": 1.5194,
      "step": 968
    },
    {
      "epoch": 3.9560776302349336,
      "grad_norm": 0.780460774898529,
      "learning_rate": 1.5754506852916352e-09,
      "loss": 1.4318,
      "step": 969
    },
    {
      "epoch": 3.9601634320735446,
      "grad_norm": 0.7897670865058899,
      "learning_rate": 1.1574901005456662e-09,
      "loss": 1.4812,
      "step": 970
    },
    {
      "epoch": 3.964249233912155,
      "grad_norm": 0.7519046664237976,
      "learning_rate": 8.038220466549674e-10,
      "loss": 1.3923,
      "step": 971
    },
    {
      "epoch": 3.968335035750766,
      "grad_norm": 0.7593739628791809,
      "learning_rate": 5.144510723154028e-10,
      "loss": 1.3946,
      "step": 972
    },
    {
      "epoch": 3.972420837589377,
      "grad_norm": 0.8448845744132996,
      "learning_rate": 2.89380899267111e-10,
      "loss": 1.4433,
      "step": 973
    },
    {
      "epoch": 3.9765066394279875,
      "grad_norm": 0.7788746356964111,
      "learning_rate": 1.2861442224565423e-10,
      "loss": 1.41,
      "step": 974
    },
    {
      "epoch": 3.9805924412665985,
      "grad_norm": 0.7904308438301086,
      "learning_rate": 3.215370894760206e-11,
      "loss": 1.5395,
      "step": 975
    },
    {
      "epoch": 3.9846782431052095,
      "grad_norm": 0.803241491317749,
      "learning_rate": 0.0,
      "loss": 1.48,
      "step": 976
    }
  ],
  "logging_steps": 1,
  "max_steps": 976,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5269583538372096.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
